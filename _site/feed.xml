<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-09T18:43:10+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Reinvent love! - Democratization of Love</title><subtitle>NLP 쪼렙입니다.</subtitle><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><entry><title type="html">Post: 딥러닝의 모든 것(All about Deep Learning)</title><link href="http://localhost:4000/aitech/post-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EB%AA%A8%EB%93%A0%EA%B2%83/" rel="alternate" type="text/html" title="Post: 딥러닝의 모든 것(All about Deep Learning)" /><published>2023-03-09T00:00:00+09:00</published><updated>2023-03-09T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98%20%EB%AA%A8%EB%93%A0%EA%B2%83</id><content type="html" xml:base="http://localhost:4000/aitech/post-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EB%AA%A8%EB%93%A0%EA%B2%83/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="딥러닝의-모든-것">딥러닝의 모든 것</h1>
<p>안녕하십니까. 최윤진입니다. 이번 포스팅을 통해 딥러닝의 전반적인 내용을 한 번 다뤄보려고 합니다.</p>

\[\begin{aligned}
\mathbf{O}_t &amp; =\mathbf{H}_t \mathbf{W}^{(2)}+\mathbf{b}^{(2)} \\
\mathbf{H}_t &amp; =\sigma\left(\mathbf{X}_t \mathbf{W}_X^{(1)}+\mathbf{H}_{t-1} \mathbf{W}_H^{(1)}+\mathbf{b}^{(1)}\right)
\end{aligned}\]

\[\begin{aligned}
\lim_{x\to 0}{\frac{e^x-1}{2x}}
\overset{\left[\frac{0}{0}\right]}{\underset{\mathrm{H}}{=}}
\lim_{x\to 0}{\frac{e^x}{2}}={\frac{1}{2}}
\end{aligned}\]]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: 딥러닝 모니터링(Monintoring) 하기</title><link href="http://localhost:4000/aitech/post-monitoring/" rel="alternate" type="text/html" title="Post: 딥러닝 모니터링(Monintoring) 하기" /><published>2023-03-09T00:00:00+09:00</published><updated>2023-03-10T10:20:02+09:00</updated><id>http://localhost:4000/aitech/post-monitoring</id><content type="html" xml:base="http://localhost:4000/aitech/post-monitoring/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="monitoring">Monitoring</h1>
<p>학습을 하는 데 굉장히 긴 시간이 걸립니다. 학습 과정을 기록하는게 좋겠죠. <code class="language-plaintext highlighter-rouge">TensorBoard</code>와 <code class="language-plaintext highlighter-rouge">Weight &amp; Bias</code>를 이용해봅시다.</p>

<h1 id="tensorboard">Tensorboard</h1>
<ul>
  <li>scalar : metric 표시</li>
  <li>graph : 계산 그래프</li>
  <li>histogram : weight 분포</li>
  <li>image: 예측 값과 실게 값을 비교 표시</li>
  <li>mesh : 3d 형태의 데이터를 표현하는 도구</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
import os
logs_base_dir = 'logs'
os.makedirs(logs_base_dir, exist_ok=True)

from torch.utils.tensorboard import SummaryWriter
import numpy as np

exp = f"{logs_base_dir}/ex3"
writer = SummaryWriter(exp)
for n_iter in range(100):
  writer.add_scalar('Loss/train', np.random.random(), n_iter)
  writer.add_scalar('Loss/test', np.random.random(), n_iter)
  writer.add_scalar('Accuracy/train', np.random.random(), n_iter)  
  writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
writer.flush()

%load_ext tensorboard
%tensorboard --logdir {"logs"}

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 분포를 본다.

from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter(logs_base_dir)
for i in range(10):
  x = np.random.random(1000)
  writer.add_histogram('distribution centers', x+i, i)
writer.close()

</code></pre></div></div>

<h1 id="weight--bias">Weight &amp; Bias</h1>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - pytorch Template 완전 분석</title><link href="http://localhost:4000/aitech/post-pytorchtemplate/" rel="alternate" type="text/html" title="Post: ai tech - pytorch Template 완전 분석" /><published>2023-03-08T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-pytorchtemplate</id><content type="html" xml:base="http://localhost:4000/aitech/post-pytorchtemplate/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="pytorch-템플릿을-문석해봅시다">Pytorch 템플릿을 문석해봅시다.</h1>
<p><a href="https://github.com/victoresque/pytorch-template">링크</a>
 여기 오픈 소스가 하나 있습니다. 이 포스팅을 통해 이 템플릿을 분해, 분석 해보는 시간을 가지려고 합니다.</p>

<p><img src="../../../image/aitech/pytorchtemplatetree.png" alt="파일 " /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* train.py              -&gt; 실행
* test.py               -&gt; 실행
* config                -&gt; 설정
* parse_config          -&gt; 설정
* base                  -&gt; base 모델
* data_loader ~         -&gt; data 
* model ~               -&gt; 모델
* save ~                -&gt; 저장소
* trainer ~             -&gt; 학습
* logger ~              -&gt; 로깅
* utils ~               -&gt; 유틸
</code></pre></div></div>

<hr />
<h2 id="실행-방법입니다">실행 방법입니다</h2>
<p>루트 디렉토리에서 <code class="language-plaintext highlighter-rouge">python train.py -c config.json</code> 을 치세요.</p>

<h2 id="분해">분해</h2>
<ul>
  <li>train.py
    <ul>
      <li>arg.add_argument</li>
    </ul>
  </li>
  <li>config.json
    <ul>
      <li>util의 <code class="language-plaintext highlighter-rouge">read_josn</code> 을 보라.</li>
    </ul>
  </li>
  <li>train/tariner.py</li>
  <li>baser/base_tariner.py
    <ul>
      <li>학습의 원천이 되는 소스</li>
    </ul>
  </li>
</ul>

<h2 id="아하-그러니까요">아하 그러니까요.</h2>
<p>스프링 부트와 같은 웹 프레임워크를 사용하신 분들에는 익숙한 구조일겁니다.
가령 우리는 yml 파일을 수정함으로써 configuration을 바꿉니다. 소스 전체를 바꾸는 것이 아니라 특정 config 요소만 바꿈으로써 소스 전체에 영향을 끼치는 겁니다.
이것이 템플릿, 프레임워크의 힘의 근원이라고 할 수 있는 것이죠.</p>

<p>pytorch-template 도 같은 시각에서 접근해봅시다. 
config.json 만 바꿈으로써 우리는 새로운 데이터를 로딩할 수 있는 것이고 batch_size, validation_split 도 변경 할 수 가 있습니다. 
train.py 만 바꿈으로써 우리는 모델을 손쉽게 교체할 수 가 있습니다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech -Day03</title><link href="http://localhost:4000/aitech/post-day03/" rel="alternate" type="text/html" title="Post: ai tech -Day03" /><published>2023-03-08T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day03</id><content type="html" xml:base="http://localhost:4000/aitech/post-day03/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<ul>
  <li>python 복습했다.</li>
  <li>aimath 1 ~ 5 강을 들었다.</li>
  <li>pytorch template 분석 했다.</li>
  <li>포스팅 정리를 했다.</li>
  <li>github 블로그를 만들었다.</li>
</ul>

<h1 id="내일-무엇을-할-것인가">내일 무엇을 할 것인가?</h1>
<ul>
  <li>ai math, pytorch 강의 마무리 하면서 포스팅 정리 하기</li>
  <li>과제 검토</li>
  <li>RNN 의 오차 역전파 연구하기</li>
</ul>

<h1 id="오늘의-생각">[오늘의 생각]</h1>
<p>오늘 NLP 팀 발표 시간을 가졌다. 논문 리뷰, 코딩 테스트 스터디를 많이 하는 듯 했다. 실력자들이 굉장히 많다는 걸 느끼면서 겸손해야 겠다는 생각을 했다. 내가 속한 팀의 팀원들도 다들 실력이 좋으시다. 팀에 도움이 될 수 있도록 노력했야 겠다는 다짐을 하게 된다. 적어도 마이너스를 하지는 말아야지.</p>

<p>다음 주 부터는 코딩테스트 스터디를 시작하려고 한다. 그리고 파이토치도 정규 수업으로 들어가고 본격적으로 딥러닝 모델로 무언가를 만들어보는 시간을 가진다. 그리고 다음 달이 되면 첫 번째 프로젝트를 참여 하게 된다. 좋은 성과를 거두고 싶다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">딥러닝에 필요한 수학적 지식</title><link href="http://localhost:4000/aitech/post-aimath/" rel="alternate" type="text/html" title="딥러닝에 필요한 수학적 지식" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-aimath</id><content type="html" xml:base="http://localhost:4000/aitech/post-aimath/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="벡터">벡터</h1>
<p>벡터란 뭡니까? 스칼라 다음의 차원을 표시하기 위한 수학 도구입니다.
$ L_1$ 노름은 변화량읠 절대값을,  $L_2$ 노름은 유클리드 거리를 말합니다</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="행렬">행렬</h1>

<p>행렬에 대해서 알아봅시다. 행렬곱은 행과 열의 조건을 맞춰야 연산가능합니다. 행렬의 곱은 교환법칙이 성립하지 않으니 유의해야 합니다. 
파이썬에서는 @ 기호를 통해 행렬의 곱을 지원합니다. <code class="language-plaintext highlighter-rouge">np.inner</code> 도 가능합니다.(자동 transpose 지원) <br />
역행렬은 곱의 연산을 했을 때 항등행렬이 나오는 행렬을 말합니다. 곱의 역원을 의미합니다. <br />
역행렬은 선형사상과 필요충분조건입니다. (선형대수학에 대한 지식이 필요합니다.) <code class="language-plaintext highlighter-rouge">np.linalg.pinv</code> 모듈을 이용해서 구할 수 있습니다 <br />
유사 역행렬에는 무어펜로스(Moore-Penrose) 역행렬이 있습니다. 왜냐하면 역행렬을 구하는 것이 쉽지 않기 때문입니다. <br />
<img src="../../../image/aitech/무어펜로즈.png" alt="무어펜로즈.png" /></p>

<p>행렬은 왜 배우는 것일까요? 연립방정식, 선형회귀 분석에 응용됩니다.<code class="language-plaintext highlighter-rouge">차원이동</code> <code class="language-plaintext highlighter-rouge">연산</code> 을 쉽게 만들어 주기 때문이죠. <br />
딥러닝을 제대로 이해하기 위해선 ‘선형대수학’을 수준 높게 학습해야 합니다.
사이킷런의 선형회귀 모델과 무어펜로즈의 선형회귀를 직접 구현보시는 건 어떻습니까? <br /></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="경사하강법-gradinet-descent">경사하강법 (Gradinet Descent)</h1>
<p>경사하강법에 대해서 알아봅시다. 우선 기초적인 미분에 대해서 알아볼까요?</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="미분">미분</h2>
<p>먼저 미분에 대해서 알아봅시다.
미분이란 접선의 기울기를 말하죠. 
<code class="language-plaintext highlighter-rouge">sysmpy.diff</code> 모듈을 이용하여 미분계산이 가능합니다.
이를 통해서 우리는 주어진 input 데이터가 <strong>어느 차원에서든</strong> 그 점에서 증가하는가, 감소하는가를 알 수 있습니다.
어느 차원으로 확장한다는 것은 변수를 스칼라가 아닌 벡터와 텐서를 사용한다는 것입니다.</p>

<p>좋습니다. 이제 경사하강법을 알아볼까요? 
경사하강법이란 기울기가 감소하는 방향으로 쭈욱 가다 <strong>보면</strong> 언젠가 평지를(극소값)을 만날 것입니다. 컴퓨터에서 미분값이 0 인 곳을 찾기란 쉽지 않으므로 아주 작은 값(오메가) 보다 더 미분 값이 작으면 종료하면 됩니다. 이것이 간략한 설명입니다.
이 때 중요한 것은 어느 만큼의 보폭으로 갈 것이냐입니다. 이것은 하이퍼 파라메터로 즉 학습률입니다.
변수가 스칼라가 아니라 벡터라면 편미분을 하면 됩니다. 
모든 데이터를 사용하는 것은 고전적인 방법입니다. 이것은 성능이 좋지 않고 하드웨어에 부담이 갑니다. 시간도 많이 걸리죠. 
조금의 데이터만 이용해서 경사를 갱신하는 것을 SGD 라고 합니다. 이 방법은 최근 많이 사용하는 방법입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="그레디언트-벡터gradient-vector">그레디언트 벡터(gradient Vector)</h2>
<p>함수의 벡터 변수 별로 편미분을 계산한 함수를 말합니다.
계산식은 이렇게 됩니다. 
<img src="../../../image/aitech/gradient.png" alt="gradient" />
그레디언트 벡터에 - 를 붙여서 이동하게 되면 <strong>가장 빨리</strong> 극소값을 향해 가게 되는 겁니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>그레디언트 벡터의 `norm` 값을 구해서 일정 값보다 작게 되면 학습을 종료하는 방식으로 극소값을 찾으면 된다.
(계속 내려가다가 더 이상 변화가 없는 것 같아. -&gt; 일단 멈춰!)
</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="선형회귀-모델에서-경사하강법-적용해보기">선형회귀 모델에서 경사하강법 적용해보기</h1>
<p>선형 모델을 구할 때 무어 펜로즈 행렬을 사용할수 도 있습니다. <br />
하지만 우리는 무어펜로즈 행렬을 곱해서 나온 우변 식을 경사하강법을 하지 않고 오직 경사 하강법을 적용시켜 극소값이 되는 지점을 찾는 것이다.<br />
$y-y$ 의 값을 미분해서 학습률을 곱해 에포크동안 최소화 지키다는 것입니다. <br /></p>

<p>직접 계산해봅시다. 학습률과 학습횟수를 적절하게 선택했을 때만 수렴을 보장할 수 있습니다. 비선형회귀의 경우, 볼록하지 않기 때문에 수렴을 보장할 수 없다. (왜 불가능한가? 고민해봅시다.)</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="미니배치-확률적-경사하강법sgd">미니배치 확률적 경사하강법(SGD)</h1>
<p>데이터를 일부 사용해서 경사하강법을 적용하는 방식을 말합니다. 경험적으로 좋다는 것이 밝혀져 있습니다.  SGD 가 경사하강법보다 낫다는 것이 실증적으로 검증되었다. 데이터를 나눠서 epoch 를 반복하여 학습하는 것을 말합니다. 데이터를 일부로 사용하기 때문에 목적식이 매번 달라집니다.
학습률, 미니배치사이즈가 하이퍼 파라메터입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="딥러닝-학습방법">딥러닝 학습방법</h1>
<p>이제 비선형모델인 신경망을 도전해봅시다.먼저 소프트 맥스 함수 부터 알아보지요.</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="소프트맥스">소프트맥스</h2>
<p>모델의 출력을 확률로 해석하여 확률 벡터로 변환하는 함수입니다. 가장 큰 값이 예측값이 되는 것입니다.
신경망은 <strong>선형모델과 활성함수를 합성한 함수</strong>입니다. 이러한 layer들은 잠재벡터들의 누적하게 되고 마지막 공간에 활성화 함수를 적용하는 것입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="활성함수">활성함수</h2>
<p>R 위에 정의된 비선형 함수입니다. 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없습니다. 대표적으로 Relu, tanh, sigmoid 가 있습니다.
순전파는 주어진 신경망 계산을 하는 것을 말합니다. 이 때 우리는 왜 여러 계층을 사용하는 걸까요? 왜 층을 여러개 쌓는가? 수학적으로 계층이 3개 이상만 있으면 임의의 연속함수를 근사할 수 있다는 것이 증명되어 있다. ‘세계’를 표현할 수 있는 것이죠.  역전파는 경사하강법을 이용해서 가중치를 업데이트 하는 것을 말합니다.  위층부터 저층으로 그레디어트 벡터를 전달해야 함(연쇄법칙) 메모리에 저장해서 사용해야 합니다. tensorflow, pytorch 에는 자동 구현되어 있습니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="확률론">확률론</h1>
<p>확률론은 딥러닝에서 대단히 중요합니다. 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.
확률론이 중요한 이유를 알아봅시다.  <code class="language-plaintext highlighter-rouge">분류</code> 문제 에서 예측 오차의 분산을 최소화하고 <code class="language-plaintext highlighter-rouge">교차 엔트로피</code> 문제에서 모델 예측의 불확실성을 최소화하는데 쓰입니다. 확률 분포는 데이터의 초상화입니다. 확률은 면적입니다. 이산 확률 변수에서는 시그마 급수, 연속 확률 변수에서는 연속 확률을 적분하는 것을 말합니다. 목표는 데이터의(표본) 확률 분포를 가지고 -&gt; 실제 (모집단)의 확률 분포를 추정하는 것입니다. 
<strong>몬테카를로 샘플링</strong>은 데이터(샘플링)를 통해 기댓값을 계산하는 방법을 말합니다. 대수의 법칙을 통해 수렴성을 보장합니다.</p>

<ul>
  <li>주변확률분포</li>
  <li>통계값(모수)
    <ul>
      <li>기댓값</li>
      <li>분산</li>
      <li>첨도</li>
      <li>공분산</li>
      <li>조건부확률</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np

def monte(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high - low)
    stat = []

    for _ in range(repeat):
        x = np.random.uniform(low=low, high=high, size=sample_size)
        fun_x = fun(x)
        int_val = int_len * np.mean(fun_x)
        stat.append(int_val)

    return np.mean(stat), np.std(stat)


- 주어진 함수
def f_x(x):
    return np.exp(-x**2)


print(monte(f_x, low=-1, high=1, sample_size=1000, repeat=100))

</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="통계학">통계학</h1>
<p>통계적 모델링 확률분포를 추정하는 것입니다. 그 중 모수를 추정한다는 것은 확률분포의 특성값(모수)를 추정한다는 것이죠. 모수적 방법론은 특정 확률 분포를 가정 한 후 모수를 추정하는 것이며, 비모수적 방법론은 가정 없이 추정하는 것입니다.
모수적 방법론의 과정은 다음과 같은 순서가 될 것입니다. <br />
<code class="language-plaintext highlighter-rouge">데이터 관찰 -&gt; 확률 분포 가정 -&gt; 평균, 분산 추정 -&gt; 모수 집단 추청</code></p>

<p>데이터를 관찰하고 확률 분포를 가정합니다.</p>

<ul>
  <li>베르누이 분포</li>
  <li>카테고리 분포</li>
  <li>베타 분포</li>
  <li>감마분포, 로그정규분포</li>
  <li>정규분포, 라플라스분포</li>
</ul>

<p>가 있습니다. 하나 하나가 공부해 볼 가치가 있는 것들이죠.</p>

<p>중심극한정리는 표뵨 평균의 표집분포는 N이 커질수록 정규분포를 따른다는 것을 말합니다.
<code class="language-plaintext highlighter-rouge">표집분포는 통계량의 확률분포를 말합니다.</code></p>

<h2 id="최대가능도-추정법">최대가능도 추정법</h2>
<p>매우 중요합니다. <code class="language-plaintext highlighter-rouge">최대가능도 추정법</code>에 대해 공부해봅시다.
가능도란 무엇일까요? <strong>모수 쌔타 분포에서 데이터 x 를 발견할 가능성.</strong> 을 말합니다.
그렇다면 자연스럽게  <strong>가장 가능성이 높은 모수를 추정하는 방법</strong> 은 최대 가능성 추정법이 되겠죠.
즉, 데이터가 많이 모인 상황에서 <code class="language-plaintext highlighter-rouge">특정 데이터의 분포</code> 를 가정하고 <code class="language-plaintext highlighter-rouge">제일 가능성 높은</code> 모수를 추정하는 것이라는 말입니다.
정답 확률분포와 모델 추정 확률분포 손실함수 구해서 학습을 시킵시다. 딥러닝의 방법이 도입되는 것이죠.</p>

<p>딥러닝에서 최대가능도 추정법을 자세히 알아봅시다. 우리는 신경망의 마지막 계층에 소프트맥스 벡터를 적용시켜주게 됩니다. 이것은 신경망의 마지막 발산 값이 카테고리 분포라고 할 수 있는 것 인데요. 두 개의 확률분포의 손실함수를 학습시키면 됩니다. 이 식은 쿨백-라이블러 발산을 최소화하는 것과 같습니다. 요약하자면 분류 문제에서 정답레이블을 P, 모델 예측을 Q 라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같습니다.</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=XhlfVtGb19c">설명링크</a></li>
  <li><a href="https://angeloyeo.github.io/2020/07/17/MLE.html">설명블로그</a></li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>확률과 가능도의 차이 ? 
  확률 : 이 사건이 일어날 경우의 수를 전체 사건의 수로 나눈 것.
  가능도 : 지금 얻은 데이터가 이 분포에서 나왔을 가능도
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>쿨백-라이블러 발산 이란?

쿨백-라이블러 발산(Kullback-Leibler divergence) 은 두 확률분포 p(y), q(y) 의 분포모양이 얼마나 다른지를 숫자로 계산한 값입니다.
그 값은 항상 양수이며 두 확률분포 완전히 같을 경우에만 0이 된다.교차엔트로피 개념을 이해할 필요가 있습니다.
</code></pre></div></div>

<p>생각해보세요. 과연 어떻게 할까?</p>

<ul>
  <li>정규분포에서 최대 가능도 추정</li>
  <li>카테고리 분포에서 최대 가능도 추정</li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="베이즈-통계학">베이즈 통계학</h1>
<p>조건부 확률을 이용하면 정보를 갱신하는 방법을 알 수 있습니다. A 라는 사건이 추가로 주어졌을 때 B 가 일어날 확률은  다음과 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B가 일어날 확률 * P(A|B) P(A)
</code></pre></div></div>

<ul>
  <li>사후확률 = 사전확률 * 가능도/증거</li>
  <li>1종 오류, 2종 오류</li>
  <li>갱신된 사후확률을 계산 할 수 있다. (연속된 계산) (ex. 두번 검진)
    <ul>
      <li>정확도를 높일 수 있다.</li>
    </ul>
  </li>
  <li>조건부 확률을 토대로 인과관계를 함부로 추론해서는 안된다.
    <ul>
      <li>데이터에 따라 달라질 수 있기 때문임.</li>
      <li>중첩효과를 제거해서 가짜 연관관계를 제거해야 함.</li>
      <li>인과관계를 뒷받침하는 모델로서 작동하는 조건부확률</li>
    </ul>
  </li>
  <li>상관관계와 인과관계는 다르다.
    <ul>
      <li>키와 지능의 연관관계
        <ul>
          <li>상관관계 성림</li>
          <li>인과관계는 성립하지 않음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>베이즈 추정법
    <ul>
      <li>베이즈 추정법(Bayesian estimation)은 모숫값이 가질 수 있는 모든 가능성의 분포를 계산하는 작업이다.</li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="cnn">CNN</h1>
<p>CNN은 이미지 연산에서 많이 사용하는 모델입니다. 국소적 증폭, 국소적 증감을 만들어낼 수 있습니다. 또 여러 차원에서 적용이 가능합니다. 채널이 여러개인 경우 커널의 채널 수와 입력의 채널수가 같아야 합니다. 순전파는 커널을 벡터상에서 움직여 함성함수를 적용합니다.역전파는 어떻게 적용하는 것일까요? 생각해봅시다.Convolution 연산에서 어떻게 역전파를 적용할 껀가요? 다음 자료를 참고해보세요.</p>

<p><a href="https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/">참고자료</a></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="rnn">RNN</h1>
<p>Recurrent Natural Network. RNN 입니다. 시퀀스데이터는 연속적인 데이터를 말하는데요. 발생 순서가 기록되어 있는 데이터입니다. 가령, 주가데이터나 학년별 성적표 같은 것이 해당 될 수 있겠죠. 시퀀스 데이터의 특징은 다음과 같습니다.</p>
<ul>
  <li><strong>독립 동등 분포를 위배한다.</strong></li>
  <li><strong>데이터의 순서가 중요하다.</strong></li>
  <li><strong>과거의 데이터가 중요하다</strong>.</li>
  <li><strong>하지만 모든 과거 데이터가 필요한 것은 아님</strong>  =&gt; 그 유명한 어텐션 모델의 탄생 (<code class="language-plaintext highlighter-rouge">Attention all you need)</code></li>
</ul>

<p>시퀀스데이터에서 조건부확률에 대해서 생각해봅시다. 시퀀스데이터는 과거의 데이터가 지속적으로 영향을 주게 되는데 이 때 모든 값을 다 고려하여 다뤄야 합니다. 즉 시간에 따라서 가변적인 데이터를 다뤄야 한다는 것을 의미하죠. 이 것 또한 특정 함수의 식이라고 한다면 고정 데이터로 바꿀 수 있습니다.</p>

<p>RNN 에서 역전파는 BPTT(Backprogaion Throgh Time) 을 사용합니다. BPTT 미분은 매우 복잡하다.</p>

<ul>
  <li><a href="https://davi06000.tistory.com/92">링크1</a></li>
  <li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">링크2</a></li>
</ul>

<p>이 두 자료를 한번 보도록 하십시요.</p>

<p>RNN 에서의 역전파는 수식의 특성상 기울기 소실 문제가 발생합니다. 과거 데이터를 반영하지 못하는 것이죠. 자연스럽게 LSTM, GRU 와 같은 고급 모델이 탄생은 필연적이었습니다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - Day02</title><link href="http://localhost:4000/aitech/post-day02/" rel="alternate" type="text/html" title="Post: ai tech - Day02" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day02</id><content type="html" xml:base="http://localhost:4000/aitech/post-day02/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- numpy, pandas  를 집중 분석했다.
- 공부하는 것과 실제 적용하는 것에는 큰 차이가 있는 거 같다.
- 경사하강법, 오차 역전파에서 스칼라가 텐서로 바뀔 때의 상황이 직관적으로 와닿지가 않아서 공부했다.
- 최대 우도법을 공부.
</code></pre></div></div>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: Pytorch</title><link href="http://localhost:4000/aitech/post-pytorch/" rel="alternate" type="text/html" title="Post: Pytorch" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-pytorch</id><content type="html" xml:base="http://localhost:4000/aitech/post-pytorch/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="-introduction-to-pytorch">✅ Introduction to PyTorch</h1>
<p>밑바닥부터 딥러닝 코드 짜기를 짤 수도 있습니다. <a href="https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=256067157&amp;start=slayer">책</a>
하지만 저희는 딥러닝 프레임워크를 사용합니다. 예를 들어서 텐서플로와 파이토치가 있습니다. 텐서플로는 구글에서 만들었고, 파이토치는 페이스북에서 만들었습니다.</p>

<p>저희는 파이토치를 사용할 것입니다. 파이토치는 장점이 꽤 많습니다. 대표적으로 Define by Run 이라는 작동 방식인데요. 이것은 실행을 한면서 계산 그래프(Computational Graph)를 생성하는 것을 말합니다. 반면 텐서플로는 Define by Run 이라고 해서 그래프를 먼저 정의하는 방식입니다. 파이토치는 학회, 논문에 강점을 가지고 있으며 텐서플로는 실무(프로덕션)에 강점을 가지고 있다고 할 수 있죠. 그리고 파이토치는 즉시 딥러닝 코드가 작동하는지 확인을 할 수가 있으며 numpy, autograd, function 의 기능을 모두 제공합니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="--dive-to-pytorch">✅  Dive To Pytorch</h1>
<p>이제 파이토치 기본에 대해서 공부해 봅시다. 
먼저 autugrad 에 대해서 공부해봅시다. 
아래 코드는 파이토치를 통해 오차 역전파를 계산하는 식입니다.
backward 라는 함수는 연쇄법칙을 적용시켜 역전파를 계산합니다.
그러면 자연스래 w.grad 값을 구할 수 있겠죠.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w = torch.tensor(2.0, requires_grad=True)
y = w**2
z = 10*y + 25
z.backward()
w.grad

&gt; z.backward() 를 통해 오차역전파를 계산
&gt; w.grad 를 통해 z 에 대한 w 그레디언트 추출
</code></pre></div></div>

<h2 id="여기서-잠깐-">여기서 잠깐 !</h2>
<blockquote>
  <p>최대가능도 추정법이란 뭘까요? 관찰된 데이터가 어떠한 모수로부터 나왔을 가능성이 가장 높은지를 추정하는 방법입니다. 모수란 무엇일까요? 모수는 모집단의 특성(모평균,모분산 등..)을 나타내는 값으로, 이 값을 모집단을 전수조사해야만 알수있는 값입니다. 실질적으로 모집단의 크기와 범위가 너무 방대하기에 전수조사를 실시하지 않고 표본조사를 하는데 표본평균,표본분산 등으로 모평균, 모분산등을 추정할수가 있다.</p>
</blockquote>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-use-template">✅ Use Template</h1>
<p>여러분은 언제까지 코랩을 쓰실 생각입니까? 템플릿을 씁시다. 고수가 되는 지름길이죠. 우리는 미래로 가야합니다. 템플릿을 사용함으로써 우리는 그 미래로 갈 수 있습니다. <code class="language-plaintext highlighter-rouge">실행, 데이터, 모델,  설정, 로깅, 지표, 유틸리티</code> 이 대표적이죠.
다음 오픈 소스에서 파이토치 템플릿을 다운 받고 면밀하게 분석해보세요. 면밀하게 분석을 해봅시다. <a href="https://github.com/victoresque/pytorch-template">링크</a>
코딩 능력이 많이 필요하다는 것을 분명 느끼셨을 겁니다. 코딩 역량을 기르십시오.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-autograd--optimizer">✅ AutoGrad &amp; Optimizer</h1>
<p>논문을 한 번 구현해봅시다.
딥러닝이란 layer를 쌓고 레고 블럭을 쌓는 일련의 과정이라고 할 수 있습니다. input, output, forward, backward가 복잡한 상호작용을 가지고 있는 것이죠.</p>
<ul>
  <li>torch.nn.Module 클래스
    <ul>
      <li>이 모듈은 기본적인 input, output, forward, backward 구현을 지원합니다.</li>
    </ul>
  </li>
  <li>torch.nn.Parameter
    <ul>
      <li>required_grad=True 옵션을 줘야 gradient 가 최신화 됩니다.</li>
    </ul>
  </li>
</ul>

<p>파이토치를 사용함으로써 우리는 backward 함수를 직접 구현할 일은 없습니다. 하지만 실제 어떻게 작동하고 있는지 분명하게 이해할 필요가 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from torch.autograd import Variable

class LinearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(LinearRegression, self).__init__()

        ## 토치가 liner를 구현해줬다.
        ## 코드 레벨에서 직접 보시라.
        self.linear = torch.nn.Linear(inputSize, outputSize)

    def forward(self, x):
        out = self.linear(x)
        return out
    
inputDim = 1
outputDim = 1
learningRate = 0.01
epochs = 100

model = LinearRegression(inputDim, outputDim)

# 모델 학습 시킬 때 쿠다 넣기
if torch.cuda.is_available():
    model.cuda()


criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)


# 에포크를 돌리란 말이여!! 
for epochs in range(epochs):
    if torch.cuda.is_available():
        inputs = Variable(torch.from_numpy(x_train).cuda())
        labels = Variable(torch.from_numpy(y_train).cuda())
    else:
        inputs = Variable(torch.from_numpy(x_train))
        lables = Variable(torch.from_numpy(y_train))

    # 이걸 해줘야 한다.
    optimizer.zero_grad()

    ## 돌려돌려 
    outputs = model(inputs)


    ## 요것이 손실함수다!
    loss = criterion(outputs, lables)

    # 오케이. 경사하강법!
    loss.backward()

    # 오차역전파 적용 -&gt; 옵티마이저
    optimizer.step()


    print('epochs {}, loss {}'.format(epochs, loss.item()))


Output exceeds the size limit. Open the full output data in a text editor
epochs 0, loss 0.0023449501022696495
epochs 1, loss 0.0023187585175037384
epochs 2, loss 0.002292879391461611
epochs 3, loss 0.0022672710474580526
epochs 4, loss 0.0022419483866542578
epochs 5, loss 0.002216903492808342
epochs 6, loss 0.002192169427871704
epochs 7, loss 0.002167687052860856
epochs 8, loss 0.002143467077985406
epochs 9, loss 0.0021195358131080866
epochs 10, loss 0.0020958760287612677
epochs 11, loss 0.0020724674686789513
epochs 12, loss 0.0020493241026997566
epochs 13, loss 0.002026451053097844
epochs 14, loss 0.002003807807341218
epochs 15, loss 0.001981443725526333
epochs 16, loss 0.0019593113102018833
epochs 17, loss 0.0019374340772628784
epochs 18, loss 0.0019157928181812167
epochs 19, loss 0.0018943949835374951
epochs 20, loss 0.0018732366152107716
epochs 21, loss 0.0018523228354752064
epochs 22, loss 0.0018316390924155712
epochs 23, loss 0.0018111761892214417
epochs 24, loss 0.001790948212146759
...
epochs 95, loss 0.0008068863535299897
epochs 96, loss 0.0007978747016750276
epochs 97, loss 0.0007889581611379981
epochs 98, loss 0.0007801674073562026

</code></pre></div></div>

<ul>
  <li>코드레벨에서 보는 습관을 들여보세요.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>optimizer.zero_grad() 를 안쓰면 어떻게 될까?
- gradient 가 초기화가 되지 않으면 다음 학습에 영향을 주기 때문에 올바른 학습이 되지 않습니다.
</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-데이터를-불러오자-dataset--dataloader">✅ [데이터를 불러오자~] Dataset &amp; Dataloader</h1>
<p>모델도 중요하지만, 데이터를 어떻게 잘 다루느냐도 중요합니다. 엄청나게 많은 데이터가 있습니다. 이것을 어찌해야 할까요? 우리에겐 파이토치가 있습니다.</p>

<h2 id="pytorch-dataset">Pytorch Dataset</h2>
<p>먼저 데이터를 텐서로 바꾸고 DataLoder 로 모델에 먹이기(Feeding)</p>
<ol>
  <li>데이터 모으기</li>
  <li>data 받기</li>
  <li>transform : 데이터 전처리
    <ul>
      <li>ToTensor()</li>
      <li>CenterCrop()</li>
    </ul>
  </li>
</ol>

<p>파이토치의 DataSet을 사용함으로써 많은 장점을 가진다.</p>

<p>CustomDataset 와의 결합해봅시다. CustomDataset을 정의 하기 위해서 3개의 메소드를 반드시 정의해야 합니다.</p>

<ul>
  <li>init</li>
  <li>len</li>
  <li>getitem</li>
</ul>

<p>utils.data.Dataset 상속
이때 Datase$$t 클래스 사용</p>

<h2 id="pytorch-dataloader">Pytorch Dataloader</h2>
<p>이제 DataLoder 모듈을 배워봅시다. CustomDataset, 기타 옵션으로 실제 데이터에 적용하면 됩니다. 옵션으로 다양한 것이 있습니다. 미니배치 사이즈 등이 있겠죠. 코드를 직접 뜻으면서 DataLoader를 해보세요.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>실전 연습
Mnist 클론 코딩을 해보세요.
</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-save-model-모델을-저장하고-불러오기">✅ [Save Model] 모델을 저장하고 불러오기</h1>
<p>우리는 실제 라벨과 예측 라벨의 오차 함수의 그레디언트를 계산하고 오차역전파를 계산하여 W 가중치 텐서를 업데이트 시켰다고 가정합니다. 우리는 이 결과를 전달하려 한다.<strong>어떻게</strong> 하는게 좋을까요 ?</p>

<p><code class="language-plaintext highlighter-rouge">model.save()</code> 함수를 사용합시다. 이를 이용해서 모델의 형태와 파라메터를 저장할 수 있습니다.</p>

<p>저장법 2가지가 있습니다.</p>

<ol>
  <li>파라메터만 저장</li>
  <li>모델형테 + 파라메터 저장</li>
</ol>

<h2 id="check-point">check point</h2>
<p>중간중간에 loss 와 metric을 지속적으로 저장하는 것을 말합니다. 코드 중간에 pickle 형태로 저장하세요. 중간에 Earlystopping(조기 종료) 처리를 할 수 도 있습니다.</p>

<p>전이학습(Transfer learning) 을 위해 구글, 페이스북에서 학습한 모델을 가져오려고 합니다. 어떻게 할까요?  NLP 분야에서는 <code class="language-plaintext highlighter-rouge">Huggingface</code>가 대표적입니다.  <code class="language-plaintext highlighter-rouge">Freezing</code> 는 Transfer 를 적용시키기 위해서 일부분의 가중치를 학습 시키지 않는것을 말합니다. 트랜스퍼 러닝은 외부 모듈에서 전이학습을 가져오고 자신만의 layer를 추가 시켜 학습시키는 방식으로 하면 됩니다. 굉장히 많이 쓰니 집중해서 공부하시기 바랍니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-모니터링-합시다---monitoring-tools-for-pytorch">✅ 모니터링 합시다 - Monitoring tools for PyTorch</h1>

<p><code class="language-plaintext highlighter-rouge">Logging</code>이란 학습이 진행되면서 주요하다고 생각되는 지표들이나 학습의 상태 등 관련된 정보들을 기록하는 것을 말합니다.</p>

<p><code class="language-plaintext highlighter-rouge">Tensorboard</code>는 TensorFlow의 프로젝트로 만들어진 시각화 도구로 학습 그래프, Metric, 학습 결과의 시각화를 지원합니다.</p>

<p><code class="language-plaintext highlighter-rouge">Weight, Biases</code>는 머신러닝 실험을 원활하게 지원하기 위한 상용 도구로써, 협업, code versioning, 실험 결과 기록 등의 기능을 제공하는 시각화 툴을 말합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
import os
logs_base_dir = 'logs'
os.makedirs(logs_base_dir, exist_ok=True)

from torch.utils.tensorboard import SummaryWriter
import numpy as np

exp = f"{logs_base_dir}/ex3"
writer = SummaryWriter(exp)
for n_iter in range(100):
  writer.add_scalar('Loss/train', np.random.random(), n_iter)
  writer.add_scalar('Loss/test', np.random.random(), n_iter)
  writer.add_scalar('Accuracy/train', np.random.random(), n_iter)  
  writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
writer.flush()

%load_ext tensorboard
%tensorboard --logdir {"logs"}

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 분포를 본다.

from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter(logs_base_dir)
for i in range(10):
  x = np.random.random(1000)
  writer.add_histogram('distribution centers', x+i, i)
writer.close()

</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-muti-gpu">✅ Muti-GPU</h1>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-hyperparameter-tuning">✅ Hyperparameter Tuning</h1>
<p>제대로 학습하기 위해선 하이퍼 파라메터 튜닝을 해야 합니다. 하이퍼 파라메티는 dl 이 학습하지 않는 변수값이죠.</p>

<ul>
  <li>학습률(Learning rate)</li>
  <li>최적화 함수(Optimizer)</li>
  <li>손실 함수(Loss Fucnction)</li>
</ul>

<p>잘 정리하고 잘 변수를 조정해야 합니다. 데이터와 모델을 수정해서 성능을 향상하는 것이 가장 빠르긴 하지만 하이퍼 파라메터를 튜닝하는 힘은 분명 필요합니다. 요즘들어서는 그렇게 많이하는 추세는 아닙니다. 아래 두 가지 방법이 있습니다.</p>

<ul>
  <li>Grid LayOut
    <ul>
      <li>균일적으로 파라메터를 변화시켜나가는 방법</li>
    </ul>
  </li>
  <li>Random LayOut
    <ul>
      <li>랜덤적으로 파라메터를 변화시켜나가는 방법</li>
    </ul>
  </li>
</ul>

<p>Ray는 멀티 노드 멀티 프로세스 지원 모듈이다. 분산병령 ml/dl 의 표준이라 할 수 있다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>하이퍼 파라메터 튜닝은 어느정도 학습 모델을 완전하게 만들어 놓고 수정하는 것을 추천합니다. 데이터와 모델에 집중하세요.
</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="-pytorch-troubleshooting">✅ PyTorch Troubleshooting</h1>
<p>OOM(Out Of Memory) 란 무엇일까요. GPU 메모리 오류를 말합니다. 
이를 방지하는 방법은 두가지가 있습니다. 하나는 batchSize를 줄이는 것이고, 둘째는 memory를 줄이는 방법입니다. <code class="language-plaintext highlighter-rouge">GPUutil</code> 모듈을 사용하세요. 사용하지 않는 gpu cache 를 삭제해야 합니다. <code class="language-plaintext highlighter-rouge">torch.no_grad()</code> 를 사용도 좋습니다. tensor의 float precision을 줄여도 된다.</p>

<p>하지만 결국 중요한 것은 발생할 때마다 직접 트러블 슈팅을 다루는 능력입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>밑바닥 부터 시작하는 딥러닝 3 추천합니다.
</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/image/aitech.png" /><media:content medium="image" url="http://localhost:4000/image/aitech.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Post: Python 정리</title><link href="http://localhost:4000/aitech/post-python/" rel="alternate" type="text/html" title="Post: Python 정리" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-python</id><content type="html" xml:base="http://localhost:4000/aitech/post-python/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<script src="https://gist.github.com/yunjinchoidev/5422bce878af5cfa31bf644662ea15e3.js"></script>

<h1 id="판다스">판다스</h1>
<p>실전 데이터는 깨끗한 데이터가 없습니다. 우리는 판다스가 가진 힘을 적극적으로 활용하여 데이터를 가공 해야 합니다. 좋은 데이터는 좋은 학습 결과를 만듭니다. 이것이 우리가 추구하는 바입니다. 판다스는 굉장히 많은 함수를 지원하고 있습니다. 이것을 모두 기억해야 하냐고요? 그렇지는 않습니다. 레퍼런스를 참고하면 되기 때문이죠. 다만 이것을 자유롭게 활용 할 수 있는 능력을 가지고 있어야 합니다. 실전 데이터를 가지고 가공하면서 판다스를 연습해보세요. 그것이 실력을 기르는 최선의 지름길입니다. $$</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech 1주차</title><link href="http://localhost:4000/aitech/post-1%EC%A3%BC%EC%B0%A8/" rel="alternate" type="text/html" title="Post: ai tech 1주차" /><published>2023-03-06T00:00:00+09:00</published><updated>2023-03-09T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-1%EC%A3%BC%EC%B0%A8</id><content type="html" xml:base="http://localhost:4000/aitech/post-1%EC%A3%BC%EC%B0%A8/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="1주차">1주차</h1>
<ul>
  <li>강의 정리
    <ul>
      <li><a href="https://yunjinchoidev.github.io/aitech/post-python/">파이썬 강의록 정리</a></li>
      <li><a href="https://yunjinchoidev.github.io/aitech/post-aimath/">AIMath 강의록 정리</a></li>
      <li><a href="https://yunjinchoidev.github.io/aitech/post-pytorch/">Pytorch 강의록 정리</a></li>
    </ul>
  </li>
</ul>

<h1 id="1주차-계획">[1주차 계획]</h1>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />python, aimath, pytorch 완강</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />과제 완수</li>
</ul>

<h1 id="1주차를-보내며">[1주차를 보내며]</h1>
<p>1주차를 보내며 온보딩기간 동안 공부했던 내용을 재복습하면서 과제를 풀었습니다. 과제를 풀며 든 생각은 부스트캠프가 정말 심혈을 기울여 제출했다는 것입니다. 문제에 정성과 의도가 분명히 느껴져서 열심히 참여했습니다. 심화 과제의 경우 난이도가 높았던 만큼 많은 도움이 되었습니다.</p>

<p>좋은 조원들을 만났습니다. 팀원분들 모두 생산적인 미래과 꿈을 가지고 있는 보였고 많은 자극을 받았습니다. 좋은 분들과 함께 해서 다행이라고 느꼈고 그 많큼 좋은 결과를 가졌으면 좋겠습니다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - Day01</title><link href="http://localhost:4000/aitech/post-day01/" rel="alternate" type="text/html" title="Post: ai tech - Day01" /><published>2023-03-06T00:00:00+09:00</published><updated>2023-03-09T07:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day01</id><content type="html" xml:base="http://localhost:4000/aitech/post-day01/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<ul>
  <li>OT 참여</li>
  <li>python 강의 수강</li>
  <li>파이토치 강의 1~5강 수강</li>
</ul>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry></feed>