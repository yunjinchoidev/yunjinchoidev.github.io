<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-08T15:24:09+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Reinvent love! - Democratization of Love</title><subtitle>NLP 쪼렙입니다.</subtitle><author><name>최윤진</name></author><entry><title type="html">Post: ai tech -Day03</title><link href="http://localhost:4000/aitech/post-day03/" rel="alternate" type="text/html" title="Post: ai tech -Day03" /><published>2023-03-08T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day03</id><content type="html" xml:base="http://localhost:4000/aitech/post-day03/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<ul>
  <li>python 복습</li>
  <li>aimath 1 ~ 5 강</li>
  <li>pytorch template 분석</li>
  <li></li>
</ul>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - pytorch Template 완전 분석</title><link href="http://localhost:4000/aitech/post-pytorchtemplate/" rel="alternate" type="text/html" title="Post: ai tech - pytorch Template 완전 분석" /><published>2023-03-08T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-pytorchtemplate</id><content type="html" xml:base="http://localhost:4000/aitech/post-pytorchtemplate/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="pytorch-템플릿을-문석해봅시다">Pytorch 템플릿을 문석해봅시다.</h1>
<p><a href="https://github.com/victoresque/pytorch-template">링크</a>
 여기 오픈 소스가 하나 있습니다. 이 포스팅을 통해 이 템플릿을 분해, 분석 해보는 시간을 가지려고 합니다.</p>

<p><img src="../../../image/aitech/pytorchtemplatetree.png" alt="파일 " /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* train.py              -&gt; 실행
* test.py               -&gt; 실행
* config                -&gt; 설정
* parse_config          -&gt; 설정
* base                  -&gt; base 모델
* data_loader ~         -&gt; data 
* model ~               -&gt; 모델
* save ~                -&gt; 저장소
* trainer ~             -&gt; 학습
* logger ~              -&gt; 로깅
* utils ~               -&gt; 유틸
</code></pre></div></div>

<hr />
<h2 id="실행-방법입니다">실행 방법입니다</h2>
<p>루트 디렉토리에서 <code class="language-plaintext highlighter-rouge">python train.py -c config.json</code> 을 치세요.</p>

<h2 id="분해">분해</h2>
<ul>
  <li>train.py
    <ul>
      <li>arg.add_argument</li>
    </ul>
  </li>
  <li>config.json
    <ul>
      <li>util의 <code class="language-plaintext highlighter-rouge">read_josn</code> 을 보라.</li>
    </ul>
  </li>
  <li>train/tariner.py</li>
  <li>baser/base_tariner.py
    <ul>
      <li>학습의 원천이 되는 소스</li>
    </ul>
  </li>
</ul>

<h2 id="아하-그러니까요">아하 그러니까요.</h2>
<p>스프링 부트와 같은 웹 프레임워크를 사용하신 분들에는 익숙한 구조일겁니다.
가령 우리는 yml 파일을 수정함으로써 configuration을 바꿉니다. 소스 전체를 바꾸는 것이 아니라 특정 config 요소만 바꿈으로써 소스 전체에 영향을 끼치는 겁니다.
이것이 템플릿, 프레임워크의 힘의 근원이라고 할 수 있는 것이죠.</p>

<p>pytorch-template 도 같은 시각에서 접근해봅시다. 
config.json 만 바꿈으로써 우리는 새로운 데이터를 로딩할 수 있는 것이고 batch_size, validation_split 도 변경 할 수 가 있습니다. 
train.py 만 바꿈으로써 우리는 모델을 손쉽게 교체할 수 가 있습니다.</p>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: Python 정리</title><link href="http://localhost:4000/aitech/post-python/" rel="alternate" type="text/html" title="Post: Python 정리" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-python</id><content type="html" xml:base="http://localhost:4000/aitech/post-python/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<script src="https://gist.github.com/yunjinchoidev/5422bce878af5cfa31bf644662ea15e3.js"></script>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: Pytorch</title><link href="http://localhost:4000/aitech/post-pytorch/" rel="alternate" type="text/html" title="Post: Pytorch" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-pytorch</id><content type="html" xml:base="http://localhost:4000/aitech/post-pytorch/"><![CDATA[<h1 id="1--introduction-to-pytorch">1.  Introduction to PyTorch</h1>
<p>밑바닥부터 딥러닝 코드 짜기를 짤 수도 있습니다. <a href="https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=256067157&amp;start=slayer">책</a>
하지만 저희는 딥러닝 프레임워크를 사용합니다. 예를 들어서 텐서플로와 파이토치가 있습니다. 텐서플로는 구글에서 만들었고, 파이토치는 페이스북에서 만들었습니다.</p>

<p>저희는 파이토치를 사용할 것입니다. 파이토치는 장점이 꽤 많습니다. 대표적으로 Define by Run 이라는 작동 방식인데요. 이것은 실행을 한면서 계산 그래프(Computational Graph)를 생성하는 것을 말합니다. 반면 텐서플로는 Define by Run 이라고 해서 그래프를 먼저 정의하는 방식입니다. 파이토치는 학회, 논문에 강점을 가지고 있으며 텐서플로는 실무(프로덕션)에 강점을 가지고 있다고 할 수 있죠. 그리고 파이토치는 즉시 딥러닝 코드가 작동하는지 확인을 할 수가 있으며 numpy, autograd, function 의 기능을 모두 제공합니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="2-dive-to-pytorch">2. Dive To Pytorch</h1>
<p>이제 파이토치 기본에 대해서 공부해 봅시다. 
먼저 autugrad 에 대해서 공부해봅시다. 
아래 코드는 파이토치를 통해 오차 역전파를 계산하는 식입니다.
backward 라는 함수는 연쇄법칙을 적용시켜 역전파를 계산합니다.
그러면 자연스래 w.grad 값을 구할 수 있겠죠.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w = torch.tensor(2.0, requires_grad=True)
y = w**2
z = 10*y + 25
z.backward()
w.grad

&gt; z.backward() 를 통해 오차역전파를 계산
&gt; w.grad 를 통해 z 에 대한 w 그레디언트 추출
</code></pre></div></div>

<h2 id="여기서-잠깐-">여기서 잠깐 !</h2>
<blockquote>
  <p>최대가능도 추정법이란 뭘까요? 관찰된 데이터가 어떠한 모수로부터 나왔을 가능성이 가장 높은지를 추정하는 방법입니다. 모수란 무엇일까요? 모수는 모집단의 특성(모평균,모분산 등..)을 나타내는 값으로, 이 값을 모집단을 전수조사해야만 알수있는 값입니다. 실질적으로 모집단의 크기와 범위가 너무 방대하기에 전수조사를 실시하지 않고 표본조사를 하는데 표본평균,표본분산 등으로 모평균, 모분산등을 추정할수가 있다.</p>
</blockquote>

<p><br />
<br />
<br />
<br /></p>

<h1 id="템플릿을-사용합시다">템플릿을 사용합시다.</h1>
<p>여러분은 언제까지 코랩을 쓰실 생각입니까? 템플릿을 씁시다. 고수가 되는 지름길이죠. 우리는 미래로 가야합니다. 템플릿을 사용함으로써 우리는 그 미래로 갈 수 있습니다. <code class="language-plaintext highlighter-rouge">실행, 데이터, 모델,  설정, 로깅, 지표, 유틸리티</code> 이 대표적이죠.
다음 오픈 소스에서 파이토치 템플릿을 다운 받고 면밀하게 분석해보세요. 면밀하게 분석을 해봅시다. <a href="https://github.com/victoresque/pytorch-template">링크</a>
코딩 능력이 많이 필요하다는 것을 분명 느끼셨을 겁니다. 코딩 역량을 기르십시오.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="autograd--optimizer">AutoGrad &amp; Optimizer</h1>
<p>논문을 한 번 구현해봅시다.
딥러닝이란 layer를 쌓고 레고 블럭을 쌓는 일련의 과정이라고 할 수 있습니다. input, output, forward, backward가 복잡한 상호작용을 가지고 있는 것이죠.</p>
<ul>
  <li>torch.nn.Module 클래스
    <ul>
      <li>이 모듈은 기본적인 input, output, forward, backward 구현을 지원합니다.</li>
    </ul>
  </li>
  <li>torch.nn.Parameter
    <ul>
      <li>required_grad=True 옵션을 줘야 gradient 가 최신화 됩니다.</li>
    </ul>
  </li>
</ul>

<p>파이토치를 사용함으로써 우리는 backward 함수를 직접 구현할 일은 없습니다. 하지만 실제 어떻게 작동하고 있는지 분명하게 이해할 필요가 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from torch.autograd import Variable

class LinearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(LinearRegression, self).__init__()

        ## 토치가 liner를 구현해줬다.
        ## 코드 레벨에서 직접 보시라.
        self.linear = torch.nn.Linear(inputSize, outputSize)

    def forward(self, x):
        out = self.linear(x)
        return out
    
inputDim = 1
outputDim = 1
learningRate = 0.01
epochs = 100

model = LinearRegression(inputDim, outputDim)

# 모델 학습 시킬 때 쿠다 넣기
if torch.cuda.is_available():
    model.cuda()


criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)


# 에포크를 돌리란 말이여!! 
for epochs in range(epochs):
    if torch.cuda.is_available():
        inputs = Variable(torch.from_numpy(x_train).cuda())
        labels = Variable(torch.from_numpy(y_train).cuda())
    else:
        inputs = Variable(torch.from_numpy(x_train))
        lables = Variable(torch.from_numpy(y_train))

    # 이걸 해줘야 한다.
    optimizer.zero_grad()

    ## 돌려돌려 
    outputs = model(inputs)


    ## 요것이 손실함수다!
    loss = criterion(outputs, lables)

    # 오케이. 경사하강법!
    loss.backward()

    # 오차역전파 적용 -&gt; 옵티마이저
    optimizer.step()


    print('epochs {}, loss {}'.format(epochs, loss.item()))


Output exceeds the size limit. Open the full output data in a text editor
epochs 0, loss 0.0023449501022696495
epochs 1, loss 0.0023187585175037384
epochs 2, loss 0.002292879391461611
epochs 3, loss 0.0022672710474580526
epochs 4, loss 0.0022419483866542578
epochs 5, loss 0.002216903492808342
epochs 6, loss 0.002192169427871704
epochs 7, loss 0.002167687052860856
epochs 8, loss 0.002143467077985406
epochs 9, loss 0.0021195358131080866
epochs 10, loss 0.0020958760287612677
epochs 11, loss 0.0020724674686789513
epochs 12, loss 0.0020493241026997566
epochs 13, loss 0.002026451053097844
epochs 14, loss 0.002003807807341218
epochs 15, loss 0.001981443725526333
epochs 16, loss 0.0019593113102018833
epochs 17, loss 0.0019374340772628784
epochs 18, loss 0.0019157928181812167
epochs 19, loss 0.0018943949835374951
epochs 20, loss 0.0018732366152107716
epochs 21, loss 0.0018523228354752064
epochs 22, loss 0.0018316390924155712
epochs 23, loss 0.0018111761892214417
epochs 24, loss 0.001790948212146759
...
epochs 95, loss 0.0008068863535299897
epochs 96, loss 0.0007978747016750276
epochs 97, loss 0.0007889581611379981
epochs 98, loss 0.0007801674073562026

</code></pre></div></div>

<ul>
  <li>코드레벨에서 보는 습관을 들여보세요.</li>
</ul>

<h2 id="optimizerzero_grad-를-안쓰면-어떻게-될까">optimizer.zero_grad() 를 안쓰면 어떻게 될까?</h2>
<ul>
  <li>gradient 가 초기화가 되지 않으면 다음 학습에 영향을 주기 때문에 올바른 학습이 되지 않습니다.</li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="dataset--dataloader">Dataset &amp; Dataloader</h1>
<p>모델도 중요하지만, 데이터를 어떻게 잘 다루느냐도 중요합니다. 엄청나게 많은 데이터가 있습니다. 이것을 어찌해야 할까요? 우리에겐 파이토치가 있습니다.</p>
<h2 id="pytorch-dataset">Pytorch Dataset</h2>
<p>먼저 데이터를 텐서로 바꾸고 DataLoder 로 모델에 먹이기(Feeding)</p>
<ol>
  <li>데이터 모으기</li>
  <li>data 받기</li>
  <li>transform : 데이터 전처리
    <ul>
      <li>ToTensor()</li>
      <li>CenterCrop()
파이토치의 DataSet을 사용함으로써 많은 장점을 가진다.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>CustomDataset 와의 결합
    <ul>
      <li>CustomDataset을 정의 하기 위해서 3개의 메소드를 반드시 정의해야 한다.
        <ul>
          <li>init, len, getitem</li>
          <li>utils.data.Dataset 상속</li>
        </ul>
      </li>
      <li>이때 Dataset 클래스 사용</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h2 id="pytorch-dataloader">Pytorch Dataloader</h2>
<ul>
  <li>DataLoder
    <ul>
      <li>CustomDataset, 기타 옵션으로 실제 데이터에 적용하면 된다.</li>
      <li>옵션
        <ul>
          <li>미니배치 사이즈</li>
          <li>… 많다.</li>
        </ul>
      </li>
      <li>코드를 뜯어보시라.</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>실전 연습
Mnist 클론 코딩을 해보시라 ~ 

</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="모델-불러오기">모델 불러오기</h1>
<ul>
  <li>상황
    <ul>
      <li>이 말인 즉슨 우리는 실제 라벨과 예측 라벨의 오차 함수의 그레디언트를 계산하고 오차역전파를 계산하여 W 가중치 텐서를 업데이트 시켰다. (공부를 했다. 경험을 했다. 수련을 했다.)</li>
      <li>우리는 이 결과를 전달하려 한다.</li>
    </ul>
  </li>
  <li>model.save()</li>
  <li>저장법 2가지
    <ul>
      <li>파라메터만 저장</li>
      <li>모델형테 + 파라메터 저장</li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="모니터링-합시다---monitoring-tools-for-pytorch">모니터링 합시다 - Monitoring tools for PyTorch</h1>
<ul>
  <li>체크포인트
    <ul>
      <li>학습의 중간 결과를 저장</li>
      <li>earlystopping</li>
      <li>loss, metric</li>
      <li>Earlystopping : 조기 종료</li>
    </ul>
  </li>
  <li>Loggin
    <ul>
      <li>Logging이란, 학습이 진행되면서 주요하다고 생각되는 지표들이나 학습의 상태 등 관련된 정보들을 기록하는 것</li>
    </ul>
  </li>
  <li>Tensorboard</li>
  <li>
    <p>Tensorboard는 TensorFlow의 프로젝트로 만들어진 시각화 도구로 학습 그래프, Metric, 학습 결과의 시각화를 지원</p>
  </li>
  <li>Weight, Biases
    <ul>
      <li>머신러닝 실험을 원활하게 지원하기 위한 상용 도구로써, 협업, code versioning, 실험 결과 기록 등의 기능을 제공하는 시각화 툴</li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="muti-gpu">Muti-GPU</h1>

<p><br />
<br />
<br />
<br /></p>

<h1 id="hyperparameter-tuning">Hyperparameter Tuning</h1>
<ul>
  <li>학습률(Learning rate)</li>
  <li>최적화 함수(Optimizer)</li>
  <li>손실 함수(Loss Fucnction)</li>
  <li></li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="pytorch-troubleshooting">PyTorch Troubleshooting</h1>

<p><br />
<br />
<br />
<br /></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="생각해볼점">생각해볼점</h1>
<ul>
  <li>AutoGrad 작동 방식 을 뜯어봅시다.</li>
  <li></li>
</ul>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[1. Introduction to PyTorch 밑바닥부터 딥러닝 코드 짜기를 짤 수도 있습니다. 책 하지만 저희는 딥러닝 프레임워크를 사용합니다. 예를 들어서 텐서플로와 파이토치가 있습니다. 텐서플로는 구글에서 만들었고, 파이토치는 페이스북에서 만들었습니다.]]></summary></entry><entry><title type="html">Post: ai tech - Day02</title><link href="http://localhost:4000/aitech/post-day02/" rel="alternate" type="text/html" title="Post: ai tech - Day02" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day02</id><content type="html" xml:base="http://localhost:4000/aitech/post-day02/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">딥러닝에 필요한 수학적 지식</title><link href="http://localhost:4000/aitech/post-aimath/" rel="alternate" type="text/html" title="딥러닝에 필요한 수학적 지식" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-aimath</id><content type="html" xml:base="http://localhost:4000/aitech/post-aimath/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="벡터">벡터</h1>
<p>벡터란 뭡니까? 스칼라 다음의 차원을 표시하기 위한 수학 도구입니다.
$L1_1$노름은 변화량읠 절대값을, $L1_2$노름은 유클리드 거리를 말합니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="행렬">행렬</h1>

<p>행렬에 대해서 알아봅시다. 행렬곱은 행과 열의 조건을 맞춰야 연산가능합니다. 행렬의 곱은 교환법칙이 성립하지 않으니 유의해야 합니다. 
파이썬에서는 @ 기호를 통해 행렬의 곱을 지원합니다. <code class="language-plaintext highlighter-rouge">np.inner</code> 도 가능합니다.(자동 transpose 지원) <br />
역행렬은 곱의 연산을 했을 때 항등행렬이 나오는 행렬을 말합니다. 곱의 역원을 의미합니다. <br />
역행렬은 선형사상과 필요충분조건입니다. (선형대수학에 대한 지식이 필요합니다.) <code class="language-plaintext highlighter-rouge">np.linalg.pinv</code> 모듈을 이용해서 구할 수 있습니다 <br />
유사 역행렬에는 무어펜로스(Moore-Penrose) 역행렬이 있습니다. 왜냐하면 역행렬을 구하는 것이 쉽지 않기 때문입니다. <br />
<img src="../../../image/aitech/무어펜로즈.png" alt="무어펜로즈.png" /></p>

<p>행렬은 왜 배우는 것일까요? 연립방정식, 선형회귀 분석에 응용됩니다.<code class="language-plaintext highlighter-rouge">차원이동</code> <code class="language-plaintext highlighter-rouge">연산</code> 을 쉽게 만들어 주기 때문이죠. <br />
딥러닝을 제대로 이해하기 위해선 ‘선형대수학’을 수준 높게 학습해야 합니다.
사이킷런의 선형회귀 모델과 무어펜로즈의 선형회귀를 직접 구현보시는 건 어떻습니까? <br /></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="경사하강법-gradinet-descent">경사하강법 (Gradinet Descent)</h1>
<p>경사하강법에 대해서 알아봅시다. 우선 기초적인 미분에 대해서 알아볼까요?</p>

<h2 id="미분">미분</h2>
<p>먼저 미분에 대해서 알아봅시다.
미분이란 접선의 기울기를 말하죠. 
<code class="language-plaintext highlighter-rouge">sysmpy.diff</code> 모듈을 이용하여 미분계산이 가능합니다.
이를 통해서 우리는 주어진 input 데이터가 **<어느 차원에서든="">** 그 점에서 증가하는가, 감소하는가를 알 수 있습니다.
어느 차원으로 확장한다는 것은 변수를 스칼라가 아닌 벡터와 텐서를 사용한다는 것입니다.</어느></p>

<p>좋습니다. 이제 경사하강법을 알아볼까요? 
경사하강법이란 기울기가 감소하는 방향으로 쭈욱 가다 <strong>보면</strong> 언젠가 평지를(극소값)을 만날 것입니다. 컴퓨터에서 미분값이 0 인 곳을 찾기란 쉽지 않으므로 아주 작은 값(오메가) 보다 더 미분 값이 작으면 종료하면 됩니다. 이것이 간략한 설명입니다.
이 때 중요한 것은 어느 만큼의 보폭으로 갈 것이냐입니다. 이것은 하이퍼 파라메터로 즉 학습률입니다.
변수가 스칼라가 아니라 벡터라면 편미분을 하면 됩니다. 
모든 데이터를 사용하는 것은 고전적인 방법입니다. 이것은 성능이 좋지 않고 하드웨어에 부담이 갑니다. 시간도 많이 걸리죠. 
조금의 데이터만 이용해서 경사를 갱신하는 것을 SGD 라고 합니다. 이 방법은 최근 많이 사용하는 방법입니다.</p>

<h2 id="그레디언트-벡터gradient-vector">그레디언트 벡터(gradient Vector)</h2>
<p>함수의 벡터 변수 별로 편미분을 계산한 함수를 말합니다.
계산식은 이렇게 됩니다. 
<img src="../../../image/aitech/gradient.png" alt="gradient" />
그레디언트 벡터에 - 를 붙여서 이동하게 되면 <strong>가장 빨리</strong> 극소값을 향해 가게 되는 겁니다.</p>
<ul>
  <li>그레디언트 벡터의 <code class="language-plaintext highlighter-rouge">norm</code> 값을 구해서 일정 값보다 작게 되면 학습을 종료하는 방식으로 극소값을 찾으면 된다. (계속 내려가다가 더 이상 변화가 없는 것 같아. -&gt; 일단 멈춰!)</li>
</ul>

<h1 id="선형회귀-모델에서-경사하강법-적용해보기">선형회귀 모델에서 경사하강법 적용해보기</h1>
<p>선형 모델을 구할 때 무어 펜로즈 행렬을 사용할수 도 있습니다. <br />
하지만 우리는 무어펜로즈 행렬을 곱해서 나온 우변 식을 경사하강법을 하지 않고 오직 경사 하강법을 적용시켜 극소값이 되는 지점을 찾는 것이다.<br />
$y-y^{^}$ 의 값을 미분해서 학습률을 곱해 에포크동안 최소화 지키다는 것입니다. <br /></p>

<p>직접 계산해봅시다. 학습률과 학습횟수를 적절하게 선택했을 때만 수렴을 보장할 수 있습니다. 비선형회귀의 경우, 볼록하지 않기 때문에 수렴을 보장할 수 없다. (왜 불가능한가? 고민해봅시다.)</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="미니배치-확률적-경사하강법sgd">미니배치 확률적 경사하강법(SGD)</h1>
<p>데이터를 일부 사용해서 경사하강법을 적용하는 방식을 말합니다. 경험적으로 좋다는 것이 밝혀져 있습니다.  SGD 가 경사하강법보다 낫다는 것이 실증적으로 검증되었다. 데이터를 나눠서 epoch 를 반복하여 학습하는 것을 말합니다. 데이터를 일부로 사용하기 때문에 목적식이 매번 달라집니다.
학습률, 미니배치사이즈가 하이퍼 파라메터입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="딥러닝-학습방법">딥러닝 학습방법</h1>
<p>이제 비선형모델인 신경망을 도전해봅시다.먼저 소프트 맥스 함수 부터 알아보지요.</p>

<h2 id="소프트맥스">소프트맥스</h2>
<p>모델의 출력을 확률로 해석하여 확률 벡터로 변환하는 함수입니다. 가장 큰 값이 예측값이 되는 것입니다.
신경망은 <strong>선형모델과 활성함수를 합성한 함수</strong>입니다. 이러한 layer들은 잠재벡터들의 누적하게 되고 마지막 공간에 활성화 함수를 적용하는 것입니다.</p>
<h2 id="활성함수">활성함수</h2>
<p>R 위에 정의된 비선형 함수입니다. 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없습니다. 대표적으로 Relu, tanh, sigmoid 가 있습니다.
순전파는 주어진 신경망 계산을 하는 것을 말합니다. 이 때 우리는 왜 여러 계층을 사용하는 걸까요? 왜 층을 여러개 쌓는가? 수학적으로 계층이 3개 이상만 있으면 임의의 연속함수를 근사할 수 있다는 것이 증명되어 있다. ‘세계’를 표현할 수 있는 것이죠.  역전파는 경사하강법을 이용해서 가중치를 업데이트 하는 것을 말합니다.  위층부터 저층으로 그레디어트 벡터를 전달해야 함(연쇄법칙) 메모리에 저장해서 사용해야 합니다. tensorflow, pytorch 에는 자동 구현되어 있습니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="확률론">확률론</h1>
<p>확률론은 딥러닝에서 대단히 중요합니다. 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.
확률론이 중요한 이유를 알아봅시다.  <code class="language-plaintext highlighter-rouge">분류</code> 문제 에서 예측 오차의 분산을 최소화하고 <code class="language-plaintext highlighter-rouge">교차 엔트로피</code> 문제에서 모델 예측의 불확실성을 최소화하는데 쓰입니다. 확률 분포는 데이터의 초상화입니다. 확률은 면적입니다. 이산 확률 변수에서는 시그마 급수, 연속 확률 변수에서는 연속 확률을 적분하는 것을 말합니다. 목표는 데이터의(표본) 확률 분포를 가지고 -&gt; 실제 (모집단)의 확률 분포를 추정하는 것입니다. 
<strong>몬테카를로 샘플링</strong>은 데이터(샘플링)를 통해 기댓값을 계산하는 방법을 말합니다. 대수의 법칙을 통해 수렴성을 보장합니다.</p>

<ul>
  <li>주변확률분포</li>
  <li>통계값(모수)
    <ul>
      <li>기댓값</li>
      <li>분산</li>
      <li>첨도</li>
      <li>공분산</li>
      <li>조건부확률</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np

def monte(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high - low)
    stat = []

    for _ in range(repeat):
        x = np.random.uniform(low=low, high=high, size=sample_size)
        fun_x = fun(x)
        int_val = int_len * np.mean(fun_x)
        stat.append(int_val)

    return np.mean(stat), np.std(stat)


- 주어진 함수
def f_x(x):
    return np.exp(-x**2)


print(monte(f_x, low=-1, high=1, sample_size=1000, repeat=100))

</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="통계학">통계학</h1>
<ul>
  <li>모수
    <ul>
      <li>통계적 모델링 : 확률분포를 추정하는 것.</li>
      <li>모수 추정
        <ul>
          <li>모수적 방법론
            <ul>
              <li>특정 확률 분포를 가정 후 모수를 추정하는 것.</li>
            </ul>
          </li>
          <li>비 모수적 방법론
            <ul>
              <li>가정 없이.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>데이터 관찰 -&gt; 확률 분포 가정
    <ul>
      <li>베르누이 분포</li>
      <li>카테고리 분포</li>
      <li>베타 분포</li>
      <li>감마분포, 로그정규분포</li>
      <li>정규분포, 라플라스분포</li>
    </ul>
  </li>
  <li>데이터 관찰 -&gt; 확률 분포 가정 -&gt; 평균, 분산 추정 -&gt; 모수 집단 추청</li>
  <li>중심극한정리
    <ul>
      <li>표뵨 평균의 표집분포는 N이 커질수록 정규분포를 따른다.</li>
    </ul>
  </li>
  <li>표집분포
    <ul>
      <li>통계량의 확률분포</li>
    </ul>
  </li>
  <li>최대가능도 추정법
    <ul>
      <li><strong>가장 가능성이 높은 모수를 추정하는 방법</strong></li>
      <li>x 에서 가장 높은 모수 집단 (가능도와 관점의 차이)</li>
      <li>기계학습에서 많이 쓰인다.</li>
      <li>가능도
        <ul>
          <li><strong>모수 쌔타 분포에서 데이터 x 를 발견할 가능성.</strong></li>
          <li>계산상 <code class="language-plaintext highlighter-rouge">로그가능도</code> 사용</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>정규분포에서 최대 가능도 추정</li>
  <li>카테고리 분포에서 최대 가능도 추정</li>
  <li>딥러닝에서 최대가능도 추정법
    <ul>
      <li>소프트맥스 벡터 =&gt; 카테고리 분포</li>
      <li>두 개의 확률분포의 손실함부를 학습시키기
        <ul>
          <li>쿨백-라이블러 발산을 최소화</li>
          <li>분류 문제에서 정답레이블을 P, 모델 예측을 Q 라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>최대 우도법
    <ul>
      <li>데이터가 많이 모인 상황에서 <code class="language-plaintext highlighter-rouge">특정 데이터의 분포</code> 를 가정하고 <code class="language-plaintext highlighter-rouge">제일 가능성 높은</code> 모수를 추정하는 것.</li>
      <li>이 데이터들은 가장 그럴싸한 모수로부터 나왔을 것이다!</li>
      <li>정답 확률분포와 모델 추정 확률분포 손실함수 구해서 학습 시키는 방식으로 구하면 된다.</li>
      <li>확률과 가능도의 차이
        <ul>
          <li>확률 : 이 사건이 일어날 경우의 수를 전체 사건의 수로 나눈 것.</li>
          <li>가능도 : 지금 얻은 데이터가 이 분포에서 나왔을 가능도</li>
        </ul>
      </li>
      <li>가능도를 사용하는 이유는 우리는 모수를 알 수 없기 때문이다. 데이터로부터 실제 세계를 추정해야 한다. 그게 DL 이다.</li>
      <li>DL 에서 많이 쓰이니 잘 기억하자. 왜냐하면 가중치를 결정하는 데 도움을 주기 때문 ?</li>
      <li><a href="https://www.youtube.com/watch?v=XhlfVtGb19c">설명링크</a></li>
      <li><a href="https://angeloyeo.github.io/2020/07/17/MLE.html">설명블로그</a></li>
      <li>재요약
        <ul>
          <li>특정한 분포 $\theta$ 에서 관찰된 데이터 집합 x 의 가능도는 확률의 곱이라 할 수 있음.</li>
          <li>가능도 값이 최대가 되도록 하는 $\theta$ 를 찾자. 그것이 최대 우도법이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>쿨백-라이블러 발산
    <ul>
      <li><strong>쿨백-라이블러 발산(Kullback-Leibler divergence)</strong>은 두 확률분포 p(y), q(y) 의 분포모양이 얼마나 다른지를 숫자로 계산한 값</li>
      <li>그 값은 항상 양수이며 두 확률분포 완전히 같을 경우에만 0이 된다.</li>
      <li>교차엔트로피 개념</li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="베이즈-통계학">베이즈 통계학</h1>
<ul>
  <li>조건부 확률을 이용하면 정보를 갱신하는 방법을 알 수 있음.</li>
  <li>A 라는 사건이 추가로 주어졌을 때 B 가 일어날 확률은
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>B가 일어날 확률 * (P(A</td>
              <td>B) / P(A))</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>사후확률 = 사전확률 * 가능도/증거</li>
  <li>1종 오류, 2종 오류</li>
  <li>갱신된 사후확률을 계산 할 수 있다. (연속된 계산) (ex. 두번 검진)
    <ul>
      <li>정확도를 높일 수 있다.</li>
    </ul>
  </li>
  <li>조건부 확률을 토대로 인과관계를 함부로 추론해서는 안된다.
    <ul>
      <li>데이터에 따라 달라질 수 있기 때문임.</li>
      <li>중첩효과를 제거해서 가짜 연관관계를 제거해야 함.</li>
      <li>인과관계를 뒷받침하는 모델로서 작동하는 조건부확률</li>
    </ul>
  </li>
  <li>상관관계와 인과관계는 다르다.
    <ul>
      <li>키와 지능의 연관관계
        <ul>
          <li>상관관계 성림</li>
          <li>인과관계는 성립하지 않음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>베이즈 추정법
    <ul>
      <li>베이즈 추정법(Bayesian estimation)은 모숫값이 가질 수 있는 모든 가능성의 분포를 계산하는 작업이다.</li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="cnn">CNN</h1>
<ul>
  <li>이미지 연산에서 많이 사용하는 모델</li>
  <li>국소적 증폭, 국소적 증감</li>
  <li>여러 차원에서 적용 가능
    <ul>
      <li>채널이 여러개인 경우 커널의 채널 수와 입력의 채널수가 같아야 한다.</li>
    </ul>
  </li>
  <li>순전파
    <ul>
      <li>커널을 벡터상에서 움직여 함성함수를 적용.</li>
    </ul>
  </li>
  <li>역전파
    <ul>
      <li>Convolution 연산에서 어떻게 역전파를 적용할 것인가 ?</li>
      <li><a href="https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/">참고자료</a></li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="rnn">RNN</h1>
<ul>
  <li>시퀀스데이터
    <ul>
      <li>연속적인 데이터</li>
      <li>발생 순서가 기록되어 있는 데이터</li>
      <li>독립 동등 분포를 위배한다.
        <ul>
          <li>데이터의 순서가 중요하다.</li>
          <li>과거의 데이터가 중요하다.</li>
          <li>하지만 모든 과거 데이터가 필요한 것은 아님 =&gt; 어텐션 모델 (<code class="language-plaintext highlighter-rouge">Attention all you need)</code></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>시퀀스데이터의 조건부확률</li>
  <li>시퀀스데이터는 과거의 길이를 사용해야 하는데 이것은 가변적인 데이터를 다뤄야 한다는 것을 의미한다.
    <ul>
      <li>하지만 이것을 타우라고 한다면 해결할 수 있다.</li>
    </ul>
  </li>
  <li>순전파</li>
  <li>역전파
    <ul>
      <li>BPTT(Backprogaion Throgh Time) 을 사용한다.
        <ul>
          <li>BPTT 미분은 매우 복잡하다. <a href="https://davi06000.tistory.com/92">링크</a></li>
          <li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">링크</a></li>
        </ul>
      </li>
      <li><strong>기울시 소실 문제 발생</strong> =&gt; 과거 데이터를 반영 못하는 문제 =&gt; 길이를 끊어서 나눠서 학습</li>
      <li>=&gt; LSTM, GRU 와 같은 고급 모델의 탄생</li>
    </ul>
  </li>
</ul>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - Day01</title><link href="http://localhost:4000/aitech/post-day01/" rel="alternate" type="text/html" title="Post: ai tech - Day01" /><published>2023-03-06T00:00:00+09:00</published><updated>2023-03-09T07:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day01</id><content type="html" xml:base="http://localhost:4000/aitech/post-day01/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech 1주차</title><link href="http://localhost:4000/aitech/post-1%EC%A3%BC%EC%B0%A8/" rel="alternate" type="text/html" title="Post: ai tech 1주차" /><published>2023-03-06T00:00:00+09:00</published><updated>2023-03-09T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-1%EC%A3%BC%EC%B0%A8</id><content type="html" xml:base="http://localhost:4000/aitech/post-1%EC%A3%BC%EC%B0%A8/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(23.3.6 ~ 23.3.12)
</code></pre></div></div>

<h1 id="1주차를-보내며">[1주차를 보내며]</h1>

<hr />
<h1 id="1주차">1주차</h1>
<ul>
  <li>강의
    <ul>
      <li>Python
        <ul>
          <li><a href="https://yunjinchoidev.github.io/aitech/post-python/">파이썬 강의록 정리</a></li>
        </ul>
      </li>
      <li>AIMath
        <ul>
          <li><a href="https://yunjinchoidev.github.io/aitech/post-aimath/">AIMath 강의록 정리</a></li>
        </ul>
      </li>
      <li>Pytorch
        <ul>
          <li><a href="https://yunjinchoidev.github.io/aitech/post-pytorch/">Pytorch 강의록 정리</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>회고</li>
</ul>

<hr />]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech 온보딩</title><link href="http://localhost:4000/aitech/post-%EC%98%A8%EB%B3%B4%EB%94%A9/" rel="alternate" type="text/html" title="Post: ai tech 온보딩" /><published>2023-03-05T00:00:00+09:00</published><updated>2023-03-09T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-%EC%98%A8%EB%B3%B4%EB%94%A9</id><content type="html" xml:base="http://localhost:4000/aitech/post-%EC%98%A8%EB%B3%B4%EB%94%A9/"><![CDATA[<!-- ![image](https://user-images.githubusercontent.com/45550607/102208312-9b284180-3f12-11eb-8467-7b5ea1779ac7.png)
{: .align-center} -->

<p><img src="../../../image/aitech.png" alt="image" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(23.3.1 ~ 23.3.5)
</code></pre></div></div>

<h1 id="온보딩기간을-보내며">[온보딩기간을 보내며]</h1>
<p>3월 1일 부터 3월 5일까지 온보딩 기간을 보내며 설레는 마음으로 공부를 했다. 파이썬, ai math, pytorch 강의를 들었다. 수학과를 다니면서 선형대수학을 과연 사용할 일이 있을까 생각을 했었는데 딥러닝에서 매우 중요하다고 한다. 통계학도 중요하다. 그래서 선형대수, 수리통계 책을 구입했다.</p>

<p>8월 2일까지 진행한다. 나는 nlp 트랙으로 참여하게 되었는데 앞으로 해야 할 일이 많다. 하반기에 취업을 하던가 대학원을 갈 생각이다.</p>

<p>깃허브에 레포지토리를 하나 만들어 두었다. 깃허브 블로그를 만들었으니 레포지토리에 올리지 않고 이곳에 올리는 게 더 깔끔 할 것 같기도 하다.</p>]]></content><author><name>최윤진</name></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry></feed>