<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-10T22:34:39+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Reinvent love! - Democratization of Love</title><subtitle>NLP 쪼렙입니다.</subtitle><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><entry><title type="html">Post: ai tech -Day05</title><link href="http://localhost:4000/aitech/post-day05/" rel="alternate" type="text/html" title="Post: ai tech -Day05" /><published>2023-03-10T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day05</id><content type="html" xml:base="http://localhost:4000/aitech/post-day05/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: 딥러닝 바닥부터 만들기 (intro ~ step10)</title><link href="http://localhost:4000/aitech/post-%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EB%A7%8C%EB%93%9C%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D(1)/" rel="alternate" type="text/html" title="Post: 딥러닝 바닥부터 만들기 (intro ~ step10)" /><published>2023-03-10T00:00:00+09:00</published><updated>2023-03-10T10:20:02+09:00</updated><id>http://localhost:4000/aitech/post-%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%20%EB%A7%8C%EB%93%9C%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D(1)</id><content type="html" xml:base="http://localhost:4000/aitech/post-%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EB%A7%8C%EB%93%9C%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D(1)/"><![CDATA[<p><img src="../../../image/바닥부터.png" alt="image" /></p>

<h1 id="바닥부터-딥러닝을-만들어보자">바닥부터 딥러닝을 만들어보자.</h1>

<p>우리는 바닥부터 딥러닝 모델을 한번 만들어 볼것입니다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: 딥러닝의 모든 것(All about Deep Learning)</title><link href="http://localhost:4000/aitech/post-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EB%AA%A8%EB%93%A0%EA%B2%83-copy/" rel="alternate" type="text/html" title="Post: 딥러닝의 모든 것(All about Deep Learning)" /><published>2023-03-09T00:00:00+09:00</published><updated>2023-03-09T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98%20%EB%AA%A8%EB%93%A0%EA%B2%83%20copy</id><content type="html" xml:base="http://localhost:4000/aitech/post-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EB%AA%A8%EB%93%A0%EA%B2%83-copy/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="딥러닝의-모든-것">딥러닝의 모든 것</h1>
<p>안녕하십니까. 최윤진입니다. 이번 포스팅을 통해 딥러닝의 전반적인 내용을 한 번 다뤄보려고 합니다.</p>

<h2 id="누가-좋은-딥러닝-엔지어인가">누가 좋은 딥러닝 엔지어인가?</h2>
<p>파이토치와 선형대수와 확률과 통계를 잘하는 사람, 최신 논문을 잘 읽는 사람은 좋은 엔지니어가 될 가능성이 높다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>논문을 읽을 때는 1. 데이터 2. 모델 3. 손실함수 4. 학습 알고리즘 를 고려하는 것이 좋습니다. 
</code></pre></div></div>

<p><br /><br /><br /><br /></p>

<h1 id="딥러닝의-중요한-아이디어">딥러닝의 중요한 아이디어</h1>

<ul>
  <li>AlexNet(2012) -&gt; 딥러닝의 가능성을 폭발적으로 알려주었다.</li>
  <li>DQN (2013) -&gt; 강화학습</li>
  <li>Encoder / Decoder (2014)</li>
  <li>Adam (2014) -&gt; 결과가 매우 좋다.</li>
  <li>GAN, ResNet (2015)</li>
  <li>Residual Networks (2015) -&gt; 네트워크를 깊게 쌓게 해준다.</li>
  <li>Transformer (2017) -&gt; 도발적</li>
  <li>Bert (2018)</li>
  <li>Big Language Models (GPT)</li>
  <li>Self-Supervised Learning -&gt; 학습 데이터를 스스로 생성한다.</li>
</ul>

<p><br /><br /><br /><br /></p>

<h1 id="신경망">신경망</h1>
<p>신경망에서 비선형 함수를 넣어주는 이유가 무얼까? 선형적 함수로만 표현을 하게 되면은 그것은 결국 선형함수 하나로 변환 될 수 있다는 것을 의미한다. 그렇기 때문에 선형적으로 표현할 수 없는 비선형 함수 (예를 들어 활성화 함수) 를 넣어줌으로써 표현력을 극대화하는 것이다.</p>

<p>손실함수의 종류</p>
<ul>
  <li>MSE</li>
  <li>CE</li>
  <li>MLE</li>
</ul>

<p><br /><br /><br /><br /></p>

<h1 id="bias-and-variance">Bias and Variance</h1>
<p align="center">
  <img src="./../../image/aitech/bias.png" alt="*출처: http://scott.fortmann-roe.com*" width="number" /> 
  <p align="center">*출처: http://scott.fortmann-roe.com*</p>
</p>

<p>cost를 최소화 한다는 것은 bias, variance, noise 를 최소화하는 것이다.</p>

<p><br /><br /><br /><br /></p>

<h1 id="최적화">최적화</h1>
<p>최적화 이론은 딥러닝에서 매우 중요한 개념입니다. 일반화와 최적화의 균일점을 찾아라 ! 그래서 우리는 cross-validation 을 사용합니다.</p>

<h2 id="bagging">Bagging</h2>
<p>학습데이터를 여러개를 만들어서 부트스트래핑을 하는것을 말합니다.  부트스트랩이란 ? 수 많은 데이터 중에 랜덤 샘플링을 여러개 만들어서 예측을 하는 것</p>
<h2 id="boosting">Boosting</h2>
<p>잘 안되는 데이터에 맞춰서 학습을 여러번 시키는 방법을 말한다.</p>

<h1 id="gradinet-descent">Gradinet Descent</h1>
<p>어떻게 산 밑으로 내려갈 것인가?</p>

\[W_{t+1} &lt;- Wt - \gamma G_{t}\]

<blockquote>
  <p>여러가지 테크니션</p>
  <ul>
    <li>모멘텀</li>
    <li>Nesterov Accelerated Gradinet</li>
    <li>Adagrad</li>
    <li>Adadelta</li>
    <li>RMSStop</li>
    <li>Adam</li>
  </ul>
</blockquote>

<p><br /><br /><br /><br /></p>

<h1 id="regularization">Regularization</h1>
<p>오버피팅을 방지하시오.</p>

<ul>
  <li>Early stopping</li>
  <li>Parameter norm penalty</li>
  <li>Data augmentation</li>
  <li>Noise robustness
    <ul>
      <li>노이즈를 넣으면 이상하게 잘된다.</li>
    </ul>
  </li>
  <li>Lable smoothing
    <ul>
      <li>Cut Mix</li>
      <li>Cut Out</li>
      <li>Mixup</li>
    </ul>
  </li>
  <li>Dropout</li>
  <li>Batch normalizaion</li>
</ul>

<p><br /><br /><br /><br /></p>

<h1 id="cnn">CNN</h1>

<h2 id="alexnet">AlexNet</h2>
<h2 id="vggnet">VGGNet</h2>

<h2 id="googlenet">GoogleNet</h2>
<p>1*1 Convolution 은 Dimension 을 줄이기 위해서 사용합니다. (bolttleneck architecture)</p>

<h2 id="incept-block">Incept Block</h2>
<h2 id="resnet">ResNet</h2>
<p>skip-connection</p>
<h2 id="densenet">DenseNet</h2>
<p>concatenation</p>

<h2 id="semantic-segmentation">Semantic Segmentation</h2>
<blockquote>
  <p>Deconvolution 이란? convloution 의 역연산</p>
</blockquote>

<h2 id="detection-문제">Detection 문제</h2>
<p>개체 탐지 문제</p>

<ul>
  <li>
    <p>RPN
특정 영역 안에 물체가 존재할 것인가?</p>
  </li>
  <li>
    <p>YOLO V2</p>
  </li>
</ul>

<p><br /><br /><br /><br /></p>

<h1 id="rnn">RNN</h1>

<p>시퀀스 데이터의 대표적 문제 -&gt; Language Model 가 있습니다.</p>

<p>Markov model 은 문제가 있다.</p>

<p>그에 대한 해결책으로 Latent autoregressive model 이 나온다. 일반적인 RNN 모델</p>

<p>RNN -&gt; fully connectied ?</p>

<p>RNN -&gt; 과거의 데이터 소실 문제가 있다. 왜냐하면 역전파에서 무한 곱이 이뤄지기 때문이다. 이때 기울기 폭발의 문제가 발생하기도 한다. 그래서 LSTM 이 탄생하게 ㅚ더있따.</p>

<p><br /><br /><br /><br /></p>

<h2 id="lstm">LSTM</h2>

<h2 id="gru">GRU</h2>

<h2 id="transformer">Transformer</h2>
<p>잘 이해 하시라. 큰 도움 될 겁니다.</p>

<h1 id="gan">GAN</h1>

<blockquote>
  <p>created By yunjinchoidev ~</p>
</blockquote>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: 딥러닝 모니터링(Monintoring) 하기</title><link href="http://localhost:4000/aitech/post-monitoring/" rel="alternate" type="text/html" title="Post: 딥러닝 모니터링(Monintoring) 하기" /><published>2023-03-09T00:00:00+09:00</published><updated>2023-03-10T10:20:02+09:00</updated><id>http://localhost:4000/aitech/post-monitoring</id><content type="html" xml:base="http://localhost:4000/aitech/post-monitoring/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="monitoring">Monitoring</h1>
<p>학습을 하는 데 굉장히 긴 시간이 걸립니다. 학습 과정을 기록하는게 좋겠죠. <code class="language-plaintext highlighter-rouge">TensorBoard</code>와 <code class="language-plaintext highlighter-rouge">Weight &amp; Bias</code>를 이용해봅시다.</p>

<h1 id="tensorboard">Tensorboard</h1>
<ul>
  <li>scalar : metric 표시</li>
  <li>graph : 계산 그래프</li>
  <li>histogram : weight 분포</li>
  <li>image: 예측 값과 실게 값을 비교 표시</li>
  <li>mesh : 3d 형태의 데이터를 표현하는 도구</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
import os
logs_base_dir = 'logs'
os.makedirs(logs_base_dir, exist_ok=True)

from torch.utils.tensorboard import SummaryWriter
import numpy as np

exp = f"{logs_base_dir}/ex3"
writer = SummaryWriter(exp)
for n_iter in range(100):
  writer.add_scalar('Loss/train', np.random.random(), n_iter)
  writer.add_scalar('Loss/test', np.random.random(), n_iter)
  writer.add_scalar('Accuracy/train', np.random.random(), n_iter)  
  writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
writer.flush()

%load_ext tensorboard
%tensorboard --logdir {"logs"}

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 분포를 본다.

from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter(logs_base_dir)
for i in range(10):
  x = np.random.random(1000)
  writer.add_histogram('distribution centers', x+i, i)
writer.close()

</code></pre></div></div>

<h1 id="weight--bias">Weight &amp; Bias</h1>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech -Day04</title><link href="http://localhost:4000/aitech/post-day04/" rel="alternate" type="text/html" title="Post: ai tech -Day04" /><published>2023-03-09T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day04</id><content type="html" xml:base="http://localhost:4000/aitech/post-day04/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<ul>
  <li>과제를 풀면서 한 주간 배운 내용을 정리했다.</li>
  <li>파이토치 내용을 보충했다.</li>
  <li>딥러닝 기초 다지기 강의를 들었다.</li>
  <li>ai math 내용을 보충했다.</li>
</ul>

<h1 id="내일-무엇을-할-것인가">내일 무엇을 할 것인가?</h1>
<ul>
  <li>RNN 강의 복습</li>
  <li>pytorch 강의 복습</li>
  <li>딥러닝 기초 다지기 강의 수강</li>
</ul>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: Dive into NLP</title><link href="http://localhost:4000/aitech/post-Dive-Into-NLP/" rel="alternate" type="text/html" title="Post: Dive into NLP" /><published>2023-03-09T00:00:00+09:00</published><updated>2023-03-09T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-Dive%20Into%20NLP</id><content type="html" xml:base="http://localhost:4000/aitech/post-Dive-Into-NLP/"><![CDATA[<p><img src="../../../image/nlp.png" alt="image" /></p>

<h1 id="dive-into-nlp">Dive Into NLP</h1>
<p>자연어 처리를 공부합시다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>자연어 처리(自然語處理) 또는 자연 언어 처리(自然言語處理)는 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 모사할 수 있도록 연구하고 이를 구현하는 인공지능의 주요 분야 중 하나 (출처: 위키피디아)
</code></pre></div></div>

<p><br /><br /><br /><br /></p>

<h1 id="자연어처리의-주요-분야">자연어처리의 주요 분야</h1>
<p>자연어처리의 주요 분야를 알아봅시다.</p>

<h2 id="1-nlp-acl-emnlp-naacl">1. NLP (ACL, EMNLP, NAACL)</h2>
<p>자연어 처리의 대표적인 학회는 ACL, EMNLP, NAACL 이 있습니다.</p>

<p>주요 분야는 아래를 참고하세요.</p>

<ul>
  <li>machine translation -&gt; 기계 번역</li>
  <li>Sentiment analysis</li>
  <li>dialog systems -&gt; 챗봇 🔥</li>
  <li>summarization</li>
  <li>Named entity recognition (NER) -&gt; 객체에 이름 부여하기 (인식의 문제)</li>
  <li>텍스트 생성</li>
  <li>Spam Detection : 스팸 디텍션</li>
</ul>

<p><br /><br /><br /><br /></p>

<h2 id="2-text-mining-kdd-www-wsdm-cikm-icwsm">2. Text mining (KDD, WWW, WSDM, CIKM, ICWSM)</h2>
<ul>
  <li>비정형 데이터로부터 유용한 정보를 찾기</li>
  <li>추천 시스템</li>
  <li>MRC -&gt; 기계 독해</li>
  <li>MRC란 인공지능이 사람처럼 문서를 읽고 이해한 후 질문에 정확히 답하는 기술</li>
</ul>

<p><br /><br /><br /><br /></p>

<h2 id="3-information-retieval-sigir-wsdm-cikm-recsys">3. Information retieval (SIGIR, WSDM, CIKM, RecSys)</h2>
<ul>
  <li>추천 시스템</li>
  <li>Keyword Search</li>
</ul>

<h1 id="nlp-의-역사">NLP 의 역사</h1>
<p>NLP 는 최근 들어서 급속도로 연구되고 발전하고 있는 분야입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- RNN
- LSTRM
- GRU
- Transformer `Àttention all you need`
 - NLP의 혁명, AI 의 혁명
- BERT, GPT
  - self-supervised learning
</code></pre></div></div>

<p>요즘엔 NLP 의 혁명을 주도하는 곳은 거대 기업이라고 할수 있다. 왜냐하면 거대 모델은 거대한 학습이 필요하기 때문이다.
개인 개발자가 할 수 있는 것은 무엇인가?  AI 는 플랫폼 독점으로 나갈 것인가? 생각이 필요합니다.</p>

<p><br /><br /><br /><br /></p>

<h1 id="문장-분해하기">문장 분해하기</h1>

<h2 id="tokenization">Tokenization</h2>
<h2 id="stemming">stemming</h2>

<h2 id="한국어-자연어-처리-모델">한국어 자연어 처리 모델</h2>
<ul>
  <li>KoNLPy</li>
  <li>카카오</li>
  <li>은전한닢</li>
  <li>KT~</li>
</ul>

<h1 id="고전---bowbag-of-words">[고전] - BOW(Bag of Words)</h1>
<p>단어들의 순서를 고려하지 않습니다. 출현 빈도만 고려해서 텍스트를 분석하는 기법을 말합니다. 
즉, 가방에다가 단어들을 넣어 놓고 ‘뭐’ 가 ‘몇개’ 있는 지를 보는 아주 직관적인 방법으로 볼 수 있는 것이죠.</p>

<h2 id="나이브-베이즈-분류기">나이브 베이즈 분류기</h2>
<p>우리 나이브 베이스 분류기를 공부해봅시다.</p>

<h1 id="고전---tf-idfterm-frequency---inverse-document-frequency">[고전] - TF-IDF(Term Frequency - Inverse Document Frequency)</h1>

<p>특정 문서에 특정 단어가 얼만큼 있는지 값을 표현하는 것.
TF(단어 빈도, term frequency)
특정 단어가 문서 내에 얼마나 자주 등장하는 지 ? 
DF(문서 빈도, document frequency)
단어 자체가 전체 문서에서 사용되는 지
One hot encoding 으로 표현합니다. -&gt; 단어 간의 유사성을 파악하지 못하는 문제가 있습니다.</p>

<h1 id="word-embedding">Word Embedding</h1>
<p>현대 nlp의 근간이 되는 기술입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>연관성 있는 단어들을 군집화하여 multi-dimension 공간에 vector 로 표시
단어나 문장을 vector space 로 끼워 넣음 (embedding)
</code></pre></div></div>

<p>k 번째 단어인 one hot encoding 와 word embedding matrix 를 곱하면 k 번째 로우가 k 번째 단어를 설명하는 weigts 가 된다.</p>

<h1 id="word2vec">Word2Vec</h1>
<p>구글이 2013년 개발했습니다. -&gt; by 비지도학습
중심단어로부터 주변단어를, 주변단어로부터 중심 단어를 임베딩 시키는 방식으로 학습합니다. 그 코사인 유사도가 유사하도록 한다.</p>

<h1 id="gloveglobal-vectors-for-word-representation">Glove(Global Vectors for Word Representation)</h1>
<p>2014, Stanford
말뭉치 전체를 고려한 word embeding</p>

<h1 id="fasttext">FastText</h1>
<p>2016, Facebook
희소한 단어가 학습되지 않는 문제점을 해겼다. 다양한 용언을 가진 한국어의 특성에 잘 맞습니다.</p>

<h1 id="rnn">RNN</h1>
<p>시퀀스데이터 
history 전달</p>

<ul>
  <li>one to one -&gt; 이미지 분류</li>
  <li>one to many -&gt; 이미지로부터 문장 생성, 작곡</li>
  <li>many to one -&gt; 감성 분석, 생성 모델</li>
  <li>many to many -&gt; 기계 번역, 챗봇, Q&amp;A</li>
</ul>

<h1 id="lstm">LSTM</h1>
<p>새로운 입력을 어떻게 받을 것인지 forget gate, update gate, output gate 가 있수다.</p>

<h1 id="gru">GRU</h1>
<p>lstm의 gate 를 하나로 줄인것.</p>

<h1 id="sentiment-analysis">Sentiment Analysis</h1>

<h1 id="ner-개체-인식">NER (개체 인식)</h1>

<h1 id="language-model">Language Model</h1>
<ul>
  <li>이전으로부터 다음을 예측하기</li>
  <li>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  기계 번역
  Qna
  chatbot
  speech recognition
  text summarization
  text to speech(tts)
  image caption
</code></pre></div>    </div>
  </li>
</ul>

<h1 id="seq2seqencoder-decoder">Seq2Seq(Encoder-decoder)</h1>
<p>Machine Translate 분야에 사용된다.</p>

<h2 id="챗봇을-만들어보자">챗봇을 만들어보자.</h2>

<h2 id="기계-번역-평가-알고리즘">기계 번역 평가 알고리즘</h2>
<ul>
  <li>BLUE</li>
  <li></li>
</ul>

<h1 id="transformers">Transformers</h1>
<p>2017 - Transformer
현재 딥러닝은 트랜스포머 이전과 이후라 나뉩니다.</p>
<h1 id="transfer-learning-전이학습">Transfer Learning (전이학습)</h1>

<h1 id="elmo">ELMO</h1>
<ul>
  <li>마지막 RNN 모델ㅇ</li>
</ul>

<h1 id="bert">BERT</h1>
<p>2018 - Bert</p>

<h1 id="gpt">GPT</h1>
<p>2020 - GPT 3</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - pytorch Template 완전 분석</title><link href="http://localhost:4000/aitech/post-pytorchtemplate/" rel="alternate" type="text/html" title="Post: ai tech - pytorch Template 완전 분석" /><published>2023-03-08T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-pytorchtemplate</id><content type="html" xml:base="http://localhost:4000/aitech/post-pytorchtemplate/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="pytorch-템플릿을-문석해봅시다">Pytorch 템플릿을 문석해봅시다.</h1>
<p><a href="https://github.com/victoresque/pytorch-template">링크</a>
 여기 오픈 소스가 하나 있습니다. 이 포스팅을 통해 이 템플릿을 분해, 분석 해보는 시간을 가지려고 합니다.</p>

<p><img src="../../../image/aitech/pytorchtemplatetree.png" alt="파일 " /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* train.py              -&gt; 실행
* test.py               -&gt; 실행
* config                -&gt; 설정
* parse_config          -&gt; 설정
* base                  -&gt; base 모델
* data_loader ~         -&gt; data 
* model ~               -&gt; 모델
* save ~                -&gt; 저장소
* trainer ~             -&gt; 학습
* logger ~              -&gt; 로깅
* utils ~               -&gt; 유틸
</code></pre></div></div>

<hr />
<h2 id="실행-방법입니다">실행 방법입니다</h2>
<p>루트 디렉토리에서 <code class="language-plaintext highlighter-rouge">python train.py -c config.json</code> 을 치세요.</p>

<h2 id="분해">분해</h2>
<ul>
  <li>train.py
    <ul>
      <li>arg.add_argument</li>
    </ul>
  </li>
  <li>config.json
    <ul>
      <li>util의 <code class="language-plaintext highlighter-rouge">read_josn</code> 을 보라.</li>
    </ul>
  </li>
  <li>train/tariner.py</li>
  <li>baser/base_tariner.py
    <ul>
      <li>학습의 원천이 되는 소스</li>
    </ul>
  </li>
</ul>

<h2 id="아하-그러니까요">아하 그러니까요.</h2>
<p>스프링 부트와 같은 웹 프레임워크를 사용하신 분들에는 익숙한 구조일겁니다.
가령 우리는 yml 파일을 수정함으로써 configuration을 바꿉니다. 소스 전체를 바꾸는 것이 아니라 특정 config 요소만 바꿈으로써 소스 전체에 영향을 끼치는 겁니다.
이것이 템플릿, 프레임워크의 힘의 근원이라고 할 수 있는 것이죠.</p>

<p>pytorch-template 도 같은 시각에서 접근해봅시다. 
config.json 만 바꿈으로써 우리는 새로운 데이터를 로딩할 수 있는 것이고 batch_size, validation_split 도 변경 할 수 가 있습니다. 
train.py 만 바꿈으로써 우리는 모델을 손쉽게 교체할 수 가 있습니다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech -Day03</title><link href="http://localhost:4000/aitech/post-day03/" rel="alternate" type="text/html" title="Post: ai tech -Day03" /><published>2023-03-08T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day03</id><content type="html" xml:base="http://localhost:4000/aitech/post-day03/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<ul>
  <li>python 복습했다.</li>
  <li>aimath 1 ~ 5 강을 들었다.</li>
  <li>pytorch template 분석 했다.</li>
  <li>포스팅 정리를 했다.</li>
  <li>github 블로그를 만들었다.</li>
</ul>

<h1 id="내일-무엇을-할-것인가">내일 무엇을 할 것인가?</h1>
<ul>
  <li>ai math, pytorch 강의 마무리 하면서 포스팅 정리 하기</li>
  <li>과제 검토</li>
  <li>RNN 의 오차 역전파 연구하기</li>
</ul>

<h1 id="오늘의-생각">[오늘의 생각]</h1>
<p>오늘 NLP 팀 발표 시간을 가졌다. 논문 리뷰, 코딩 테스트 스터디를 많이 하는 듯 했다. 실력자들이 굉장히 많다는 걸 느끼면서 겸손해야 겠다는 생각을 했다. 내가 속한 팀의 팀원들도 다들 실력이 좋으시다. 팀에 도움이 될 수 있도록 노력했야 겠다는 다짐을 하게 된다. 적어도 마이너스를 하지는 말아야지.</p>

<p>다음 주 부터는 코딩테스트 스터디를 시작하려고 한다. 그리고 파이토치도 정규 수업으로 들어가고 본격적으로 딥러닝 모델로 무언가를 만들어보는 시간을 가진다. 그리고 다음 달이 되면 첫 번째 프로젝트를 참여 하게 된다. 좋은 성과를 거두고 싶다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Post: ai tech - Day02</title><link href="http://localhost:4000/aitech/post-day02/" rel="alternate" type="text/html" title="Post: ai tech - Day02" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-day02</id><content type="html" xml:base="http://localhost:4000/aitech/post-day02/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="오늘-무엇을-했나">오늘 무엇을 했나?</h1>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- numpy, pandas  를 집중 분석했다.
- 공부하는 것과 실제 적용하는 것에는 큰 차이가 있는 거 같다.
- 경사하강법, 오차 역전파에서 스칼라가 텐서로 바뀔 때의 상황이 직관적으로 와닿지가 않아서 공부했다.
- 최대 우도법을 공부.
</code></pre></div></div>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">딥러닝에 필요한 수학적 지식</title><link href="http://localhost:4000/aitech/post-aimath/" rel="alternate" type="text/html" title="딥러닝에 필요한 수학적 지식" /><published>2023-03-07T00:00:00+09:00</published><updated>2023-03-08T06:20:02+09:00</updated><id>http://localhost:4000/aitech/post-aimath</id><content type="html" xml:base="http://localhost:4000/aitech/post-aimath/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="벡터">벡터</h1>
<p>벡터란 뭡니까? 스칼라 다음의 차원을 표시하기 위한 수학 도구입니다.
$ L_1$ 노름은 변화량읠 절대값을,  $L_2$ 노름은 유클리드 거리를 말합니다</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="행렬">행렬</h1>

<p>행렬에 대해서 알아봅시다. 행렬곱은 행과 열의 조건을 맞춰야 연산가능합니다. 행렬의 곱은 교환법칙이 성립하지 않으니 유의해야 합니다. 
파이썬에서는 @ 기호를 통해 행렬의 곱을 지원합니다. <code class="language-plaintext highlighter-rouge">np.inner</code> 도 가능합니다.(자동 transpose 지원) <br />
역행렬은 곱의 연산을 했을 때 항등행렬이 나오는 행렬을 말합니다. 곱의 역원을 의미합니다. <br />
역행렬은 선형사상과 필요충분조건입니다. (선형대수학에 대한 지식이 필요합니다.) <code class="language-plaintext highlighter-rouge">np.linalg.pinv</code> 모듈을 이용해서 구할 수 있습니다 <br />
유사 역행렬에는 무어펜로스(Moore-Penrose) 역행렬이 있습니다. 왜냐하면 역행렬을 구하는 것이 쉽지 않기 때문입니다. <br />
<img src="../../../image/aitech/무어펜로즈.png" alt="무어펜로즈.png" /></p>

<p>행렬은 왜 배우는 것일까요? 연립방정식, 선형회귀 분석에 응용됩니다.<code class="language-plaintext highlighter-rouge">차원이동</code> <code class="language-plaintext highlighter-rouge">연산</code> 을 쉽게 만들어 주기 때문이죠. <br />
딥러닝을 제대로 이해하기 위해선 ‘선형대수학’을 수준 높게 학습해야 합니다.
사이킷런의 선형회귀 모델과 무어펜로즈의 선형회귀를 직접 구현보시는 건 어떻습니까? <br /></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="경사하강법-gradinet-descent">경사하강법 (Gradinet Descent)</h1>
<p>경사하강법에 대해서 알아봅시다. 우선 기초적인 미분에 대해서 알아볼까요?</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="미분">미분</h2>
<p>먼저 미분에 대해서 알아봅시다.
미분이란 접선의 기울기를 말하죠. 
<code class="language-plaintext highlighter-rouge">sysmpy.diff</code> 모듈을 이용하여 미분계산이 가능합니다.
이를 통해서 우리는 주어진 input 데이터가 <strong>어느 차원에서든</strong> 그 점에서 증가하는가, 감소하는가를 알 수 있습니다.
어느 차원으로 확장한다는 것은 변수를 스칼라가 아닌 벡터와 텐서를 사용한다는 것입니다.</p>

<p>좋습니다. 이제 경사하강법을 알아볼까요? 
경사하강법이란 기울기가 감소하는 방향으로 쭈욱 가다 <strong>보면</strong> 언젠가 평지를(극소값)을 만날 것입니다. 컴퓨터에서 미분값이 0 인 곳을 찾기란 쉽지 않으므로 아주 작은 값(오메가) 보다 더 미분 값이 작으면 종료하면 됩니다. 이것이 간략한 설명입니다.
이 때 중요한 것은 어느 만큼의 보폭으로 갈 것이냐입니다. 이것은 하이퍼 파라메터로 즉 학습률입니다.
변수가 스칼라가 아니라 벡터라면 편미분을 하면 됩니다. 
모든 데이터를 사용하는 것은 고전적인 방법입니다. 이것은 성능이 좋지 않고 하드웨어에 부담이 갑니다. 시간도 많이 걸리죠. 
조금의 데이터만 이용해서 경사를 갱신하는 것을 SGD 라고 합니다. 이 방법은 최근 많이 사용하는 방법입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="그레디언트-벡터gradient-vector">그레디언트 벡터(gradient Vector)</h2>
<p>함수의 벡터 변수 별로 편미분을 계산한 함수를 말합니다.
계산식은 이렇게 됩니다. 
<img src="../../../image/aitech/gradient.png" alt="gradient" />
그레디언트 벡터에 - 를 붙여서 이동하게 되면 <strong>가장 빨리</strong> 극소값을 향해 가게 되는 겁니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>그레디언트 벡터의 `norm` 값을 구해서 일정 값보다 작게 되면 학습을 종료하는 방식으로 극소값을 찾으면 된다.
(계속 내려가다가 더 이상 변화가 없는 것 같아. -&gt; 일단 멈춰!)
</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="선형회귀-모델에서-경사하강법-적용해보기">선형회귀 모델에서 경사하강법 적용해보기</h1>
<p>선형 모델을 구할 때 무어 펜로즈 행렬을 사용할수 도 있습니다. <br />
하지만 우리는 무어펜로즈 행렬을 곱해서 나온 우변 식을 경사하강법을 하지 않고 오직 경사 하강법을 적용시켜 극소값이 되는 지점을 찾는 것이다.<br />
$y-y$ 의 값을 미분해서 학습률을 곱해 에포크동안 최소화 지키다는 것입니다. <br /></p>

<p>직접 계산해봅시다. 학습률과 학습횟수를 적절하게 선택했을 때만 수렴을 보장할 수 있습니다. 비선형회귀의 경우, 볼록하지 않기 때문에 수렴을 보장할 수 없다. (왜 불가능한가? 고민해봅시다.)</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="미니배치-확률적-경사하강법sgd">미니배치 확률적 경사하강법(SGD)</h1>
<p>데이터를 일부 사용해서 경사하강법을 적용하는 방식을 말합니다. 경험적으로 좋다는 것이 밝혀져 있습니다.  SGD 가 경사하강법보다 낫다는 것이 실증적으로 검증되었다. 데이터를 나눠서 epoch 를 반복하여 학습하는 것을 말합니다. 데이터를 일부로 사용하기 때문에 목적식이 매번 달라집니다.
학습률, 미니배치사이즈가 하이퍼 파라메터입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="딥러닝-학습방법">딥러닝 학습방법</h1>
<p>이제 비선형모델인 신경망을 도전해봅시다.먼저 소프트 맥스 함수 부터 알아보지요.</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="소프트맥스">소프트맥스</h2>
<p>모델의 출력을 확률로 해석하여 확률 벡터로 변환하는 함수입니다. 가장 큰 값이 예측값이 되는 것입니다.
신경망은 <strong>선형모델과 활성함수를 합성한 함수</strong>입니다. 이러한 layer들은 잠재벡터들의 누적하게 되고 마지막 공간에 활성화 함수를 적용하는 것입니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h2 id="활성함수">활성함수</h2>
<p>R 위에 정의된 비선형 함수입니다. 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없습니다. 대표적으로 Relu, tanh, sigmoid 가 있습니다.
순전파는 주어진 신경망 계산을 하는 것을 말합니다. 이 때 우리는 왜 여러 계층을 사용하는 걸까요? 왜 층을 여러개 쌓는가? 수학적으로 계층이 3개 이상만 있으면 임의의 연속함수를 근사할 수 있다는 것이 증명되어 있다. ‘세계’를 표현할 수 있는 것이죠.  역전파는 경사하강법을 이용해서 가중치를 업데이트 하는 것을 말합니다.  위층부터 저층으로 그레디어트 벡터를 전달해야 함(연쇄법칙) 메모리에 저장해서 사용해야 합니다. tensorflow, pytorch 에는 자동 구현되어 있습니다.</p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="확률론">확률론</h1>
<p>확률론은 딥러닝에서 대단히 중요합니다. 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.
확률론이 중요한 이유를 알아봅시다.  <code class="language-plaintext highlighter-rouge">분류</code> 문제 에서 예측 오차의 분산을 최소화하고 <code class="language-plaintext highlighter-rouge">교차 엔트로피</code> 문제에서 모델 예측의 불확실성을 최소화하는데 쓰입니다. 확률 분포는 데이터의 초상화입니다. 확률은 면적입니다. 이산 확률 변수에서는 시그마 급수, 연속 확률 변수에서는 연속 확률을 적분하는 것을 말합니다. 목표는 데이터의(표본) 확률 분포를 가지고 -&gt; 실제 (모집단)의 확률 분포를 추정하는 것입니다. 
<strong>몬테카를로 샘플링</strong>은 데이터(샘플링)를 통해 기댓값을 계산하는 방법을 말합니다. 대수의 법칙을 통해 수렴성을 보장합니다.</p>

<ul>
  <li>주변확률분포</li>
  <li>통계값(모수)
    <ul>
      <li>기댓값</li>
      <li>분산</li>
      <li>첨도</li>
      <li>공분산</li>
      <li>조건부확률</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np

def monte(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high - low)
    stat = []

    for _ in range(repeat):
        x = np.random.uniform(low=low, high=high, size=sample_size)
        fun_x = fun(x)
        int_val = int_len * np.mean(fun_x)
        stat.append(int_val)

    return np.mean(stat), np.std(stat)


- 주어진 함수
def f_x(x):
    return np.exp(-x**2)


print(monte(f_x, low=-1, high=1, sample_size=1000, repeat=100))

</code></pre></div></div>

<p><br />
<br />
<br />
<br /></p>

<h1 id="통계학">통계학</h1>
<p>통계적 모델링 확률분포를 추정하는 것입니다. 그 중 모수를 추정한다는 것은 확률분포의 특성값(모수)를 추정한다는 것이죠. 모수적 방법론은 특정 확률 분포를 가정 한 후 모수를 추정하는 것이며, 비모수적 방법론은 가정 없이 추정하는 것입니다.
모수적 방법론의 과정은 다음과 같은 순서가 될 것입니다. <br />
<code class="language-plaintext highlighter-rouge">데이터 관찰 -&gt; 확률 분포 가정 -&gt; 평균, 분산 추정 -&gt; 모수 집단 추청</code></p>

<p>데이터를 관찰하고 확률 분포를 가정합니다.</p>

<ul>
  <li>베르누이 분포</li>
  <li>카테고리 분포</li>
  <li>베타 분포</li>
  <li>감마분포, 로그정규분포</li>
  <li>정규분포, 라플라스분포</li>
</ul>

<p>가 있습니다. 하나 하나가 공부해 볼 가치가 있는 것들이죠.</p>

<p>중심극한정리는 표뵨 평균의 표집분포는 N이 커질수록 정규분포를 따른다는 것을 말합니다.
<code class="language-plaintext highlighter-rouge">표집분포는 통계량의 확률분포를 말합니다.</code></p>

<h2 id="최대가능도-추정법">최대가능도 추정법</h2>
<p>매우 중요합니다. <code class="language-plaintext highlighter-rouge">최대가능도 추정법</code>에 대해 공부해봅시다.
가능도란 무엇일까요? <strong>모수 쌔타 분포에서 데이터 x 를 발견할 가능성.</strong> 을 말합니다.
그렇다면 자연스럽게  <strong>가장 가능성이 높은 모수를 추정하는 방법</strong> 은 최대 가능성 추정법이 되겠죠.
즉, 데이터가 많이 모인 상황에서 <code class="language-plaintext highlighter-rouge">특정 데이터의 분포</code> 를 가정하고 <code class="language-plaintext highlighter-rouge">제일 가능성 높은</code> 모수를 추정하는 것이라는 말입니다.
정답 확률분포와 모델 추정 확률분포 손실함수 구해서 학습을 시킵시다. 딥러닝의 방법이 도입되는 것이죠.</p>

<p>딥러닝에서 최대가능도 추정법을 자세히 알아봅시다. 우리는 신경망의 마지막 계층에 소프트맥스 벡터를 적용시켜주게 됩니다. 이것은 신경망의 마지막 발산 값이 카테고리 분포라고 할 수 있는 것 인데요. 두 개의 확률분포의 손실함수를 학습시키면 됩니다. 이 식은 쿨백-라이블러 발산을 최소화하는 것과 같습니다. 요약하자면 분류 문제에서 정답레이블을 P, 모델 예측을 Q 라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같습니다.</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=XhlfVtGb19c">설명링크</a></li>
  <li><a href="https://angeloyeo.github.io/2020/07/17/MLE.html">설명블로그</a></li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>확률과 가능도의 차이 ? 
  확률 : 이 사건이 일어날 경우의 수를 전체 사건의 수로 나눈 것.
  가능도 : 지금 얻은 데이터가 이 분포에서 나왔을 가능도
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>쿨백-라이블러 발산 이란?

쿨백-라이블러 발산(Kullback-Leibler divergence) 은 두 확률분포 p(y), q(y) 의 분포모양이 얼마나 다른지를 숫자로 계산한 값입니다.
그 값은 항상 양수이며 두 확률분포 완전히 같을 경우에만 0이 된다.교차엔트로피 개념을 이해할 필요가 있습니다.
</code></pre></div></div>

<p>생각해보세요. 과연 어떻게 할까?</p>

<ul>
  <li>정규분포에서 최대 가능도 추정</li>
  <li>카테고리 분포에서 최대 가능도 추정</li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="베이즈-통계학">베이즈 통계학</h1>
<p>조건부 확률을 이용하면 정보를 갱신하는 방법을 알 수 있습니다. A 라는 사건이 추가로 주어졌을 때 B 가 일어날 확률은  다음과 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B가 일어날 확률 * P(A|B) P(A)
</code></pre></div></div>

<ul>
  <li>사후확률 = 사전확률 * 가능도/증거</li>
  <li>1종 오류, 2종 오류</li>
  <li>갱신된 사후확률을 계산 할 수 있다. (연속된 계산) (ex. 두번 검진)
    <ul>
      <li>정확도를 높일 수 있다.</li>
    </ul>
  </li>
  <li>조건부 확률을 토대로 인과관계를 함부로 추론해서는 안된다.
    <ul>
      <li>데이터에 따라 달라질 수 있기 때문임.</li>
      <li>중첩효과를 제거해서 가짜 연관관계를 제거해야 함.</li>
      <li>인과관계를 뒷받침하는 모델로서 작동하는 조건부확률</li>
    </ul>
  </li>
  <li>상관관계와 인과관계는 다르다.
    <ul>
      <li>키와 지능의 연관관계
        <ul>
          <li>상관관계 성림</li>
          <li>인과관계는 성립하지 않음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>베이즈 추정법
    <ul>
      <li>베이즈 추정법(Bayesian estimation)은 모숫값이 가질 수 있는 모든 가능성의 분포를 계산하는 작업이다.</li>
    </ul>
  </li>
</ul>

<p><br />
<br />
<br />
<br /></p>

<h1 id="cnn">CNN</h1>
<p>CNN은 이미지 연산에서 많이 사용하는 모델입니다. 국소적 증폭, 국소적 증감을 만들어낼 수 있습니다. 또 여러 차원에서 적용이 가능합니다. 채널이 여러개인 경우 커널의 채널 수와 입력의 채널수가 같아야 합니다. 순전파는 커널을 벡터상에서 움직여 함성함수를 적용합니다.역전파는 어떻게 적용하는 것일까요? 생각해봅시다.Convolution 연산에서 어떻게 역전파를 적용할 껀가요? 다음 자료를 참고해보세요.</p>

<p><a href="https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/">참고자료</a></p>

<p><br />
<br />
<br />
<br /></p>

<h1 id="rnn">RNN</h1>
<p>Recurrent Natural Network. RNN 입니다. 시퀀스데이터는 연속적인 데이터를 말하는데요. 발생 순서가 기록되어 있는 데이터입니다. 가령, 주가데이터나 학년별 성적표 같은 것이 해당 될 수 있겠죠. 시퀀스 데이터의 특징은 다음과 같습니다.</p>
<ul>
  <li><strong>독립 동등 분포를 위배한다.</strong></li>
  <li><strong>데이터의 순서가 중요하다.</strong></li>
  <li><strong>과거의 데이터가 중요하다</strong>.</li>
  <li><strong>하지만 모든 과거 데이터가 필요한 것은 아님</strong>  =&gt; 그 유명한 어텐션 모델의 탄생 (<code class="language-plaintext highlighter-rouge">Attention all you need)</code></li>
</ul>

<p>시퀀스데이터에서 조건부확률에 대해서 생각해봅시다. 시퀀스데이터는 과거의 데이터가 지속적으로 영향을 주게 되는데 이 때 모든 값을 다 고려하여 다뤄야 합니다. 즉 시간에 따라서 가변적인 데이터를 다뤄야 한다는 것을 의미하죠. 이 것 또한 특정 함수의 식이라고 한다면 고정 데이터로 바꿀 수 있습니다.</p>

<p>RNN 에서 역전파는 BPTT(Backprogaion Throgh Time) 을 사용합니다. BPTT 미분은 매우 복잡하다.</p>

<ul>
  <li><a href="https://davi06000.tistory.com/92">링크1</a></li>
  <li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">링크2</a></li>
</ul>

<p>이 두 자료를 한번 보도록 하십시요.</p>

<p>RNN 에서의 역전파는 수식의 특성상 기울기 소실 문제가 발생합니다. 과거 데이터를 반영하지 못하는 것이죠. 자연스럽게 LSTM, GRU 와 같은 고급 모델이 탄생은 필연적이었습니다.</p>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech" /><summary type="html"><![CDATA[]]></summary></entry></feed>