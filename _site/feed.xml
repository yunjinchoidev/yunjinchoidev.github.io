<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-23T21:18:03+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Reinvent love! - Democratization of Love</title><subtitle>NLP 쪼렙입니다.</subtitle><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><entry><title type="html">[연재][Clear Algo][1편] - 그리디 알고리즘</title><link href="http://localhost:4000/ps/post-ps_study_-%EC%97%B0%EC%9E%AC-clearAlgo-1%ED%8E%B8/" rel="alternate" type="text/html" title="[연재][Clear Algo][1편] - 그리디 알고리즘" /><published>2023-03-23T00:00:00+09:00</published><updated>2023-03-24T06:20:02+09:00</updated><id>http://localhost:4000/ps/post-ps_study_%5B%EC%97%B0%EC%9E%AC%5D%20clearAlgo%20%5B1%ED%8E%B8%5D</id><content type="html" xml:base="http://localhost:4000/ps/post-ps_study_-%EC%97%B0%EC%9E%AC-clearAlgo-1%ED%8E%B8/"><![CDATA[<h1 id="연재clear-algo1편---그리디-알고리즘">[연재][Clear Algo][1편] - 그리디 알고리즘</h1>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="ps" /><summary type="html"><![CDATA[[연재][Clear Algo][1편] - 그리디 알고리즘]]></summary></entry><entry><title type="html">ai tech - Day14</title><link href="http://localhost:4000/aitech_daily/post-day14/" rel="alternate" type="text/html" title="ai tech - Day14" /><published>2023-03-23T00:00:00+09:00</published><updated>2023-03-24T06:20:02+09:00</updated><id>http://localhost:4000/aitech_daily/post-day14</id><content type="html" xml:base="http://localhost:4000/aitech_daily/post-day14/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="5f">5F</h1>
<h2 id="그날의-사실-facts-">그날의 사실 (Facts) :</h2>

<h2 id="느낌-feeling-">느낌 (Feeling) :</h2>

<h2 id="배운점-findings-">배운점 (Findings) :</h2>

<h2 id="미래의-행동계획-future-">미래의 행동계획 (Future) :</h2>

<h2 id="피드백-feedback-">피드백 (Feedback) :</h2>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech_daily" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">[프로그래머스] 기둥과보</title><link href="http://localhost:4000/ps/post-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EA%B8%B0%EB%91%A5%EA%B3%BC%EB%B3%B4/" rel="alternate" type="text/html" title="[프로그래머스] 기둥과보" /><published>2023-03-23T00:00:00+09:00</published><updated>2023-03-24T06:20:02+09:00</updated><id>http://localhost:4000/ps/post-%5B%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4%5D%5B%EA%B8%B0%EB%91%A5%EA%B3%BC%EB%B3%B4%5D</id><content type="html" xml:base="http://localhost:4000/ps/post-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EA%B8%B0%EB%91%A5%EA%B3%BC%EB%B3%B4/"><![CDATA[<p><img src="../../../image/ps.png" alt="paper.png" /></p>

<h1 id="해결-전략">해결 전략</h1>

<p>is_normal 함수를 이용해서 정상성 여부를 검사한다. 수의 범위가 적으니 완전탐색으로 풀어도 되는 문제였다. 수가 적으면 완전탐색을 의심해봅시다.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/yEslpDMIoS0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

<p><br />
<br />
<br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def solution(n, build_frame):
    answer = [[]]
    
    
    update_map = set()
    
    def is_normal(update_map):
        
        for material in update_map:
            # 보 라면
            if material[2] == 1:                
                # 한쪽에라도 기둥이 있으면 된다.
                if (material[0], material[1]-1, 0) in update_map or\
                   (material[0]+1, material[1]-1, 0) in update_map :
                    continue
                # 양쪽에 보가 있으면 된다.
                if (material[0]-1, material[1], 1) in update_map and\
                    (material[0]+1, material[1], 1) in update_map:
                    continue

            
                return False
                
            # 기둥 이라면
            elif material[2] == 0:
                
                # 바닥이라면
                if material[1] == 0:
                    continue
                    
                # 아래에 기둥이 있다면
                if (material[0], material[1]-1, 0) in update_map:  
                    continue
                    
                # 양쪽에 보가 하나라도 있으면 된다.
                if (material[0]-1, material[1], 1) in update_map or\
                    (material[0], material[1], 1) in update_map:              
                    continue
                
                return False
                
            

        return True
                
    for frame in build_frame:

        if frame[3] == 1:
            update_map.add((frame[0], frame[1], frame[2]))

            if not is_normal(update_map):
                update_map.remove((frame[0], frame[1], frame[2]))

        # 삭제한다면
        if frame[3] == 0:

            update_map.remove((frame[0], frame[1], frame[2]))

            if not is_normal(update_map):
                update_map.add((frame[0], frame[1], frame[2]))              

                    
    r = []
    for f in update_map:
        r.append(list(f))
    r.sort(key=lambda x: (x[0], x[1], x[2]))
        
    answer = r
    return answer

</code></pre></div></div>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="ps" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ai tech - Day13</title><link href="http://localhost:4000/aitech_daily/post-day13/" rel="alternate" type="text/html" title="ai tech - Day13" /><published>2023-03-22T00:00:00+09:00</published><updated>2023-03-23T06:20:02+09:00</updated><id>http://localhost:4000/aitech_daily/post-day13</id><content type="html" xml:base="http://localhost:4000/aitech_daily/post-day13/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="5f">5F</h1>
<h2 id="그날의-사실-facts-">그날의 사실 (Facts) :</h2>

<h2 id="느낌-feeling-">느낌 (Feeling) :</h2>

<h2 id="배운점-findings-">배운점 (Findings) :</h2>

<h2 id="미래의-행동계획-future-">미래의 행동계획 (Future) :</h2>

<h2 id="피드백-feedback-">피드백 (Feedback) :</h2>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech_daily" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">RNN, LSTM, GRU</title><link href="http://localhost:4000/ai/post-RNN,-LSTM,-GRU/" rel="alternate" type="text/html" title="RNN, LSTM, GRU" /><published>2023-03-21T00:00:00+09:00</published><updated>2023-03-22T10:20:02+09:00</updated><id>http://localhost:4000/ai/post-RNN,%20LSTM,%20GRU</id><content type="html" xml:base="http://localhost:4000/ai/post-RNN,-LSTM,-GRU/"><![CDATA[<p align="center">
<img src="../../../image/ai.png" width="400" height="400" />
</p>

<h1 id="rnn">RNN</h1>

<p>RNN(Recurrent Neural Network)은 시계열, 자연어 및 음성 신호와 같은 순차적 데이터로 작업하도록 설계된 인공 신경망 클래스입니다. 기존의 피드포워드 신경망과 달리 RNN에는 자체 루프백 연결이 있어 일종의 메모리 역할을 할 수 있는 숨겨진 상태를 유지할 수 있습니다. 따라서 <code class="language-plaintext highlighter-rouge">RNN은 입력 시퀀스를 처리하고 입력 데이터의 컨텍스트 또는 기록을 기반으로 결과를 예측하는 작업에 특히 적합</code>합니다.</p>

<p>RNN의 주요 구성 요소는 입력 계층, 숨겨진 계층 및 출력 계층입니다. 입력 계층은 각 시간 단계에서 입력 데이터를 받는 반면 숨겨진 계층은 이전 시간 단계에서 정보를 캡처하는 숨겨진 상태를 유지합니다. <code class="language-plaintext highlighter-rouge">출력 레이어는 현재 입력 및 숨겨진 상태를 기반으로 최종 출력 또는 예측을 생성</code>합니다.</p>

<p>RNN의 핵심 기능은 시간 단계에 걸쳐 가중치를 공유하는 기능입니다. 즉, 입력 시퀀스의 각 요소를 처리하는 데 동일한 가중치 집합이 사용됩니다. 이를 통해 RNN은 서로 다른 시퀀스 길이에 걸쳐 일반화하고 입력 데이터의 기본 구조를 학습할 수 있습니다.</p>

<p>그러나 <code class="language-plaintext highlighter-rouge">RNN에는 몇 가지 제한 사항이 있습니다. 한 가지 중요한 문제는 그래디언트 소멸 및 폭발 문제로 인해 장거리 종속성을 학습하는 데 어려움이 있다는 것입니다.</code> 이는 시간을 통한 역전파(BPTT) 알고리즘 동안 기울기가 너무 작거나 너무 커져서 가중치를 조정하고 신경망을 효과적으로 훈련시키기 어려울 때 발생합니다. 이 문제를 해결하기 위해 <code class="language-plaintext highlighter-rouge">LSTM</code>(Long Short-Term Memory) 및 <code class="language-plaintext highlighter-rouge">GRU</code>(Gated Recurrent Units)와 같은 고급 RNN 아키텍처가 개발되었습니다.</p>

<p><code class="language-plaintext highlighter-rouge">이러한 한계에도 불구하고 RNN은 자연어 처리, 기계 번역, 음성 인식 및 시계열 예측을 포함한 다양한 sequence-to-sequence 작업에 성공적으로 적용되었습니다.</code></p>

<blockquote>
  <p><strong>기울기 소실 문제를 해결해야 한다 !</strong></p>

</blockquote>

<h2 id="lstmlong-short-term-memory">LSTM(Long Short-Term Memory)</h2>

<p>LSTM(Long Short-Term Memory)은 RNN(Recurrent Neural Network) 아키텍처의 한 유형으로 기존 <code class="language-plaintext highlighter-rouge">RNN의 한계, 특히 기울기 소멸 및 폭발 문제를 해결하도록 특별히 설계</code>되었습니다. 이러한 문제는 심층 RNN을 교육할 때 발생하여 순차적 데이터에서 장거리 종속성을 학습하기 어렵게 만듭니다.</p>

<p>LSTM은 1997년 Hochreiter와 Schmidhuber에 의해 소개되었으며 이후 자연어 처리, 음성 인식 및 시계열 예측과 같은 다양한 시퀀스 간 작업에 널리 사용되었습니다.</p>

<p>LSTM 네트워크의 주요 혁신은 <code class="language-plaintext highlighter-rouge">입력 게이트, 망각 게이트 및 출력 게이트</code>의 세 가지 주요 구성 요소로 구성된 메모리 셀입니다. 이러한 게이트는 함께 작동하여 <code class="language-plaintext highlighter-rouge">네트워크를 통한 정보 흐름을 규제하므로 LSTM이 긴 시퀀스에서 내부 상태를 효과적으로 유지하고 업데이트할 수 있</code>습니다.</p>

<p>LSTM의 세 가지 주요 게이트는 각각 조금씩 다르게 작동합니다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">**input Gate**는 현재 입력과 이전 상태를 기반으로 메모리에 추가할 새 정보의 양을 결정합니다</code>. 입력 및 이전 상태에 학습된 가중치를 곱하고 시그모이드 함수를 통해 입력한 다음 시그모이드의 출력을 입력 및 다른 학습된 가중치 세트에서 계산된 후보 상태와 곱하여 이를 수행합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">**Forget Gate**는 현재 입력과 이전 상태를 기반으로 이전 상태를 얼마나 잊을 것인지 결정합니다.</code> 입력과 이전 상태에 학습된 가중치를 곱하고 시그모이드 함수를 통해 입력한 다음 시그모이드의 출력을 이전 상태와 곱하여 이전 상태를 얼마나 유지할지 결정합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">**output gate** 현재 입력과 이전 상태를 기반으로 네트워크의 다음 계층으로 출력할 현재 상태의 양을 결정합니다.</code> 입력과 이전 상태에 학습된 가중치를 곱하고 시그모이드 함수를 통해 입력한 다음 시그모이드의 출력에 현재 상태를 곱하여 현재 상태에서 출력할 양을 결정합니다.</li>
  <li>Update Cell</li>
</ul>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c4b99ef2-1d31-479a-bf49-e339559cc79b/Untitled.png" alt="Untitled" /></p>

<h2 id="grugated-reccurrent-unit">GRU(Gated Reccurrent Unit)</h2>

<p>Gated Recurrent Unit(GRU)은 표<code class="language-plaintext highlighter-rouge">준 RNN의 한계, 특히 RNN이 순차 데이터에서 장거리 종속성을 학습하기 어렵게 만드는 기울기 소실 문제를 해결하도록 설계된 순환 신경망(RNN) 아키텍처 유형</code>입니다. GRU는 Cho et al. 2014년에 LSTM(Long Short-Term Memory) 아키텍처에 대한 더 간단한 대안으로 등장했습니다.</p>

<p>GRU는 업데이트 게이트와 재설정 게이트라는 두 개의 게이트로 구성되며, 함께 작동하여 네트워크를 통한 정보 흐름을 규제하고 장거리 종속성을 효율적으로 학습할 수 있습니다. 이 게이트는 GRU가 이전 숨겨진 상태 및 현재 입력에서 유지하거나 폐기할 정보를 결정하는 데 도움이 됩니다.</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">**Update gate** : Update gate는 이전 숨겨진 상태를 유지하거나 새 입력으로 업데이트해야 하는 정도를 결정</code>합니다. 0과 1 사이의 값을 출력하는 시그모이드 활성화 함수를 사용하여 이전 숨겨진 상태와 현재 입력의 정보의 중요성을 평가합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">**Reset gate** : 재설정 게이트는 GRU가 현재 숨겨진 상태를 계산하는 데 사용되어야 하는 과거 숨겨진 상태의 양을 결정하는 데 도움</code>이 됩니다. 또한 시그모이드 활성화 기능을 사용하여 네트워크가 이전의 숨겨진 상태를 잊고 현재 입력에 집중해야 할 때를 학습할 수 있습니다.</li>
</ol>

<p>GRU와 LSTM의 주요 차이점 중 하나는 GRU에 출력 게이트와 별도의 메모리 셀이 없다는 것입니다. 이러한 단순화로 인해 GRU는 LSTM보다 계산적으로 더 효율적이지만 더 복잡한 메모리 관리가 필요한 특정 작업에서는 성능이 약간 떨어질 수 있습니다.</p>

<p>단순성에도 불구하고 GRU는 자연어 처리, 기계 번역, 음성 인식 및 시계열 예측과 같은 다양한 시퀀스 간 작업에서 뛰어난 성능을 보여주었습니다. 표준 RNN의 계산 효율성과 LSTM의 고급 메모리 관리 기능 간의 균형을 제공합니다. GRU와 LSTM의 주요 차이점 중 하나는 GRU에 출력 게이트와 별도의 메모리 셀이 없다는 것이다.</p>

<blockquote>
  <p>RNN &lt; <strong>LSTM &lt; GRU «« Transfomer</strong></p>

</blockquote>

<blockquote>
  <p>적은 파라메터로 최적화 하는 것이 일반화 성능이 높습니다.</p>

</blockquote>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ai tech - Day12</title><link href="http://localhost:4000/aitech_daily/post-day12/" rel="alternate" type="text/html" title="ai tech - Day12" /><published>2023-03-21T00:00:00+09:00</published><updated>2023-03-22T06:20:02+09:00</updated><id>http://localhost:4000/aitech_daily/post-day12</id><content type="html" xml:base="http://localhost:4000/aitech_daily/post-day12/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="5f">5F</h1>
<h2 id="그날의-사실-facts-">그날의 사실 (Facts) :</h2>

<h2 id="느낌-feeling-">느낌 (Feeling) :</h2>

<h2 id="배운점-findings-">배운점 (Findings) :</h2>

<h2 id="미래의-행동계획-future-">미래의 행동계획 (Future) :</h2>

<h2 id="피드백-feedback-">피드백 (Feedback) :</h2>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech_daily" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">[논문리뷰] Attention is all you need</title><link href="http://localhost:4000/paper/post-attention-is-all-you-need/" rel="alternate" type="text/html" title="[논문리뷰] Attention is all you need" /><published>2023-03-21T00:00:00+09:00</published><updated>0219-04-01T10:20:02+09:00</updated><id>http://localhost:4000/paper/post-attention%20is%20all%20you%20need</id><content type="html" xml:base="http://localhost:4000/paper/post-attention-is-all-you-need/"><![CDATA[<p><img src="../../../image/paper.png" alt="paper.png" /></p>

<h1 id="attention-is-all-you-need"><code class="language-plaintext highlighter-rouge">Attention is All you need</code></h1>

<p>구글의 <code class="language-plaintext highlighter-rouge">Attention is All you</code> 논문을 통해 nlp 의 혁명이 일어났습니다.  기존의 rnn 모델은 attention 모델로 대체되었습니다.  왜 이런 혁명이 일어났을 까요? 간단합니다. 뛰어난 성능 때문이죠.</p>

<p>“Attention is All You Need”는 자연어 처리 및 기계 번역과 같은 sequence-to-sequence 작업을 위한 새로운 딥 러닝 아키텍처인 Transformer 모델을 소개한 획기적인 논문입니다. Transformer 모델의 주요 혁신은 self-attention 메커니즘으로 입력 시퀀스를 순차적이 아닌 병렬로 처리하여 더 빠른 훈련과 더 나은 성능을 제공합니다.</p>

<p>이 논문의 결과는 Transformer 모델이 WMT 2014 영어-독일어 및 영어-프랑스어 번역 작업을 포함한 여러 벤치마크 작업에서 기존의 최첨단 기술을 능가한다는 것을 보여주었습니다. 트랜스포머는 병렬화 가능한 아키텍처로 인해 기존의 순환 신경망(RNN) 및 컨볼루션 신경망(CNN)보다 훨씬 적은 계산 시간으로 이러한 결과를 달성했습니다. Transformer 모델의 성공은 NLP 연구의 패러다임 전환과 그 아키텍처를 기반으로 한 수많은 후속 모델 개발로 이어졌습니다.</p>

<h2 id="attention의-원리">Attention의 원리</h2>

<p>Transformer는 자연어 처리 및 기계 번역과 같은 시퀀스 간 작업을 위해 설계된 딥 러닝 아키텍처입니다. 핵심 원리는 입력 시퀀스의 효율적인 병렬 처리를 가능하게 하는 self-attention 메커니즘입니다. 다음은 Transformer 모델에 대한 자세한 설명입니다.</p>

<p>아키텍처: Transformer는 인코더와 디코더로 구성되며 둘 다 동일한 구조의 여러 레이어로 구성됩니다. 인코더는 입력 시퀀스를 처리하고 디코더는 출력 시퀀스를 생성합니다.</p>

<p>Self-Attention: Self-Attention 메커니즘을 사용하면 모델이 시퀀스의 각 요소의 중요성을 다른 요소와 비교하여 평가할 수 있으므로 거리에 관계없이 종속성을 캡처할 수 있습니다. 이 메커니즘은 확장된 내적 어텐션을 사용하여 구현됩니다. 이 어텐션은 쿼리, 키 및 값 벡터의 내적을 취하여 시퀀스의 각 단어에 대한 어텐션 점수를 계산한 다음 소프트맥스 함수를 사용하여 확률을 생성합니다.</p>

<p>Multi-Head Attention: 모델이 단어 사이의 여러 관계를 캡처할 수 있도록 self-attention 메커니즘이 병렬로 여러 번 적용되어 여러 개의 어텐션 헤드가 생성됩니다. 이러한 어텐션 헤드의 출력은 연결되어 선형 레이어를 통과하여 최종 다중 헤드 어텐션 출력을 생성합니다.</p>

<p>위치별 피드포워드 네트워크: 다중 헤드 어텐션 레이어 이후 입력 시퀀스의 각 위치는 위치별 피드포워드 네트워크(FFN)에 의해 독립적으로 처리됩니다. FFN은 사이에 ReLU 활성화 기능이 있는 두 개의 선형 계층으로 구성됩니다.</p>

<p>위치 인코딩: Transformer 모델에는 시퀀스에서 단어의 위치에 대한 고유한 지식이 없기 때문에 위치 인코딩이 입력 임베딩에 추가됩니다. 이러한 인코딩은 빈도가 다른 정현파 함수이므로 모델이 단어의 상대적 위치에 대한 정보를 캡처할 수 있습니다.</p>

<p>계층 정규화 및 잔여 연결: 다중 헤드 어텐션 및 FFN을 포함하여 Transformer의 각 하위 계층 다음에는 계층 정규화 및 잔여 연결이 있습니다. 이러한 기술은 훈련을 안정화하고 복잡한 패턴을 학습하는 모델의 능력을 향상시키는 데 도움이 됩니다.</p>

<p>인코더-디코더 주의: 디코더에는 인코더-디코더 주의라는 추가 주의 메커니즘이 있어 디코더가 출력 시퀀스를 생성하는 동안 입력 시퀀스의 관련 부분에 집중할 수 있습니다. 이 어텐션 레이어는 셀프 어텐션 메커니즘과 유사하게 작동하지만 인코더의 출력을 키 및 값 벡터로 사용합니다.</p>

<p>선형 레이어와 소프트맥스: 마지막 디코더 레이어의 출력을 선형 레이어와 소프트맥스 함수에 통과시켜 디코더의 최종 출력을 얻는다. softmax 함수는 출력을 대상 어휘에 대한 확률 분포로 변환합니다.</p>

<p>입력 시퀀스를 병렬로 처리하는 트랜스포머의 능력은 셀프 어텐션 메커니즘과 결합되어 기존의 RNN 및 CNN보다 더 효율적으로 장거리 종속성을 처리할 수 있습니다. 이로 인해 다양한 NLP 작업에 널리 채택되고 아키텍처를 기반으로 한 후속 모델이 개발되었습니다.</p>

<h1 id="transformer">Transformer</h1>

<blockquote>
  <p>attention 은 입력 시퀀스의 일부에 선택적으로 집중하는 일반적인 메커니즘인 반면 transfomer는 입력 시퀀스를 보다 효율적이고 효과적으로 처리하기 위해 셀프 어텐션 메커니즘을 사용하는 특정 신경망 아키텍처입니다.</p>

</blockquote>

<blockquote>
  <p>Seq2seq는 기계 번역, 텍스트 요약 및 질문 응답과 같은 자연어 처리에서 시퀀스 간 작업에 사용되는 일종의 신경망 아키텍처입니다. seq2seq 아키텍처는 입력 시퀀스를 고정 길이 벡터로 인코딩하는 인코더와 인코딩된 벡터를 기반으로 출력 시퀀스를 생성하는 디코더의 두 가지 주요 부분으로 구성됩니다. seq2seq 아키텍처는 순환 신경망(RNN)을 사용하여 가변 길이 입력 및 출력 시퀀스를 처리</p>
</blockquote>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="paper" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Pytorch DataLoader DataSet</title><link href="http://localhost:4000/ai/post-Pytorch-DataLoader-DataSet/" rel="alternate" type="text/html" title="Pytorch DataLoader DataSet" /><published>2023-03-21T00:00:00+09:00</published><updated>2023-03-22T10:20:02+09:00</updated><id>http://localhost:4000/ai/post-Pytorch%20DataLoader%20DataSet</id><content type="html" xml:base="http://localhost:4000/ai/post-Pytorch-DataLoader-DataSet/"><![CDATA[<p align="center">
<img src="../../../image/ai.png" width="400" height="400" />
</p>

<h1 id="pytorch-dataloader-dataset">Pytorch DataLoader DataSet</h1>

<h2 id="옵션">옵션</h2>
<p>데이터 집합: 로드할 데이터 집합 개체를 지정하는 필수 인수입니다. 데이터 집합 개체는 torch.utils.data에서 파생된 클래스의 인스턴스여야 합니다.<strong>getitem</strong> 및 <strong>len</strong> 메서드가 구현된 데이터 세트.</p>

<p>배치_크기: 이 옵션 인수는 각 배치에서 로드하고 처리할 샘플 수를 정의합니다. 기본적으로 1(즉, 배치 없음)로 설정됩니다. 배치 크기가 클수록 교육 효율성이 향상될 수 있지만 메모리도 더 많이 필요합니다.</p>

<p>셔플: 이 선택적 부울 인수는 배치를 만들기 전에 데이터 집합을 섞을지 여부를 나타냅니다. 셔플링은 연속된 표본 간의 상관 관계를 줄임으로써 과적합을 방지하는 데 도움이 됩니다. 기본적으로 False로 설정됩니다.</p>

<p>샘플러: 이 옵션 인수를 사용하면 샘플러 개체를 사용하여 사용자 지정 샘플링 전략을 지정할 수 있습니다. torch.utils.data에서 파생된 클래스의 인스턴스여야 합니다.샘플러. 제공된 경우 샘플러가 샘플 순서를 정의하므로 shuffle 인수를 False로 설정해야 합니다.</p>

<p>num_workers: 이 옵션 인수는 데이터 로드에 사용할 하위 프로세스의 수를 지정합니다. 기본적으로 이 값은 0으로 설정되며, 이는 주 프로세스가 데이터를 로드함을 의미합니다. 작업자 수를 늘리면 데이터 로드 속도를 향상시킬 수 있지만 메모리 사용량도 증가할 수 있습니다.</p>

<p>collate_fn: 이 옵션 인수를 사용하면 개별 표본을 배치로 병합하는 사용자 정의 함수를 지정할 수 있습니다. 함수는 샘플 목록을 입력으로 가져가서 배치를 반환해야 합니다. 기본적으로 DataLoader는 torch.utils.data를 사용합니다.0번째 차원을 따라 텐서를 연결하는 _delocs.collate.default_collate.</p>

<p>핀_메모리: 이 선택적 부울 인수를 True로 설정하면 DataLoader가 고정 메모리(페이지 잠금 메모리)의 텐서를 할당하여 GPU를 사용할 때 전송 속도를 향상시킬 수 있습니다. 기본적으로 False로 설정됩니다.</p>

<p>drop_last: 이 선택적 부울 인수는 데이터 집합 크기를 배치 크기로 구분할 수 없는 경우 마지막 불완전한 배치를 삭제할지 여부를 나타냅니다. 기본적으로 False로 설정됩니다.</p>

<p>시간 초과: 이 선택적 인수는 작업자로부터 배치를 수집하기 위한 시간 초과 값(초)을 지정합니다. 기본적으로 0으로 설정되어 시간 초과가 없음을 의미합니다.</p>

<p>worker_init_fn: 이 옵션 인수를 사용하면 초기화 시 각 작업자 하위 프로세스에서 호출할 함수를 지정할 수 있습니다. 함수는 작업자 ID를 나타내는 단일 정수 인수를 사용해야 합니다.</p>

<h2 id="예시">예시</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from torch.utils.data import DataLoader

data_loader = DataLoader(
    dataset=my_dataset,
    batch_size=64,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
    drop_last=True
)
</code></pre></div></div>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ai tech - Day11</title><link href="http://localhost:4000/aitech_daily/post-day11/" rel="alternate" type="text/html" title="ai tech - Day11" /><published>2023-03-20T00:00:00+09:00</published><updated>2023-03-21T06:20:02+09:00</updated><id>http://localhost:4000/aitech_daily/post-day11</id><content type="html" xml:base="http://localhost:4000/aitech_daily/post-day11/"><![CDATA[<p><img src="../../../image/aitech.png" alt="image" /></p>

<h1 id="5f">5F</h1>
<h2 id="그날의-사실-facts-">그날의 사실 (Facts) :</h2>

<h2 id="느낌-feeling-">느낌 (Feeling) :</h2>

<h2 id="배운점-findings-">배운점 (Findings) :</h2>

<h2 id="미래의-행동계획-future-">미래의 행동계획 (Future) :</h2>

<h2 id="피드백-feedback-">피드백 (Feedback) :</h2>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="aitech_daily" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">컴퓨터 비전의 중요한 7가지 모델</title><link href="http://localhost:4000/ai/post-%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84%EC%9D%98-%EC%A4%91%EC%9A%94%ED%95%9C-7%EA%B0%80%EC%A7%80-%EB%AA%A8%EB%8D%B8/" rel="alternate" type="text/html" title="컴퓨터 비전의 중요한 7가지 모델" /><published>2023-03-20T00:00:00+09:00</published><updated>2023-03-21T10:20:02+09:00</updated><id>http://localhost:4000/ai/post-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EB%B9%84%EC%A0%84%EC%9D%98%20%EC%A4%91%EC%9A%94%ED%95%9C%207%EA%B0%80%EC%A7%80%20%EB%AA%A8%EB%8D%B8</id><content type="html" xml:base="http://localhost:4000/ai/post-%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84%EC%9D%98-%EC%A4%91%EC%9A%94%ED%95%9C-7%EA%B0%80%EC%A7%80-%EB%AA%A8%EB%8D%B8/"><![CDATA[<p align="center">
<img src="../../../image/ai.png" width="400" height="400" />
</p>

<h1 id="ai">ai</h1>
<h1 id="컴퓨터-비전의-중요한-7가지-모델">컴퓨터 비전의 중요한 7가지 모델</h1>

<p>컴퓨터 비전 분야에는 몇 가지 영향력 있는 모델과 아키텍처가 있으며, 그 중 다수는 이 분야를 발전시키는 데 중요한 역할을 했습니다. 다음은 7가지 중요한 모델입니다.</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">LeNet-5:</code> 1990년대에 Yann LeCun이 개발한 LeNet-5는 필기 숫자 인식을 위해 설계된 선구적인 컨볼루션 신경망(CNN)입니다. 이는 많은 미래 CNN 아키텍처의 토대를 마련했습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">AlexNet</code>: Alex Krizhevsky, Ilya Sutskever 및 Geoffrey Hinton이 만든 AlexNet은 2012 ImageNet Large Scale Visual Recognition Challenge(ILSVRC)에서 우승했습니다. 심층 CNN 아키텍처는 다른 모델을 훨씬 능가하여 컴퓨터 비전에서 딥 러닝의 전환점이 되었습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">VGGNet</code>: 옥스포드 대학의 Visual Geometry Group에서 개발한 VGGNet은 다중 컨벌루션 레이어가 있는 심층 아키텍처로 유명합니다. VGGNet은 2014 ILSVRC에서 2위를 차지했으며 여전히 다양한 컴퓨터 비전 작업의 벤치마크로 사용되고 있습니다.</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">인셉션(GoogLeNet):</code> GoogLeNet으로도 알려진 인셉션은 2014년 Google의 연구원들이 소개했습니다. 매우 깊은 네트워크를 효율적으로 훈련할 수 있는 혁신적인 인셉션 모듈로 2014 ILSVRC에서 우승했습니다.</p>

    <p>Inception 블록은 이미지 인식 작업을 위해 컨벌루션 신경망에 사용되는 빌딩 블록입니다. Inception 블록의 주요 이점은 동일한 레이어 내에서 다양한 규모와 복잡성의 기능을 추출하고 결합하는 기능입니다.</p>

    <p>전통적으로 컨볼루션 신경망은 서로 다른 규모의 특징을 추출하기 위해 별도의 계층을 가지고 있어 더 많은 계산 리소스와 훈련 시간이 필요합니다. 인셉션 블록은 크기가 다른 여러 필터(1x1, 3x3, 5x5)를 사용하고 동일한 레이어 내에서 작업을 풀링하여 이를 보다 효율적으로 달성할 수 있습니다. 이를 통해 네트워크는 다양한 수준의 추상화에서 기능을 학습하고 추출할 수 있으며 동시에 모델을 교육하는 데 필요한 매개변수의 수를 줄일 수 있습니다.</p>

    <p>또한 Inception 블록 내에서 1x1 컨볼루션 필터를 사용하면 3x3 및 5x5 컨볼루션과 같이 계산 비용이 더 많이 드는 작업을 적용하기 전에 기능 맵의 채널 수를 줄임으로써 네트워크의 계산 비용을 줄이는 데 도움이 될 수 있습니다. 이 기술을 차원 축소라고 합니다.</p>

    <p>전반적으로 Inception 블록은 이미지 인식 작업에서 컨볼루션 신경망의 성능을 개선하는 동시에 훈련에 필요한 계산 비용과 매개변수 수를 줄이는 것으로 나타났습니다.</p>

    <p>인셉션 블록의 맥락에서 “인셉션”이라는 용어는 새로운 것을 시작하거나 처음부터 무언가를 만드는 아이디어를 의미합니다. Inception 블록은 신경망이 미리 정의된 필터나 손으로 만든 기능에 의존하지 않고 처음부터 다양한 규모와 복잡성의 기능을 학습하고 추출할 수 있도록 설계되었기 때문에 이 이름이 붙여졌습니다.</p>

    <p>인셉션 블록은 각 인형 안에 작은 인형이 들어 있는 러시아 인형의 아이디어에서 영감을 받았습니다. 마찬가지로 Inception 블록에는 필터 크기가 다른 더 작은 레이어와 더 큰 레이어 내부의 풀링 작업이 포함되어 있습니다.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">ResNet</code>: Microsoft Research에서 개발한 ResNet(Residual Network)이 2015 ILSVRC에서 우승했습니다. 기울기 소실 문제를 겪지 않고 훨씬 더 깊은 네트워크(최대 152개 계층)를 훈련할 수 있는 잔류 연결(연결 건너뛰기)을 도입했습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">YOLO</code>(You Only Look Once): YOLO는 Joseph Redmon, Santosh Divvala, Ross Girshick 및 Ali Farhadi가 만든 실시간 객체 감지 시스템입니다. 속도와 정확성으로 유명하여 실시간 응용 프로그램에 적합합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">Mask R-CNN</code>: Kaiming He, Georgia Gkioxari, Piotr Dollár 및 Ross Girshick이 소개한 Mask R-CNN은 Faster R-CNN 객체 감지 모델의 확장입니다. 분할 마스크를 예측하기 위한 분기를 추가하여 이미지 내에서 개체를 동시에 감지하고 분할하는 작업인 인스턴스 분할을 활성화합니다.</li>
</ol>]]></content><author><name>최윤진</name><email>yunjinchoidev@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[]]></summary></entry></feed>