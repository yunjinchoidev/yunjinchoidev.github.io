---
title: "Post: 딥러닝에 필요한 math 정리"
last_modified_at: 2023-03-07T16:20:02-05:00
categories:
    - aitech
tages:
    - aitech
    - nlp
---
# boostcamp_AITech_5th
![image](../../../image/aitech.png)


* # <벡터> 

---
* # <행렬>
  * 행렬곱
    * 행과 열의 조건을 맞춰야 연산가능함.
  * 역행렬
    * 곱의 연산을 했을 때 항등행렬이 나온다.
  * 유사 역행렬
    * 무어펜로스(Moore-Penrose) 역행렬
      * ![무어펜로즈.png](무어펜로즈.png)
      * `np.linalg.pinv`
      * 행과 열의 크기에 따라 순서가 달라지니 유의 할 것.
    * 연립방정식, 선형회귀 분석에 응용됨.
  * `생각`
    * 왜 행렬을 배우는가? 차원이동, 연산
  * 문제
    * 사이킷런의 선형회귀 모델과 무어펜로즈의 선형회귀를 직접 구현해보시오.

#### 딥러닝을 제대로 이해하기 위해선 '선형대수학'을 수준 높게 학습해야 한다

---
# <경사하강법> 
* 미분
  * 접선의 기울기
  * sysmpy.diff -> 미분계산 가능
  * **<어느 차원에서든>** 그 점에서 증가하는가, 감소하는가를 알 수 있다.
  * 어느 차원으로 확장한다는 것은 변수를 스칼라가 아닌 벡터를 사용한다는 것.
* 경사하강법
  * 감소하는 방향으로 쭈욱 가다 **보면** 언젠가 평지를(극소값)을 만날 것이야 => 경사하강법
  * 컴퓨터에서 미분값이 0 인 곳을 찾기란 쉽지 않으므로 아주 작은 값(오메가) 보다 더 미분 값이 작으면 종료하면 되시것다.
  * 학습률 -> 얼마나 큰 보폭으로 갈것인가? (하이퍼 파라메터)
  * 모든 데이터 사용
    * 성능이 확률적 경사 하강법에 비해 좋지 않고, 하드웨어에 부담이 간다.
* 변수가 벡터인경우엔
  * 편미분을 사용한다.
* 그레디언트 벡터
  * 벡터의 변수 별로 편미분을 계산한 함수
  * ![gradient.png](gradient.png)
  * 그레디언트 벡터에 - 를 붙여서 이동하게 되면 **가장 빨리** 극소값을 향해 가게 된다.
  * 그레디언트 벡터의 `norm` 값을 구해서 일정 값보다 작게 되면 학습을 종료하는 방식으로 극소값을 찾으면 된다. (계속 내려가다가 더 이상 변화가 없는 것 같아. -> 일단 멈춰!)
# 선형회귀 모델에서 경사하강법 적용해보기
* 위에서 무어 펜로즈 행렬을 통해서 선형 모델을 어떻게 찾아야 할 지 알았다.
* 우리가 해야 할것은 무어펜로즈 행렬을 곱해서 나온 우변 식을 경사하강법을 하지 않고 !! 오직 경사 하강법을 적용시켜 극소값이 되는 지점을 찾는 것이다.
* #####  this is funny ! 
  * 직접 계산해보자.
* 학습률과 학습횟수를 적절하게 선택했을 때만 수렴을 보장할 수 있다.
* 비선형회귀의 경우, 볼록하지 않기 때문에 수렴을 보장할 수 없다.

# 확률적 경사하강법
*   데이터를 일부 사용해서 경사하강법을 적용하는 것.
  * 데이터 하나만 사용

# 미니배치 확률 경사하강법 (일반적)
  * 데이터 여러개 사용 
  * SGD 가 경사하강법보다 낫다는 것이 실증적으로 검증되었다.
  * 데이터를 일부로 사용하기 때문에 목적식이 매번 달라짐.
  * 학습률, 미니배치사이즈 고려해야 함.


---
# 5. 딥러닝 학습방법 
* 이제 비선형모델인 신경망을 도전해보자.
* 소프트맥스 
  * 모델의 출력을 확률로 해석 => 확률 벡터로 변환
    * 가장 큰 값이 예측값이 되는 것임.
* 신경망은 **선형모델과 활성함수를 합성한 함수**다. [MLP]
  * 잠재벡터들의 누적 공간에 활성화 함수를 적용한다.
* 활성함수
  * R 위에 정의된 비선형 함수
  * 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다.
  * ex) Relu, tanh, sigmoid
* 순전파
  * 주어진 신경망 계산을 하는 것
* 왜 층을 여러개 쌓는가?
  * 수학적으로 임의의 연속함수를 근사할 수 있다는 것이 증명되어 있다.
  * '세계'를 표현할 수 있다.
* 역전파
  * 경사하강법을 이용해서 가중치를 업데이트 하는 것.
  * 위층부터 저층으로 그레디어트 벡터를 전달해야 함(연쇄법칙)
  * 메모리에 저장해서 사용해야 함.
  * tensorflow, pytorch 에는 자동 구현되어 있음.


---
# 6. 확률론
* 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.
  * 확률론이 중요한 이유
  * 사례
    * 분류 문제
      * 예측 오차의 분산을 최소화
    * 교차 엔트로피 문제
      * 모델 예측의 불확실성을 최소화
* 확률 분포는 데이터의 초상화
* 확률은 면적이다.
* 확률변수
  * 이산 확률 변수
    * 이산 확률을 급수(시그마)
  * 연속 확률 변수
    * 연속 확률을 적분
* 목표
  * 데이터의(표본) 확률 분포를 가지고 -> 실제 (모집단)의 확률 분포를 추정한다.
* 주변확률분포
* 통계값(모수)
  * 기댓값
  * 분산
  * 첨도
  * 공분산
  * 조건부확률
* **몬테카를로 샘플링**
  * 데이터(샘플링)를 통해 기댓값을 계산하는 방법
  * 대수의 법칙을 통해 수렴성을 보장한다.

```
import numpy as np

def monte(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high - low)
    stat = []

    for _ in range(repeat):
        x = np.random.uniform(low=low, high=high, size=sample_size)
        fun_x = fun(x)
        int_val = int_len * np.mean(fun_x)
        stat.append(int_val)

    return np.mean(stat), np.std(stat)


- 주어진 함수
def f_x(x):
    return np.exp(-x**2)


print(monte(f_x, low=-1, high=1, sample_size=1000, repeat=100))

```




---
# 7. 통계학
- 모수
  - 통계적 모델링 : 확률분포를 추정하는 것.
  - 모수 추정
    - 모수적 방법론
      - 특정 확률 분포를 가정 후 모수를 추정하는 것.
    - 비 모수적 방법론
      - 가정 없이.
- 데이터 관찰 -> 확률 분포 가정
  - 베르누이 분포
  - 카테고리 분포
  - 베타 분포
  - 감마분포, 로그정규분포
  - 정규분포, 라플라스분포
- 데이터 관찰 -> 확률 분포 가정 -> 평균, 분산 추정 -> 모수 집단 추청
- 중심극한정리
  - 표뵨 평균의 표집분포는 N이 커질수록 정규분포를 따른다.
- 표집분포
  - 통계량의 확률분포
- 최대가능도 추정법
  - **가장 가능성이 높은 모수를 추정하는 방법**
  - x 에서 가장 높은 모수 집단 (가능도와 관점의 차이)
  - 기계학습에서 많이 쓰인다.
  - 가능도
    - **모수 쌔타 분포에서 데이터 x 를 발견할 가능성.**
    - 계산상 `로그가능도` 사용
- 정규분포에서 최대 가능도 추정
- 카테고리 분포에서 최대 가능도 추정
- 딥러닝에서 최대가능도 추정법
  - 소프트맥스 벡터 => 카테고리 분포
  - 두 개의 확률분포의 손실함부를 학습시키기
    - 쿨백-라이블러 발산을 최소화
    - 분류 문제에서 정답레이블을 P, 모델 예측을 Q 라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같음
- 최대 우도법
  - 데이터가 많이 모인 상황에서 `특정 데이터의 분포` 를 가정하고 `제일 가능성 높은` 모수를 추정하는 것.
  - 이 데이터들은 가장 그럴싸한 모수로부터 나왔을 것이다!
  - 정답 확률분포와 모델 추정 확률분포 손실함수 구해서 학습 시키는 방식으로 구하면 된다.
  - 확률과 가능도의 차이
    - 확률 : 이 사건이 일어날 경우의 수를 전체 사건의 수로 나눈 것.
    - 가능도 : 지금 얻은 데이터가 이 분포에서 나왔을 가능도
  - 가능도를 사용하는 이유는 우리는 모수를 알 수 없기 때문이다. 데이터로부터 실제 세계를 추정해야 한다. 그게 DL 이다.
  - DL 에서 많이 쓰이니 잘 기억하자. 왜냐하면 가중치를 결정하는 데 도움을 주기 때문 ? 
  - [설명링크](https://www.youtube.com/watch?v=XhlfVtGb19c)
  - [설명블로그](https://angeloyeo.github.io/2020/07/17/MLE.html)
  - 재요약
    - 특정한 분포 $\theta$ 에서 관찰된 데이터 집합 x 의 가능도는 확률의 곱이라 할 수 있음. 
    -  가능도 값이 최대가 되도록 하는 $\theta$ 를 찾자. 그것이 최대 우도법이다.

- 쿨백-라이블러 발산
  - **쿨백-라이블러 발산(Kullback-Leibler divergence)**은 두 확률분포 p(y), q(y) 의 분포모양이 얼마나 다른지를 숫자로 계산한 값
  - 그 값은 항상 양수이며 두 확률분포 완전히 같을 경우에만 0이 된다.
  - 교차엔트로피 개념





---
# 8. 베이즈 통계학
* 조건부 확률을 이용하면 정보를 갱신하는 방법을 알 수 있음.
* A 라는 사건이 추가로 주어졌을 때 B 가 일어날 확률은
  * B가 일어날 확률 * (P(A|B) / P(A))
* 사후확률 = 사전확률 * 가능도/증거
* 1종 오류, 2종 오류
* 갱신된 사후확률을 계산 할 수 있다. (연속된 계산) (ex. 두번 검진)
  * 정확도를 높일 수 있다.
* 조건부 확률을 토대로 인과관계를 함부로 추론해서는 안된다.
  * 데이터에 따라 달라질 수 있기 때문임.
  * 중첩효과를 제거해서 가짜 연관관계를 제거해야 함.
  * 인과관계를 뒷받침하는 모델로서 작동하는 조건부확률
* 상관관계와 인과관계는 다르다.
  * 키와 지능의 연관관계
    * 상관관계 성림
    * 인과관계는 성립하지 않음.
* 베이즈 추정법
  * 베이즈 추정법(Bayesian estimation)은 모숫값이 가질 수 있는 모든 가능성의 분포를 계산하는 작업이다.



---
# 9. CNN
- 이미지 연산에서 많이 사용하는 모델
- 국소적 증폭, 국소적 증감
- 여러 차원에서 적용 가능
  - 채널이 여러개인 경우 커널의 채널 수와 입력의 채널수가 같아야 한다.
- 순전파
  - 커널을 벡터상에서 움직여 함성함수를 적용.
- 역전파
  - Convolution 연산에서 어떻게 역전파를 적용할 것인가 ? 
  - [참고자료](https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/)



---
# 10. RNN
* 시퀀스데이터
  * 연속적인 데이터
  * 발생 순서가 기록되어 있는 데이터
  * 독립 동등 분포를 위배한다.
    * 데이터의 순서가 중요하다.
    * 과거의 데이터가 중요하다.
    * 하지만 모든 과거 데이터가 필요한 것은 아님 => 어텐션 모델 (`Attention all you need)`
* 시퀀스데이터의 조건부확률
* 시퀀스데이터는 과거의 길이를 사용해야 하는데 이것은 가변적인 데이터를 다뤄야 한다는 것을 의미한다.
  * 하지만 이것을 타우라고 한다면 해결할 수 있다.
* 순전파
* 역전파
  * BPTT(Backprogaion Throgh Time) 을 사용한다.
    * BPTT 미분은 매우 복잡하다. [링크](https://davi06000.tistory.com/92)
    * [링크](https://mmuratarat.github.io/2019-02-07/bptt-of-rnn)
  * **기울시 소실 문제 발생** => 과거 데이터를 반영 못하는 문제 => 길이를 끊어서 나눠서 학습
  * => LSTM, GRU 와 같은 고급 모델의 탄생

---
# 생각해 볼 문제
- 
-  