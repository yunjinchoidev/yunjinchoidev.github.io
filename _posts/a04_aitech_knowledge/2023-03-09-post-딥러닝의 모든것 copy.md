---
title: "딥러닝의 모든 것(All about Deep Learning)"
last_modified_at: 2023-03-08T16:20:02-05:00
categories:
    - a04_aitech_knowledge
tages:
    - aitech
    - nlp
toc: true
toc_sticky: true
toc_label: "목차"
use_math: true
sidebar:
    nav: "counts"
---




![image](../../../image/aitech.png)


# 누가 좋은 딥러너인가 ?

누가 좋은 딥러너 일까요? 수학적 지식이 뛰어나야 합니다. 논문을 잘 읽어야 하고요. 구현 능력이 뛰어나야 합니다.  여러분들은 그 실력을 키우시기 바랍니다.

```
논문을 읽거나 모델을 연구할 때

    1. 데이터 
    2. 모델 
    3. 손실함수 
    4. 학습 알고리즘 

를 고려합시다.
```



## 공부합시다.

```

1. Historical Review
2. Neural Networks & Multi-Layer Perceptron
3. Optimization Methods
4. Convolutional Neural Networks
5. Modern CNN
6. Computer Vision Applications
7. Recurrent Neural Networks
8. Transformer
9. Generative Models Part1
10.Generative Models Part2
```

## 딥러닝 역사의 중요한 기점들.

```
- AlexNet(2012) -> 딥러닝의 가능성을 폭발적으로 알려주었다. 
- DQN (2013) -> 강화학습
- Encoder / Decoder (2014)
- Adam (2014) -> 결과가 매우 좋다.
- GAN, ResNet (2015) 
- Residual Networks (2015) -> 네트워크를 깊게 쌓게 해준다.
- Transformer (2017) -> 도발적
- Bert (2018)
- Big Language Models (GPT)
- Self-Supervised Learning -> 학습 데이터를 스스로 생성한다.
```

# 딥러닝 개괄

## 신경망

신경망에서 비선형 함수를 넣어주는 이유가 무얼까? 선형적 함수로만 표현을 하게 되면은 그것은 결국 선형함수 하나로 변환 될 수 있다는 것을 의미한다. 그렇기 때문에 선형적으로 표현할 수 없는 비선형 함수 (예를 들어 활성화 함수) 를 넣어줌으로써 표현력을 극대화하는 것이다.

## 손실함수의 종류

- MSE
- CE
- MLE



# Deep Learning’s Most Important Ideas - A Brief Historical Review

# 혁신들

## 2012 – AlexNet

AlexNet은 2012년 Alex Krizhevsky, Ilya Sutskever 및 Geoffrey Hinton이 개발한 심층 컨볼루션 신경망(CNN`) 아키텍처다. 오류율은 15.3%로 두 번째로 좋은 모델보다 훨씬 뛰어나다.`

`AlexNet은 8개의 계층으로 구성되어 있다. 5개의 컨볼루션 계층과 3개의 완전 연결 계층이 있습니다. 이 네트워크는 ReLU(Rectified Linear Unit) 활성화 함수를 사용하여 시그모이드 또는 tanh와 같은 기존 활성화 함수에 비해 더 빠르게 훈련할 수 있습니다. 또한 AlexNet은 최대 풀링 계층, 로컬 응답 정규화(LRN) 및 드롭아웃 정규화를 사용하여 과적합을 방지합니다.`

`AlexNet의 성공은 이미지 분류, 객체 감지 및 의미론적 분할과 같은 광범위한 컴퓨터 비전 작업을 위한 딥 러닝, 특히 CNN에 대한 관심의 물결을 촉발시켰습니다. 오늘날 AlexNet은 VGG, ResNet 및 Inception과 같은 고급 아키텍처를 위한 길을 닦은 기본 모델로 간주됩니다.`

## 2013 – DQN

DQN(Deep Q-Network)은 2013년 `DeepMind` 연구원들이 개발한 강화 학습 알고리즘입니다. 인기 있는 무모델 강화 학습 방법인 Q-러닝과 심층 신경망을 결합하여 원시에서 직접 학습할 수 있는 강력한 알고리즘을 만듭니다. 복잡한 환경의 픽셀 데이터. DQN은 여러 게임에서 인간 수준의 성능을 달성한 Atari 2600 게임에서 처음 시연되었습니다.

`DQN의 주요 혁신은 주어진 상태에서 행동을 취하는 데 대한 예상되는 미래 보상을 예측하는 행동-가치 함수 또는 Q-함수를 추정하기 위한 함수 근사값으로 심층 신경망을 사용하는 것`입니다. DQN은 두 가지 주요 기술을 도입하여 강화 학습을 위한 심층 신경망 훈련에서 종종 발생하는 불안정성과 발산 문제를 해결합니다.

1. `경험 재생:` 순차적 경험에서 학습하는 대신 DQN은 경험(상태, 작업, 보상 및 다음 상태)을 재생 버퍼에 저장하고 훈련 중에 이 버퍼에서 임의의 미니 배치를 샘플링합니다. 이것은 연속 경험 간의 상관 관계를 깨고 보다 안정적인 학습으로 이어집니다.
2. `대상 네트워크:` DQN은 메인 네트워크의 가중치로 주기적으로 업데이트되는 별도의 대상 네트워크를 사용합니다. 이렇게 하면 지속적으로 변화하는 목표 값으로 인해 발생하는 진동 및 발산 가능성을 줄여 학습 프로세스를 안정화하는 데 도움이 됩니다.

DQN은 Double DQN, Dueling DQN 및 Prioritized Experience Replay와 같은 많은 확장 및 개선 사항에 영감을 주어 심층 강화 학습 분야에서 영향력 있는 알고리즘이었습니다.

알파고를 만든 알고리즘 입니다.

## 2014 – Encoder/Decoder, Adam

인코더/디코더:

`인코더-디코더 아키텍처는 특히 기계 번역, 텍스트 요약 및 이미지 캡션과 같은 시퀀스 간(seq2seq) 작업을 위한 딥 러닝에서 널리 사용되는 프레임워크`입니다. 아키텍처는 두 가지 주요 구성 요소로 구성됩니다.

1. `인코더`: `인코더는 입력 시퀀스를 처리하고 이를 고정 크기 컨텍스트 벡터 또는 잠재 표현으로 압축하는 신경망(일반적으로 RNN 또는 LSTM)입니다.` `이 벡터는 디코딩 프로세스에 필요한 입력 시퀀스에서 필수 정보를 캡처`합니다.
2. `디코더`: 디코더는 인코더에서 `생성된 컨텍스트 벡터를 가져와 단계적으로 출력 시퀀스를 생성하는 또 다른 신경망(일반적으로 RNN 또는 LSTM)입니다.` 컨텍스트 벡터와 이전 예측을 기반으로 출력 시퀀스의 `다음 요소를 예측합니다.`

인코더-디코더 아키텍처는 디코더가 디코딩 프로세스 중에 입력 시퀀스의 특정 부분에 집중할 수 있게 하여 긴 시퀀스에서 성능을 향상시키는 어텐션 메커니즘과 결합되는 경우가 많습니다.

Adam(적응 모멘트 추정):

Adam은 2014년에 Diederik Kingma와 Jimmy Ba가 소개한 신경망의 기울기 기반 최적화를 위한 최적화 알고리즘입니다. 이것은 확률적 기울기 하강(SGD) 방법의 확장이며 각 매개변수에 대한 학습 속도를 개별적으로 조정하도록 설계되었습니다. `Adam은 그래디언트의 1차 모멘트(평균)와 2차 모멘트(비중심 분산)를 모두 계산하여 두 가지 인기 있는 적응형 학습 속도 기술인 AdaGrad 및 RMSProp의 이점을 결합`합니다.

Adam의 주요 기능은 다음과 같습니다.

1. `적응형 학습 속도`: Adam은 기울기의 첫 번째 및 두 번째 모멘트를 기반으로 각 매개변수에 대한 학습 속도를 조정하여 표준 SGD에 비해 더 빠른 수렴 및 개선된 일반화로 이어질 수 있습니다.
2. `모멘텀`: Adam은 기울기의 지수 이동 평균을 사용하여 모멘텀을 통합하여 수렴을 가속화하고 최적화 환경에서 로컬 최소값 또는 안장 지점을 극복하는 데 도움이 될 수 있습니다.
3. `편향 보정`: Adam은 모멘트가 0으로 편향될 수 있는 훈련 시작 시 첫 번째 및 두 번째 모멘트의 정확한 추정을 보장하기 위해 편향 수정 항을 포함합니다.

Adam은 구현 용이성, 빠른 수렴, 다양한 신경망 아키텍처 및 문제에 대한 견고성으로 인해 딥 러닝에서 널리 사용되는 최적화 알고리즘이 되었습니다.

## 2015 – GAN, ResNet

GAN(생성적 적대 신경망):
GAN은 2014년 이안 굿펠로우(Ian Goodfellow)와 그의 동료들이 소개한 생성 모델입니다. 두 개의 신경망인 생성기와 판별기로 구성되며 경쟁 환경에서 동시에 훈련됩니다. Generator는 합성 데이터 샘플을 생성하고 Discriminator는 실제 샘플과 생성된 샘플을 구별하는 방법을 학습합니다. 교육 과정은 현실적인 샘플을 생성하는 생성기의 능력과 진짜와 가짜 샘플을 정확하게 식별하는 판별기의 능력을 향상시키는 것을 목표로 합니다. 훈련이 진행됨에 따라 생성기는 점점 더 사실적인 샘플을 생성하는 능력이 향상되어 판별자가 실제 데이터와 가짜 데이터를 구별하기가 더 어려워집니다. GAN은 이미지 합성, 스타일 전송, 데이터 확장 및 도메인 적응을 포함한 다양한 응용 분야에서 널리 사용되었습니다.

ResNet(잔여 네트워크):
ResNet은 2015년 Kaiming He와 그의 동료들이 소개한 심층 합성곱 신경망(CNN) 아키텍처입니다. 심층 신경망의 훈련을 방해하는 기울기 소멸 문제를 해결하도록 설계되었습니다. ResNet은 그라디언트가 네트워크를 통해 보다 효과적으로 흐를 수 있도록 건너뛰기 연결 또는 바로가기 연결을 도입했습니다. 이러한 연결은 블록의 입력과 출력 사이의 잔차 함수를 학습하는 잔차 블록을 생성합니다. 이 아키텍처는 정확도를 유지하고 기울기 소실 문제를 완화하면서 훨씬 더 깊은 네트워크(원본 논문에서 최대 152개 계층)의 교육을 가능하게 합니다. ResNet은 이미지 분류, 객체 감지 및 의미론적 분할과 같은 다양한 컴퓨터 비전 작업에서 최첨단 결과를 달성했으며 딥 러닝 아키텍처의 수많은 변형 및 개선에 영감을 주었습니다.

## 2016 –

## 2017 – Transformer

`Attention is All you need`

Transformer는 Vaswani 등이 도입한 신경망 아키텍처입니다. 2017년에 기계 번역과 같은 시퀀스 간 작업을 위해 설계되었습니다. 자가 주의 메커니즘을 사용하여 순환 신경망(RNN)의 한계를 극복하여 더 효율적인 병렬화와 장거리 종속성을 더 잘 처리할 수 있습니다. `트랜스포머는 인코더와 디코더로 구성되어 있으며 각각 여러 레이어의 자체 주의 및 피드포워드 하위 레이어가 있어 모델이 입력 및 출력 시퀀스의 요소 간의 복잡한 관계를 학습`할 수 있습니다.

> Attention 구조를 이해하는 게 정말 중요하다.
> 

## 2018 – Bert

BERT (Bidirectional Encoder Representations from Transformers):

BERT는 2018년 Google에서 도입한 사전 학습된 Transformer 기반 언어 모델입니다. `양방향 컨텍스트를 사용`하여 레이블이 지정되지 않은 텍스트 데이터에서 풍부한 언어 표현을 학습함으로써 자연어 처리(NLP)에 혁명을 일으켰습니다. `BERT는 마스킹된 언어 모델링 및 다음 문장 예측 작업을 사용하여 대규모 비지도 데이터에 대해 사전 훈련`됩니다. 감정 분석 또는 명명된 엔터티 인식과 같은 특정 다운스트림 작업에서 `BERT를 미세 조정하면 상대적으로 최소한의 추가 교육으로 최첨단 성능`을 얻을 수 있습니다.

> Find Tuning 의 시초
> 

## 2019 – Big Language Models(GPT-X)

GPT(Generative Pre-trained Transformer):
GPT는 OpenAI에서 개발한 일련의 Transformer 기반 언어 모델이며, GPT-3는 2021년 9월 제 지식 컷오프 당시 가장 강력한 최신 버전입니다. `GPT는 다음 단어를 예측하는 단방향 자동 회귀 접근 방식을 사용하여 훈련`됩니다. `이전 단어를 기반으로 순서대로. 방대한 양의 텍스트 데이터에 대해 사전 훈련된 GPT`는 `강력한 일반화 기능`을 나타내`며 텍스트 생성, 요약, 번역 및 질문 답변과 같은 다양한 NLP 작업에 대해 미세 조정할 수 있`습니다.

## 2020 – Self-Supervised Learning

Self-Supervised Learning 자기 지도 학습:
`자기 지도 학습은 모델이 명시적으로 레이블이 지정된 데이터를 사용하지 않고 데이터 자체에서 유용한 표현이나 기능을 학습하는 일종의 비지도 학습`입니다. 대신 문장에서 누락된 단어를 예측(마스킹된 언어 모델링)하거나 두 문장의 상대적 위치를 예측(다음 문장 예측)하는 등 데이터에서 감독 신호를 생성하도록 설계된 작업인 `프리텍스트 작업을 사용`합니다. 자기 지도 학습은 BERT 및 GPT와 같은 모델이 레이블이 지정되지 않은 방대한 양의 데이터에서 풍부하고 전송 가능한 표현을 학습할 수 있도록 하므로 딥 러닝, 특히 NLP에서 점점 인기를 얻고 있습니다.


딥러닝은 Neural 을 모방해서 만들기 시작했지만 지금은 더 수학적으로 구분되어 지고 있다. 예를 들어 새와 전투기를 생각해보자. Neural Network 는 손실함수를 최소화하는 하나의 시스템이다. 과연 인간의 뇌에서 오차역전파같은 일들이 벌어질까? 

선형 결합을 풍부하게 만들어주기 위해서는 중간에 Nonlinear transfom 함수를 넣어줘야 한다. 선형 결합은 하나의 선형결합으로 환원될 수 있기 때문이다. 

MLP 에서 손실함수

- 회귀 문제 : `MSE`
- 분류 문제 : `CE`
- 확률 문제 : `MLE`

# Futher Question

- `과연 분류 문제에서 CE 함수를 사용하는 것이 최선일까?`
    
    교차 엔트로피(CE) 손실 함수는 일반적으로 분류 문제에 사용되지만 "최상의" 선택인지 여부는 특정 문제 및 관심 있는 성능 메트릭에 따라 다릅니다.
    
    CE 손실 함수는 예측 확률과 실제 레이블 간의 차이를 측정하도록 설계되었으며 몇 가지 바람직한 속성을 가지고 있습니다. 예를 들어, 그들은 볼록합니다. 즉 경사 하강법 최적화 알고리즘이 전역 최소값을 찾을 수 있습니다. 또한 올바른 예측보다 부정확한 예측에 더 많은 벌점을 부과합니다. 이는 거짓 긍정과 거짓 부정의 비용이 서로 다른 분류 작업에 유용합니다.
    
    그러나 CE 손실 함수는 모든 분류 문제에 대해 최선의 선택이 아닐 수 있습니`다. 예를 들어, 클래스가 불균형한 경우(즉, 한 클래스의 인스턴스가 다른 클래스보다 훨씬 더 많다는 의미) CE 손실이 다수 클래스 쪽으로 편향되어 소수 클래스의 성능을 정확하게 캡처하지 못할 수 있습니다.` 이러한 경우 초점 손실 또는 클래스 균형 손실과 같은 대체 손실 함수가 더 적절할 수 있습니다.
    
    또한 관심 있는 성능 메트릭이 분류 정확도가 아니라 정밀도, 재현율 또는 F1 점수와 같은 항목인 경우 CE 손실을 최적화해도 전체 성능이 최상이 되지 않을 수 있습니다. 이러한 경우 사용자 지정 손실 함수 또는 최적화 절차를 사용하여 원하는 성능 지표에 대해 직접 최적화하는 것이 더 효과적일 수 있습니다.
    
    `요약하면, CE 손실 함수는 많은 분류 문제에 대해 널리 사용되고 효과적인 선택이지만 모든 경우에 "최상의" 선택은 아닐 수 있습니다`. 손실 함수의 선택은 `관심 있는 특정 문`제 및 `성능 메트릭`에 따라 달라집니다.


















## Bias and Variance

cost를 최소화 한다는 것은 bias, variance, noise 를 최소화하는 것이다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e9c6c94b-a33c-4d71-aee7-6f00be330d8e/Untitled.png)

## 최적화

최적화 이론은 딥러닝에서 매우 중요한 개념입니다. 
일반화와 최적화의 균일점을 찾아라 ! 그래서 우리는 cross-validation 을 사용합니다.

- Bagging
학습데이터를 여러개를 만들어서 부트스트래핑을 하는것을 말합니다.  부트스트랩이란 ? 수 많은 데이터 중에 랜덤 샘플링을 여러개 만들어서 예측을 하는
- Boosting
잘 안되는 데이터에 맞춰서 학습을 여러번 시키는 방법을 말한다.

## Gradinet Descent

어떻게 산 밑으로 내려갈 것인가? 

- 여러가지 테크니션
    - 모멘텀
    - Nesterov Accelerated Gradinet
    - Adagrad
    - Adadelta
    - RMSStop

## Regularization

 오버피팅을 방지 하세요.

- Early stopping
- Parameter norm penalty
- Data augmentation
- Noise robustness
  - 노이즈를 넣으면 이상하게 잘된다.
- Lable smoothing
  - Cut Mix 
  - Cut Out 
  - Mixup
- Dropout
- Batch normalizaion