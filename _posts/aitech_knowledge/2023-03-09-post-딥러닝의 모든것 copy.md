---
title: "딥러닝의 모든 것(All about Deep Learning)"
last_modified_at: 2023-03-08T16:20:02-05:00
categories:
    - aitech_knowledge
tages:
    - aitech
    - nlp
toc: true
toc_sticky: true
toc_label: "목차"
use_math: true
sidebar:
    nav: "counts"
---




![image](../../../image/aitech.png)


# 누가 좋은 딥러너인가 ?

누가 좋은 딥러너 일까요? 수학적 지식이 뛰어나야 합니다. 논문을 잘 읽어야 하고요. 구현 능력이 뛰어나야 합니다.  여러분들은 그 실력을 키우시기 바랍니다.

```
논문을 읽거나 모델을 연구할 때

    1. 데이터 
    2. 모델 
    3. 손실함수 
    4. 학습 알고리즘 

를 고려합시다.
```



## 공부합시다.

```

1. Historical Review
2. Neural Networks & Multi-Layer Perceptron
3. Optimization Methods
4. Convolutional Neural Networks
5. Modern CNN
6. Computer Vision Applications
7. Recurrent Neural Networks
8. Transformer
9. Generative Models Part1
10.Generative Models Part2
```

## 딥러닝 역사의 중요한 기점들.

```
- AlexNet(2012) -> 딥러닝의 가능성을 폭발적으로 알려주었다. 
- DQN (2013) -> 강화학습
- Encoder / Decoder (2014)
- Adam (2014) -> 결과가 매우 좋다.
- GAN, ResNet (2015) 
- Residual Networks (2015) -> 네트워크를 깊게 쌓게 해준다.
- Transformer (2017) -> 도발적
- Bert (2018)
- Big Language Models (GPT)
- Self-Supervised Learning -> 학습 데이터를 스스로 생성한다.
```

# 딥러닝 개괄

## 신경망

신경망에서 비선형 함수를 넣어주는 이유가 무얼까? 선형적 함수로만 표현을 하게 되면은 그것은 결국 선형함수 하나로 변환 될 수 있다는 것을 의미한다. 그렇기 때문에 선형적으로 표현할 수 없는 비선형 함수 (예를 들어 활성화 함수) 를 넣어줌으로써 표현력을 극대화하는 것이다.

## 손실함수의 종류

- MSE
- CE
- MLE



# Deep Learning’s Most Important Ideas - A Brief Historical Review

# 혁신들

## 2012 – AlexNet

AlexNet은 2012년 Alex Krizhevsky, Ilya Sutskever 및 Geoffrey Hinton이 개발한 심층 컨볼루션 신경망(CNN`) 아키텍처다. 오류율은 15.3%로 두 번째로 좋은 모델보다 훨씬 뛰어나다.`

`AlexNet은 8개의 계층으로 구성되어 있다. 5개의 컨볼루션 계층과 3개의 완전 연결 계층이 있습니다. 이 네트워크는 ReLU(Rectified Linear Unit) 활성화 함수를 사용하여 시그모이드 또는 tanh와 같은 기존 활성화 함수에 비해 더 빠르게 훈련할 수 있습니다. 또한 AlexNet은 최대 풀링 계층, 로컬 응답 정규화(LRN) 및 드롭아웃 정규화를 사용하여 과적합을 방지합니다.`

`AlexNet의 성공은 이미지 분류, 객체 감지 및 의미론적 분할과 같은 광범위한 컴퓨터 비전 작업을 위한 딥 러닝, 특히 CNN에 대한 관심의 물결을 촉발시켰습니다. 오늘날 AlexNet은 VGG, ResNet 및 Inception과 같은 고급 아키텍처를 위한 길을 닦은 기본 모델로 간주됩니다.`

## 2013 – DQN

DQN(Deep Q-Network)은 2013년 `DeepMind` 연구원들이 개발한 강화 학습 알고리즘입니다. 인기 있는 무모델 강화 학습 방법인 Q-러닝과 심층 신경망을 결합하여 원시에서 직접 학습할 수 있는 강력한 알고리즘을 만듭니다. 복잡한 환경의 픽셀 데이터. DQN은 여러 게임에서 인간 수준의 성능을 달성한 Atari 2600 게임에서 처음 시연되었습니다.

`DQN의 주요 혁신은 주어진 상태에서 행동을 취하는 데 대한 예상되는 미래 보상을 예측하는 행동-가치 함수 또는 Q-함수를 추정하기 위한 함수 근사값으로 심층 신경망을 사용하는 것`입니다. DQN은 두 가지 주요 기술을 도입하여 강화 학습을 위한 심층 신경망 훈련에서 종종 발생하는 불안정성과 발산 문제를 해결합니다.

1. `경험 재생:` 순차적 경험에서 학습하는 대신 DQN은 경험(상태, 작업, 보상 및 다음 상태)을 재생 버퍼에 저장하고 훈련 중에 이 버퍼에서 임의의 미니 배치를 샘플링합니다. 이것은 연속 경험 간의 상관 관계를 깨고 보다 안정적인 학습으로 이어집니다.
2. `대상 네트워크:` DQN은 메인 네트워크의 가중치로 주기적으로 업데이트되는 별도의 대상 네트워크를 사용합니다. 이렇게 하면 지속적으로 변화하는 목표 값으로 인해 발생하는 진동 및 발산 가능성을 줄여 학습 프로세스를 안정화하는 데 도움이 됩니다.

DQN은 Double DQN, Dueling DQN 및 Prioritized Experience Replay와 같은 많은 확장 및 개선 사항에 영감을 주어 심층 강화 학습 분야에서 영향력 있는 알고리즘이었습니다.

알파고를 만든 알고리즘 입니다.

## 2014 – Encoder/Decoder, Adam

인코더/디코더:

`인코더-디코더 아키텍처는 특히 기계 번역, 텍스트 요약 및 이미지 캡션과 같은 시퀀스 간(seq2seq) 작업을 위한 딥 러닝에서 널리 사용되는 프레임워크`입니다. 아키텍처는 두 가지 주요 구성 요소로 구성됩니다.

1. `인코더`: `인코더는 입력 시퀀스를 처리하고 이를 고정 크기 컨텍스트 벡터 또는 잠재 표현으로 압축하는 신경망(일반적으로 RNN 또는 LSTM)입니다.` `이 벡터는 디코딩 프로세스에 필요한 입력 시퀀스에서 필수 정보를 캡처`합니다.
2. `디코더`: 디코더는 인코더에서 `생성된 컨텍스트 벡터를 가져와 단계적으로 출력 시퀀스를 생성하는 또 다른 신경망(일반적으로 RNN 또는 LSTM)입니다.` 컨텍스트 벡터와 이전 예측을 기반으로 출력 시퀀스의 `다음 요소를 예측합니다.`

인코더-디코더 아키텍처는 디코더가 디코딩 프로세스 중에 입력 시퀀스의 특정 부분에 집중할 수 있게 하여 긴 시퀀스에서 성능을 향상시키는 어텐션 메커니즘과 결합되는 경우가 많습니다.

Adam(적응 모멘트 추정):

Adam은 2014년에 Diederik Kingma와 Jimmy Ba가 소개한 신경망의 기울기 기반 최적화를 위한 최적화 알고리즘입니다. 이것은 확률적 기울기 하강(SGD) 방법의 확장이며 각 매개변수에 대한 학습 속도를 개별적으로 조정하도록 설계되었습니다. `Adam은 그래디언트의 1차 모멘트(평균)와 2차 모멘트(비중심 분산)를 모두 계산하여 두 가지 인기 있는 적응형 학습 속도 기술인 AdaGrad 및 RMSProp의 이점을 결합`합니다.

Adam의 주요 기능은 다음과 같습니다.

1. `적응형 학습 속도`: Adam은 기울기의 첫 번째 및 두 번째 모멘트를 기반으로 각 매개변수에 대한 학습 속도를 조정하여 표준 SGD에 비해 더 빠른 수렴 및 개선된 일반화로 이어질 수 있습니다.
2. `모멘텀`: Adam은 기울기의 지수 이동 평균을 사용하여 모멘텀을 통합하여 수렴을 가속화하고 최적화 환경에서 로컬 최소값 또는 안장 지점을 극복하는 데 도움이 될 수 있습니다.
3. `편향 보정`: Adam은 모멘트가 0으로 편향될 수 있는 훈련 시작 시 첫 번째 및 두 번째 모멘트의 정확한 추정을 보장하기 위해 편향 수정 항을 포함합니다.

Adam은 구현 용이성, 빠른 수렴, 다양한 신경망 아키텍처 및 문제에 대한 견고성으로 인해 딥 러닝에서 널리 사용되는 최적화 알고리즘이 되었습니다.

## 2015 – GAN, ResNet

GAN(생성적 적대 신경망):
GAN은 2014년 이안 굿펠로우(Ian Goodfellow)와 그의 동료들이 소개한 생성 모델입니다. 두 개의 신경망인 생성기와 판별기로 구성되며 경쟁 환경에서 동시에 훈련됩니다. Generator는 합성 데이터 샘플을 생성하고 Discriminator는 실제 샘플과 생성된 샘플을 구별하는 방법을 학습합니다. 교육 과정은 현실적인 샘플을 생성하는 생성기의 능력과 진짜와 가짜 샘플을 정확하게 식별하는 판별기의 능력을 향상시키는 것을 목표로 합니다. 훈련이 진행됨에 따라 생성기는 점점 더 사실적인 샘플을 생성하는 능력이 향상되어 판별자가 실제 데이터와 가짜 데이터를 구별하기가 더 어려워집니다. GAN은 이미지 합성, 스타일 전송, 데이터 확장 및 도메인 적응을 포함한 다양한 응용 분야에서 널리 사용되었습니다.

ResNet(잔여 네트워크):
ResNet은 2015년 Kaiming He와 그의 동료들이 소개한 심층 합성곱 신경망(CNN) 아키텍처입니다. 심층 신경망의 훈련을 방해하는 기울기 소멸 문제를 해결하도록 설계되었습니다. ResNet은 그라디언트가 네트워크를 통해 보다 효과적으로 흐를 수 있도록 건너뛰기 연결 또는 바로가기 연결을 도입했습니다. 이러한 연결은 블록의 입력과 출력 사이의 잔차 함수를 학습하는 잔차 블록을 생성합니다. 이 아키텍처는 정확도를 유지하고 기울기 소실 문제를 완화하면서 훨씬 더 깊은 네트워크(원본 논문에서 최대 152개 계층)의 교육을 가능하게 합니다. ResNet은 이미지 분류, 객체 감지 및 의미론적 분할과 같은 다양한 컴퓨터 비전 작업에서 최첨단 결과를 달성했으며 딥 러닝 아키텍처의 수많은 변형 및 개선에 영감을 주었습니다.

## 2016 –

## 2017 – Transformer

## 2018 – Bert

## 2019 – Big Language Models(GPT-X)

## 2020 – Self-Supervised Learning





















## Bias and Variance

cost를 최소화 한다는 것은 bias, variance, noise 를 최소화하는 것이다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e9c6c94b-a33c-4d71-aee7-6f00be330d8e/Untitled.png)

## 최적화

최적화 이론은 딥러닝에서 매우 중요한 개념입니다. 
일반화와 최적화의 균일점을 찾아라 ! 그래서 우리는 cross-validation 을 사용합니다.

- Bagging
학습데이터를 여러개를 만들어서 부트스트래핑을 하는것을 말합니다.  부트스트랩이란 ? 수 많은 데이터 중에 랜덤 샘플링을 여러개 만들어서 예측을 하는
- Boosting
잘 안되는 데이터에 맞춰서 학습을 여러번 시키는 방법을 말한다.

## Gradinet Descent

어떻게 산 밑으로 내려갈 것인가? 

- 여러가지 테크니션
    - 모멘텀
    - Nesterov Accelerated Gradinet
    - Adagrad
    - Adadelta
    - RMSStop

## Regularization

 오버피팅을 방지 하세요.

- Early stopping
- Parameter norm penalty
- Data augmentation
- Noise robustness
  - 노이즈를 넣으면 이상하게 잘된다.
- Lable smoothing
  - Cut Mix 
  - Cut Out 
  - Mixup
- Dropout
- Batch normalizaion