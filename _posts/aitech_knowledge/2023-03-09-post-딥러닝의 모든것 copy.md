---
title: "딥러닝의 모든 것(All about Deep Learning)"
last_modified_at: 2023-03-08T16:20:02-05:00
categories:
    - aitech_knowledge
tages:
    - aitech
    - nlp
toc: true
toc_sticky: true
toc_label: "목차"
use_math: true
sidebar:
    nav: "counts"
---




![image](../../../image/aitech.png)


# 누가 좋은 딥러너인가 ?

누가 좋은 딥러너 일까요? 수학적 지식이 뛰어나야 합니다. 논문을 잘 읽어야 하고요. 구현 능력이 뛰어나야 합니다.  여러분들은 그 실력을 키우시기 바랍니다.

```
논문을 읽거나 모델을 연구할 때

    1. 데이터 
    2. 모델 
    3. 손실함수 
    4. 학습 알고리즘 

를 고려합시다.
```

## 공부합시다.

```

1. Historical Review
2. Neural Networks & Multi-Layer Perceptron
3. Optimization Methods
4. Convolutional Neural Networks
5. Modern CNN
6. Computer Vision Applications
7. Recurrent Neural Networks
8. Transformer
9. Generative Models Part1
10.Generative Models Part2
```

## 딥러닝 역사의 중요한 기점들.

```
- AlexNet(2012) -> 딥러닝의 가능성을 폭발적으로 알려주었다. 
- DQN (2013) -> 강화학습
- Encoder / Decoder (2014)
- Adam (2014) -> 결과가 매우 좋다.
- GAN, ResNet (2015) 
- Residual Networks (2015) -> 네트워크를 깊게 쌓게 해준다.
- Transformer (2017) -> 도발적
- Bert (2018)
- Big Language Models (GPT)
- Self-Supervised Learning -> 학습 데이터를 스스로 생성한다.
```

# 딥러닝 개괄

## 신경망

신경망에서 비선형 함수를 넣어주는 이유가 무얼까? 선형적 함수로만 표현을 하게 되면은 그것은 결국 선형함수 하나로 변환 될 수 있다는 것을 의미한다. 그렇기 때문에 선형적으로 표현할 수 없는 비선형 함수 (예를 들어 활성화 함수) 를 넣어줌으로써 표현력을 극대화하는 것이다.

## 손실함수의 종류

- MSE
- CE
- MLE

## Bias and Variance

cost를 최소화 한다는 것은 bias, variance, noise 를 최소화하는 것이다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e9c6c94b-a33c-4d71-aee7-6f00be330d8e/Untitled.png)

## 최적화

최적화 이론은 딥러닝에서 매우 중요한 개념입니다. 
일반화와 최적화의 균일점을 찾아라 ! 그래서 우리는 cross-validation 을 사용합니다.

- Bagging
학습데이터를 여러개를 만들어서 부트스트래핑을 하는것을 말합니다.  부트스트랩이란 ? 수 많은 데이터 중에 랜덤 샘플링을 여러개 만들어서 예측을 하는
- Boosting
잘 안되는 데이터에 맞춰서 학습을 여러번 시키는 방법을 말한다.

## Gradinet Descent

어떻게 산 밑으로 내려갈 것인가? 

- 여러가지 테크니션
    - 모멘텀
    - Nesterov Accelerated Gradinet
    - Adagrad
    - Adadelta
    - RMSStop

## Regularization

 오버피팅을 방지 하세요.

- Early stopping
- Parameter norm penalty
- Data augmentation
- Noise robustness
  - 노이즈를 넣으면 이상하게 잘된다.
- Lable smoothing
  - Cut Mix 
  - Cut Out 
  - Mixup
- Dropout
- Batch normalizaion