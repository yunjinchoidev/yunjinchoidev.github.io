---
title: "RNN, LSTM, GRU"
last_modified_at: 2023-03-21T20:20:02-05:00
categories:
    - ai
tages:
    - nlp
toc: true
toc_sticky: true
toc_label: "목차"
---


<p align="center">
<img src="../../../image/ai.png" 
width="400" height="400"/>
</p>




# RNN

RNN(Recurrent Neural Network)은 시계열, 자연어 및 음성 신호와 같은 순차적 데이터로 작업하도록 설계된 인공 신경망 클래스입니다. 기존의 피드포워드 신경망과 달리 RNN에는 자체 루프백 연결이 있어 일종의 메모리 역할을 할 수 있는 숨겨진 상태를 유지할 수 있습니다. 따라서 `RNN은 입력 시퀀스를 처리하고 입력 데이터의 컨텍스트 또는 기록을 기반으로 결과를 예측하는 작업에 특히 적합`합니다.

RNN의 주요 구성 요소는 입력 계층, 숨겨진 계층 및 출력 계층입니다. 입력 계층은 각 시간 단계에서 입력 데이터를 받는 반면 숨겨진 계층은 이전 시간 단계에서 정보를 캡처하는 숨겨진 상태를 유지합니다. `출력 레이어는 현재 입력 및 숨겨진 상태를 기반으로 최종 출력 또는 예측을 생성`합니다.

RNN의 핵심 기능은 시간 단계에 걸쳐 가중치를 공유하는 기능입니다. 즉, 입력 시퀀스의 각 요소를 처리하는 데 동일한 가중치 집합이 사용됩니다. 이를 통해 RNN은 서로 다른 시퀀스 길이에 걸쳐 일반화하고 입력 데이터의 기본 구조를 학습할 수 있습니다.

그러나 `RNN에는 몇 가지 제한 사항이 있습니다. 한 가지 중요한 문제는 그래디언트 소멸 및 폭발 문제로 인해 장거리 종속성을 학습하는 데 어려움이 있다는 것입니다.` 이는 시간을 통한 역전파(BPTT) 알고리즘 동안 기울기가 너무 작거나 너무 커져서 가중치를 조정하고 신경망을 효과적으로 훈련시키기 어려울 때 발생합니다. 이 문제를 해결하기 위해 `LSTM`(Long Short-Term Memory) 및 `GRU`(Gated Recurrent Units)와 같은 고급 RNN 아키텍처가 개발되었습니다.

`이러한 한계에도 불구하고 RNN은 자연어 처리, 기계 번역, 음성 인식 및 시계열 예측을 포함한 다양한 sequence-to-sequence 작업에 성공적으로 적용되었습니다.`

> **기울기 소실 문제를 해결해야 한다 !**
> 

## LSTM(Long Short-Term Memory)

LSTM(Long Short-Term Memory)은 RNN(Recurrent Neural Network) 아키텍처의 한 유형으로 기존 `RNN의 한계, 특히 기울기 소멸 및 폭발 문제를 해결하도록 특별히 설계`되었습니다. 이러한 문제는 심층 RNN을 교육할 때 발생하여 순차적 데이터에서 장거리 종속성을 학습하기 어렵게 만듭니다.

LSTM은 1997년 Hochreiter와 Schmidhuber에 의해 소개되었으며 이후 자연어 처리, 음성 인식 및 시계열 예측과 같은 다양한 시퀀스 간 작업에 널리 사용되었습니다.

LSTM 네트워크의 주요 혁신은 `입력 게이트, 망각 게이트 및 출력 게이트`의 세 가지 주요 구성 요소로 구성된 메모리 셀입니다. 이러한 게이트는 함께 작동하여 `네트워크를 통한 정보 흐름을 규제하므로 LSTM이 긴 시퀀스에서 내부 상태를 효과적으로 유지하고 업데이트할 수 있`습니다.

LSTM의 세 가지 주요 게이트는 각각 조금씩 다르게 작동합니다.

- `**input Gate**는 현재 입력과 이전 상태를 기반으로 메모리에 추가할 새 정보의 양을 결정합니다`. 입력 및 이전 상태에 학습된 가중치를 곱하고 시그모이드 함수를 통해 입력한 다음 시그모이드의 출력을 입력 및 다른 학습된 가중치 세트에서 계산된 후보 상태와 곱하여 이를 수행합니다.
- `**Forget Gate**는 현재 입력과 이전 상태를 기반으로 이전 상태를 얼마나 잊을 것인지 결정합니다.` 입력과 이전 상태에 학습된 가중치를 곱하고 시그모이드 함수를 통해 입력한 다음 시그모이드의 출력을 이전 상태와 곱하여 이전 상태를 얼마나 유지할지 결정합니다.
- `**output gate** 현재 입력과 이전 상태를 기반으로 네트워크의 다음 계층으로 출력할 현재 상태의 양을 결정합니다.` 입력과 이전 상태에 학습된 가중치를 곱하고 시그모이드 함수를 통해 입력한 다음 시그모이드의 출력에 현재 상태를 곱하여 현재 상태에서 출력할 양을 결정합니다.
- Update Cell

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c4b99ef2-1d31-479a-bf49-e339559cc79b/Untitled.png)

## GRU(Gated Reccurrent Unit)

Gated Recurrent Unit(GRU)은 표`준 RNN의 한계, 특히 RNN이 순차 데이터에서 장거리 종속성을 학습하기 어렵게 만드는 기울기 소실 문제를 해결하도록 설계된 순환 신경망(RNN) 아키텍처 유형`입니다. GRU는 Cho et al. 2014년에 LSTM(Long Short-Term Memory) 아키텍처에 대한 더 간단한 대안으로 등장했습니다.

GRU는 업데이트 게이트와 재설정 게이트라는 두 개의 게이트로 구성되며, 함께 작동하여 네트워크를 통한 정보 흐름을 규제하고 장거리 종속성을 효율적으로 학습할 수 있습니다. 이 게이트는 GRU가 이전 숨겨진 상태 및 현재 입력에서 유지하거나 폐기할 정보를 결정하는 데 도움이 됩니다.

1. `**Update gate** : Update gate는 이전 숨겨진 상태를 유지하거나 새 입력으로 업데이트해야 하는 정도를 결정`합니다. 0과 1 사이의 값을 출력하는 시그모이드 활성화 함수를 사용하여 이전 숨겨진 상태와 현재 입력의 정보의 중요성을 평가합니다.
2. `**Reset gate** : 재설정 게이트는 GRU가 현재 숨겨진 상태를 계산하는 데 사용되어야 하는 과거 숨겨진 상태의 양을 결정하는 데 도움`이 됩니다. 또한 시그모이드 활성화 기능을 사용하여 네트워크가 이전의 숨겨진 상태를 잊고 현재 입력에 집중해야 할 때를 학습할 수 있습니다.

GRU와 LSTM의 주요 차이점 중 하나는 GRU에 출력 게이트와 별도의 메모리 셀이 없다는 것입니다. 이러한 단순화로 인해 GRU는 LSTM보다 계산적으로 더 효율적이지만 더 복잡한 메모리 관리가 필요한 특정 작업에서는 성능이 약간 떨어질 수 있습니다.

단순성에도 불구하고 GRU는 자연어 처리, 기계 번역, 음성 인식 및 시계열 예측과 같은 다양한 시퀀스 간 작업에서 뛰어난 성능을 보여주었습니다. 표준 RNN의 계산 효율성과 LSTM의 고급 메모리 관리 기능 간의 균형을 제공합니다. GRU와 LSTM의 주요 차이점 중 하나는 GRU에 출력 게이트와 별도의 메모리 셀이 없다는 것이다.

> RNN < **LSTM < GRU <<<< Transfomer**
> 

> 적은 파라메터로 최적화 하는 것이 일반화 성능이 높습니다.
>